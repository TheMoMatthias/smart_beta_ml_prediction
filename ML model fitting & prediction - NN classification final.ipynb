{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### run once before each notebook execution and clear output\n",
    "# %pip install pandas_datareader \n",
    "# %pip install statsmodels\n",
    "# %pip install linearmodels\n",
    "# %pip install quantstats\n",
    "# %pip install wrds\n",
    "# %pip install pathlib\n",
    "# %pip install scipy\n",
    "# %pip install sklearn\n",
    "# %pip install pandas_datareader \n",
    "# %pip install tqdm\n",
    "# %pip install ZipFile\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install xgboost\n",
    "# %pip install quandl\n",
    "# %pip install keras\n",
    "# %pip install --upgrade tensorflow \n",
    "# %pip install shap\n",
    "\n",
    "# print('############################# completed installations #####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 16:31:14.146505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-28 16:31:14.146541: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# after we install all the packages, import all of them for the use in today's lecture!\n",
    "# database access\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "import quandl as quandl\n",
    "import wrds as wrds\n",
    "# storage and operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import linearmodels as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#portfolio optimization:\n",
    "# libraries we might use for testing\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "import linearmodels as lm\n",
    "from linearmodels.panel import compare  \n",
    "import time\n",
    "import quantstats as qs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc\n",
    "\n",
    "# path_factors = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\Factors')\n",
    "# path_wrds = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\WRDS')\n",
    "# path_ml = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Machine Learning Models')\n",
    "# path_visual = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Visualisation/')\n",
    "# path_betas = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\Factor Betas')\n",
    "\n",
    "# factor_data = path_factors / 'factor_data.h5'\n",
    "# macro_data = path_factors / 'macro_data.h5'\n",
    "# wrds_data = path_wrds / 'wrds_data.h5'\n",
    "# saved_models = path_ml / 'saved_ml_models.h5'\n",
    "\n",
    "#bucket s3 AWS\n",
    "path_bucket = 's3://mm-master-thesis-data/'\n",
    "path_bucket_ml = 's3://mm-master-thesis-data/ml-models/'\n",
    "\n",
    "#factor data \n",
    "factor_data = path_bucket + \"monthly_factor_data.csv\"\n",
    "factor_data_adj = path_bucket + 'monthly_factors_adjusted.csv'\n",
    "factor_data_base = path_bucket + 'monthly_factors_base.csv'\n",
    "factor_data_base_extended = path_bucket + 'monthly_factors_base_extended.csv'\n",
    "\n",
    "benchmark_ff = path_bucket + 'benchmark_factor_data.csv'\n",
    "benchmark_bk = path_bucket + 'benchmark_factor_data_bkelly.csv'\n",
    "\n",
    "#macro data \n",
    "macro_data = path_bucket + 'macro_data.csv'\n",
    "\n",
    "# stock data\n",
    "wrds_data_file = path_bucket +'wrds_data_complete_abs.csv' \n",
    "wrds_data_file_wins = path_bucket + 'monthly_wrds_data_complete_wins.csv'\n",
    "wrds_data_file_wins_quin = path_bucket + 'monthly_stock_data_wins_quin.csv'\n",
    "wrds_data_file_final = path_bucket + 'monthly_stock_data_final.csv'\n",
    "\n",
    "#pca factors\n",
    "pca_factor_path = path_bucket + 'monthly_pca_factors.csv'\n",
    "\n",
    "\n",
    "# prediction results\n",
    "OLS_df = path_bucket + 'ols_results.csv'\n",
    "Ridge_df = path_bucket + 'ridge_results.csv'\n",
    "Lasso_df = path_bucket + 'lasso_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def file_saver(data_object, data_name_as_string):\n",
    "    saver = boto3.Session().resource('s3').Bucket('mm-master-thesis-data').Object(data_object).upload_file(data_name_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def winsorizer(df, columns, limits):\n",
    "    \n",
    "    winsorized_df = df.copy(deep=True)\n",
    "    for c in columns:\n",
    "        goods    = winsorized_df[c].notna()\n",
    "        winsorized_df.loc[goods,c] = winsorize(winsorized_df.loc[goods,c], limits=limits)\n",
    "\n",
    "    return winsorized_df\n",
    "\n",
    "def find_min_max(df, number_of_extremes, variable):\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "\n",
    "        max_df_returns = df.max().tolist()\n",
    "        max_df_returns.sort(reverse=True)\n",
    "        number_max_returns = max_df_returns[:number_of_extremes]\n",
    "        number_max_returns = [ '%.2f' % elem for elem in number_max_returns ]\n",
    "        print('The', number_of_extremes, 'highest', variable, 'are: ') \n",
    "        print(number_max_returns)\n",
    "        print('')\n",
    "        min_df_returns = df.min().tolist()\n",
    "        min_df_returns.sort()\n",
    "        number_min_returns = min_df_returns[:number_of_extremes]\n",
    "        number_min_returns = [ '%.2f' % elem for elem in number_min_returns ]\n",
    "        print('The', number_of_extremes, 'lowest', variable, 'are: ') \n",
    "        print(number_min_returns)\n",
    "    elif isinstance(df,pd.Series):\n",
    "        max_df_returns = df.values.tolist()\n",
    "        max_df_returns.sort(reverse=True)\n",
    "        number_max_returns = max_df_returns[:number_of_extremes]\n",
    "        number_max_returns = [ '%.2f' % elem for elem in number_max_returns ]\n",
    "        print('The', number_of_extremes, 'highest', variable, 'are: ') \n",
    "        print(number_max_returns)\n",
    "        print('')\n",
    "        min_df_returns = df.values.tolist()\n",
    "        min_df_returns.sort()\n",
    "        number_min_returns = min_df_returns[:number_of_extremes]\n",
    "        number_min_returns = [ '%.2f' % elem for elem in number_min_returns ]\n",
    "        print('The', number_of_extremes, 'lowest', variable, 'are: ') \n",
    "        print(number_min_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>bm</th>\n",
       "      <th>tbl</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>...</th>\n",
       "      <th>DDURRG3M086SBEA</th>\n",
       "      <th>DNDGRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>4.407</td>\n",
       "      <td>10.517</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>85.536</td>\n",
       "      <td>35.172</td>\n",
       "      <td>24.873</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.54</td>\n",
       "      <td>5.58</td>\n",
       "      <td>12900.00</td>\n",
       "      <td>35574.00</td>\n",
       "      <td>230.302</td>\n",
       "      <td>15.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>4.453</td>\n",
       "      <td>10.613</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>85.817</td>\n",
       "      <td>35.270</td>\n",
       "      <td>25.027</td>\n",
       "      <td>5.99</td>\n",
       "      <td>7.56</td>\n",
       "      <td>5.61</td>\n",
       "      <td>13058.00</td>\n",
       "      <td>36030.00</td>\n",
       "      <td>230.172</td>\n",
       "      <td>14.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>4.500</td>\n",
       "      <td>10.710</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>85.946</td>\n",
       "      <td>35.317</td>\n",
       "      <td>25.173</td>\n",
       "      <td>6.03</td>\n",
       "      <td>7.62</td>\n",
       "      <td>5.65</td>\n",
       "      <td>13177.00</td>\n",
       "      <td>36241.00</td>\n",
       "      <td>230.965</td>\n",
       "      <td>14.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>4.557</td>\n",
       "      <td>10.770</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>86.439</td>\n",
       "      <td>35.456</td>\n",
       "      <td>25.293</td>\n",
       "      <td>6.08</td>\n",
       "      <td>7.66</td>\n",
       "      <td>5.69</td>\n",
       "      <td>14686.00</td>\n",
       "      <td>37965.00</td>\n",
       "      <td>229.856</td>\n",
       "      <td>15.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>4.613</td>\n",
       "      <td>10.830</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>86.802</td>\n",
       "      <td>35.699</td>\n",
       "      <td>25.434</td>\n",
       "      <td>6.11</td>\n",
       "      <td>7.68</td>\n",
       "      <td>5.72</td>\n",
       "      <td>14975.00</td>\n",
       "      <td>38553.00</td>\n",
       "      <td>231.645</td>\n",
       "      <td>19.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>59.129</td>\n",
       "      <td>98.557</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>86.662</td>\n",
       "      <td>98.675</td>\n",
       "      <td>120.600</td>\n",
       "      <td>25.50</td>\n",
       "      <td>29.43</td>\n",
       "      <td>22.85</td>\n",
       "      <td>344023.25</td>\n",
       "      <td>726723.85</td>\n",
       "      <td>4385.243</td>\n",
       "      <td>22.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>58.851</td>\n",
       "      <td>98.220</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>86.616</td>\n",
       "      <td>98.508</td>\n",
       "      <td>120.964</td>\n",
       "      <td>25.46</td>\n",
       "      <td>29.11</td>\n",
       "      <td>23.00</td>\n",
       "      <td>347627.43</td>\n",
       "      <td>730734.42</td>\n",
       "      <td>4452.042</td>\n",
       "      <td>27.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>58.660</td>\n",
       "      <td>96.857</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>86.532</td>\n",
       "      <td>98.398</td>\n",
       "      <td>121.094</td>\n",
       "      <td>25.54</td>\n",
       "      <td>29.31</td>\n",
       "      <td>22.99</td>\n",
       "      <td>348262.68</td>\n",
       "      <td>730398.69</td>\n",
       "      <td>4514.528</td>\n",
       "      <td>29.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11</th>\n",
       "      <td>58.470</td>\n",
       "      <td>95.493</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>86.339</td>\n",
       "      <td>98.547</td>\n",
       "      <td>121.109</td>\n",
       "      <td>25.67</td>\n",
       "      <td>29.49</td>\n",
       "      <td>23.11</td>\n",
       "      <td>350766.10</td>\n",
       "      <td>733096.73</td>\n",
       "      <td>4621.237</td>\n",
       "      <td>24.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12</th>\n",
       "      <td>58.279</td>\n",
       "      <td>94.130</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>86.486</td>\n",
       "      <td>98.992</td>\n",
       "      <td>121.597</td>\n",
       "      <td>25.78</td>\n",
       "      <td>29.63</td>\n",
       "      <td>23.18</td>\n",
       "      <td>350336.43</td>\n",
       "      <td>733463.42</td>\n",
       "      <td>4685.876</td>\n",
       "      <td>22.383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D12     E12     bm    tbl    lty   ntis  Rfree   infl    ltr  \\\n",
       "date                                                                       \n",
       "1977-07   4.407  10.517  0.897  0.052  0.077  0.033  0.004  0.005 -0.007   \n",
       "1977-08   4.453  10.613  0.927  0.055  0.075  0.034  0.004  0.003  0.020   \n",
       "1977-09   4.500  10.710  0.942  0.058  0.076  0.032  0.004  0.003 -0.003   \n",
       "1977-10   4.557  10.770  0.975  0.062  0.078  0.033  0.005  0.003 -0.009   \n",
       "1977-11   4.613  10.830  0.962  0.061  0.078  0.029  0.005  0.005  0.009   \n",
       "...         ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-08  59.129  98.557  0.236  0.001  0.006 -0.009  0.000  0.003 -0.035   \n",
       "2020-09  58.851  98.220  0.241  0.001  0.007 -0.006  0.000  0.001  0.008   \n",
       "2020-10  58.660  96.857  0.253  0.001  0.008 -0.002  0.000  0.000 -0.024   \n",
       "2020-11  58.470  95.493  0.226  0.001  0.009 -0.005  0.000 -0.001  0.009   \n",
       "2020-12  58.279  94.130  0.219  0.001  0.009 -0.000  0.000  0.001 -0.011   \n",
       "\n",
       "         corpr  ...  DDURRG3M086SBEA  DNDGRG3M086SBEA  DSERRG3M086SBEA  \\\n",
       "date            ...                                                      \n",
       "1977-07 -0.001  ...           85.536           35.172           24.873   \n",
       "1977-08  0.014  ...           85.817           35.270           25.027   \n",
       "1977-09 -0.002  ...           85.946           35.317           25.173   \n",
       "1977-10 -0.004  ...           86.439           35.456           25.293   \n",
       "1977-11  0.006  ...           86.802           35.699           25.434   \n",
       "...        ...  ...              ...              ...              ...   \n",
       "2020-08 -0.049  ...           86.662           98.675          120.600   \n",
       "2020-09  0.004  ...           86.616           98.508          120.964   \n",
       "2020-10 -0.019  ...           86.532           98.398          121.094   \n",
       "2020-11  0.051  ...           86.339           98.547          121.109   \n",
       "2020-12  0.000  ...           86.486           98.992          121.597   \n",
       "\n",
       "         CES0600000008  CES2000000008  CES3000000008  DTCOLNVHFNM   DTCTHFNM  \\\n",
       "date                                                                           \n",
       "1977-07           5.97           7.54           5.58     12900.00   35574.00   \n",
       "1977-08           5.99           7.56           5.61     13058.00   36030.00   \n",
       "1977-09           6.03           7.62           5.65     13177.00   36241.00   \n",
       "1977-10           6.08           7.66           5.69     14686.00   37965.00   \n",
       "1977-11           6.11           7.68           5.72     14975.00   38553.00   \n",
       "...                ...            ...            ...          ...        ...   \n",
       "2020-08          25.50          29.43          22.85    344023.25  726723.85   \n",
       "2020-09          25.46          29.11          23.00    347627.43  730734.42   \n",
       "2020-10          25.54          29.31          22.99    348262.68  730398.69   \n",
       "2020-11          25.67          29.49          23.11    350766.10  733096.73   \n",
       "2020-12          25.78          29.63          23.18    350336.43  733463.42   \n",
       "\n",
       "           INVEST  VIXCLSx  \n",
       "date                        \n",
       "1977-07   230.302   15.362  \n",
       "1977-08   230.172   14.493  \n",
       "1977-09   230.965   14.087  \n",
       "1977-10   229.856   15.454  \n",
       "1977-11   231.645   19.638  \n",
       "...           ...      ...  \n",
       "2020-08  4385.243   22.879  \n",
       "2020-09  4452.042   27.587  \n",
       "2020-10  4514.528   29.436  \n",
       "2020-11  4621.237   24.390  \n",
       "2020-12  4685.876   22.383  \n",
       "\n",
       "[522 rows x 136 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_factors = pd.read_csv(macro_data)\n",
    "macro_factors = macro_factors.set_index('date',drop=True)\n",
    "macro_factors = macro_factors.applymap(lambda x: f'{x:.3f}')\n",
    "macro_factors = macro_factors.drop(columns=['Unnamed: 0'])\n",
    "macro_factors = macro_factors.astype(float)\n",
    "macro_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0trade</th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>adm</th>\n",
       "      <th>almq_1</th>\n",
       "      <th>amihud</th>\n",
       "      <th>atoq_1</th>\n",
       "      <th>bab</th>\n",
       "      <th>bm</th>\n",
       "      <th>bmj</th>\n",
       "      <th>...</th>\n",
       "      <th>ta</th>\n",
       "      <th>vfp</th>\n",
       "      <th>vhp</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>fp_6</th>\n",
       "      <th>iaq_12</th>\n",
       "      <th>p52w_6</th>\n",
       "      <th>r6_1</th>\n",
       "      <th>resid6</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11</th>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12</th>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0trade  abr_1    aci    adm  almq_1  amihud  atoq_1    bab     bm  \\\n",
       "date                                                                         \n",
       "1977-07   0.019 -0.005 -0.021  0.013  -0.017   0.013   0.011  0.029 -0.011   \n",
       "1977-08  -0.006  0.017 -0.004 -0.004  -0.017   0.001   0.018  0.003 -0.034   \n",
       "1977-09   0.010  0.018 -0.003 -0.008  -0.005   0.012  -0.007  0.004 -0.002   \n",
       "1977-10   0.011 -0.007 -0.006  0.012   0.016   0.017  -0.000  0.002  0.014   \n",
       "1977-11  -0.031  0.032 -0.010  0.001   0.024   0.041   0.007  0.003  0.011   \n",
       "...         ...    ...    ...    ...     ...     ...     ...    ...    ...   \n",
       "2020-08  -0.049  0.018 -0.011  0.000   0.009  -0.014   0.030 -0.040 -0.010   \n",
       "2020-09  -0.006  0.006 -0.013 -0.010  -0.004  -0.005   0.011  0.013 -0.028   \n",
       "2020-10  -0.012 -0.003  0.008  0.045   0.056   0.041   0.002 -0.020  0.025   \n",
       "2020-11  -0.100 -0.014 -0.032  0.021   0.088   0.078  -0.058 -0.051  0.056   \n",
       "2020-12  -0.042  0.010 -0.008 -0.066  -0.027   0.047   0.005  0.053 -0.046   \n",
       "\n",
       "           bmj  ...     ta    vfp    vhp  dtv_12   fp_6  iaq_12  p52w_6  \\\n",
       "date            ...                                                       \n",
       "1977-07 -0.004  ... -0.006  0.009 -0.008  -0.011 -0.009  -0.007   0.008   \n",
       "1977-08 -0.033  ...  0.016 -0.006 -0.030  -0.066 -0.025   0.017  -0.018   \n",
       "1977-09 -0.014  ...  0.011 -0.010  0.019  -0.071 -0.022   0.005   0.022   \n",
       "1977-10  0.019  ...  0.007 -0.108  0.011   0.055 -0.008   0.000   0.010   \n",
       "1977-11  0.004  ...  0.004  0.029  0.004  -0.005  0.013  -0.014  -0.005   \n",
       "...        ...  ...    ...    ...    ...     ...    ...     ...     ...   \n",
       "2020-08  0.032  ... -0.001 -0.006  0.038   0.036  0.006   0.023  -0.011   \n",
       "2020-09 -0.063  ...  0.020 -0.006  0.006   0.141 -0.018   0.012   0.049   \n",
       "2020-10  0.043  ... -0.000  0.005  0.002  -0.036  0.003  -0.000  -0.027   \n",
       "2020-11  0.113  ... -0.026  0.013  0.035   0.083  0.219  -0.018  -0.197   \n",
       "2020-12 -0.018  ... -0.014 -0.024 -0.019  -0.006  0.019  -0.004  -0.017   \n",
       "\n",
       "          r6_1  resid6  tbiq_12  \n",
       "date                             \n",
       "1977-07 -0.016   0.001    0.015  \n",
       "1977-08 -0.012  -0.017   -0.003  \n",
       "1977-09  0.025   0.007    0.013  \n",
       "1977-10 -0.007   0.003    0.008  \n",
       "1977-11  0.026   0.007    0.001  \n",
       "...        ...     ...      ...  \n",
       "2020-08 -0.034  -0.025    0.022  \n",
       "2020-09  0.043  -0.014   -0.030  \n",
       "2020-10 -0.014  -0.006   -0.013  \n",
       "2020-11  0.017   0.005    0.019  \n",
       "2020-12  0.014  -0.013    0.002  \n",
       "\n",
       "[522 rows x 106 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors = pd.read_csv(factor_data)\n",
    "monthly_factors= monthly_factors.set_index('date',drop=True)\n",
    "monthly_factors = monthly_factors.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors= monthly_factors.astype('float')\n",
    "monthly_factors= monthly_factors.drop(columns=['Unnamed: 0','poa_y','rf'])\n",
    "monthly_factors= monthly_factors.rename(columns={'poa_x':'poa'})\n",
    "monthly_factors= monthly_factors.loc[(monthly_factors.index>=('1977-01'))&(monthly_factors.index<=('2020-12'))]\n",
    "monthly_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>amihud</th>\n",
       "      <th>bab</th>\n",
       "      <th>def_1</th>\n",
       "      <th>dfin</th>\n",
       "      <th>dii</th>\n",
       "      <th>dlno</th>\n",
       "      <th>dsre</th>\n",
       "      <th>ep</th>\n",
       "      <th>...</th>\n",
       "      <th>pda</th>\n",
       "      <th>poa</th>\n",
       "      <th>qmj</th>\n",
       "      <th>r1a</th>\n",
       "      <th>rer</th>\n",
       "      <th>rev_1</th>\n",
       "      <th>roe_1</th>\n",
       "      <th>sm_1</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abr_1    aci  amihud    bab  def_1   dfin    dii   dlno   dsre  \\\n",
       "date                                                                      \n",
       "1977-07 -0.005 -0.021   0.013  0.029  0.026  0.007 -0.016  0.002 -0.004   \n",
       "1977-08  0.017 -0.004   0.001  0.003  0.004  0.020  0.001  0.001 -0.008   \n",
       "1977-09  0.018 -0.003   0.012  0.004  0.057  0.000  0.000  0.013 -0.007   \n",
       "1977-10 -0.007 -0.006   0.017  0.002  0.009  0.007  0.003 -0.005 -0.004   \n",
       "1977-11  0.032 -0.010   0.041  0.003  0.021  0.017 -0.003 -0.019 -0.007   \n",
       "\n",
       "            ep  ...    pda    poa    qmj    r1a    rer  rev_1  roe_1   sm_1  \\\n",
       "date            ...                                                           \n",
       "1977-07  0.001  ... -0.005  0.005 -0.008  0.013  0.012 -0.016  0.018 -0.012   \n",
       "1977-08 -0.035  ... -0.002  0.010  0.009  0.021 -0.007 -0.023  0.018  0.004   \n",
       "1977-09  0.014  ...  0.011  0.007  0.031  0.008  0.002 -0.014  0.023 -0.011   \n",
       "1977-10  0.015  ... -0.000  0.010  0.004  0.013  0.019  0.008 -0.013 -0.010   \n",
       "1977-11 -0.000  ... -0.010 -0.009  0.011  0.019 -0.014 -0.034  0.042  0.017   \n",
       "\n",
       "         dtv_12  tbiq_12  \n",
       "date                      \n",
       "1977-07  -0.011    0.015  \n",
       "1977-08  -0.066   -0.003  \n",
       "1977-09  -0.071    0.013  \n",
       "1977-10   0.055    0.008  \n",
       "1977-11  -0.005    0.001  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors_adj = pd.read_csv(path_bucket+'monthly_factors_adjusted.csv')\n",
    "monthly_factors_adj = monthly_factors_adj.set_index('date',drop=True)\n",
    "monthly_factors_adj = monthly_factors_adj.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors_adj = monthly_factors_adj.astype('float')\n",
    "monthly_factors_adj= monthly_factors_adj.loc[(monthly_factors_adj.index>=('1977-01'))&(monthly_factors_adj.index<=('2020-12'))]\n",
    "\n",
    "monthly_factors_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>amihud</th>\n",
       "      <th>bab</th>\n",
       "      <th>cla</th>\n",
       "      <th>ctoq_1</th>\n",
       "      <th>def_1</th>\n",
       "      <th>dfin</th>\n",
       "      <th>dii</th>\n",
       "      <th>dlno</th>\n",
       "      <th>...</th>\n",
       "      <th>qmj</th>\n",
       "      <th>r1a</th>\n",
       "      <th>rer</th>\n",
       "      <th>rev_1</th>\n",
       "      <th>roa_1</th>\n",
       "      <th>roe_1</th>\n",
       "      <th>sm_1</th>\n",
       "      <th>sp</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abr_1    aci  amihud    bab    cla  ctoq_1  def_1   dfin    dii  \\\n",
       "date                                                                       \n",
       "1977-07 -0.005 -0.021   0.013  0.029  0.024   0.003  0.026  0.007 -0.016   \n",
       "1977-08  0.017 -0.004   0.001  0.003  0.022   0.017  0.004  0.020  0.001   \n",
       "1977-09  0.018 -0.003   0.012  0.004  0.022  -0.007  0.057  0.000  0.000   \n",
       "1977-10 -0.007 -0.006   0.017  0.002  0.003  -0.003  0.009  0.007  0.003   \n",
       "1977-11  0.032 -0.010   0.041  0.003  0.007   0.018  0.021  0.017 -0.003   \n",
       "\n",
       "          dlno  ...    qmj    r1a    rer  rev_1  roa_1  roe_1   sm_1     sp  \\\n",
       "date            ...                                                           \n",
       "1977-07  0.002  ... -0.008  0.013  0.012 -0.016  0.012  0.018 -0.012 -0.005   \n",
       "1977-08  0.001  ...  0.009  0.021 -0.007 -0.023  0.022  0.018  0.004 -0.027   \n",
       "1977-09  0.013  ...  0.031  0.008  0.002 -0.014  0.013  0.023 -0.011 -0.007   \n",
       "1977-10 -0.005  ...  0.004  0.013  0.019  0.008 -0.007 -0.013 -0.010  0.006   \n",
       "1977-11 -0.019  ...  0.011  0.019 -0.014 -0.034  0.035  0.042  0.017  0.012   \n",
       "\n",
       "         dtv_12  tbiq_12  \n",
       "date                      \n",
       "1977-07  -0.011    0.015  \n",
       "1977-08  -0.066   -0.003  \n",
       "1977-09  -0.071    0.013  \n",
       "1977-10   0.055    0.008  \n",
       "1977-11  -0.005    0.001  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors_base = pd.read_csv(path_bucket+'monthly_factors_base.csv')\n",
    "monthly_factors_base = monthly_factors_base.set_index('date',drop=True)\n",
    "monthly_factors_base = monthly_factors_base.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors_base = monthly_factors_base.astype('float')\n",
    "monthly_factors_base= monthly_factors_base.loc[(monthly_factors_base.index>=('1977-01'))&(monthly_factors_base.index<=('2020-12'))]\n",
    "\n",
    "monthly_factors_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data = pd.read_csv(wrds_data_file_final)\n",
    "monthly_stock_data = monthly_stock_data.loc[(monthly_stock_data.date>=('1977-01'))&(monthly_stock_data.date<=('2020-12'))]\n",
    "monthly_stock_data = monthly_stock_data.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding s&p 500 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_list = pd.read_csv(path_bucket+'sp500_permnos.csv')\n",
    "component_list.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "component_list =  component_list.rename(columns={'0':'permno'})\n",
    "len(component_list.permno.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data = monthly_stock_data.loc[monthly_stock_data.permno.isin(component_list.permno.unique())]\n",
    "len(monthly_stock_data.permno.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_stock_data\n",
    "# data_to_scale = np.array(monthly_stock_data['fwd_quintile'])\n",
    "# data_to_scale = data_to_scale.reshape(-1,1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(data_to_scale)\n",
    "# monthly_stock_data['scaled_Y'] = scaled_data\n",
    "# monthly_stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Categorization purposes the quintiles need to be shifted by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>mktcap</th>\n",
       "      <th>vweights</th>\n",
       "      <th>ret_mk</th>\n",
       "      <th>monthly_exret</th>\n",
       "      <th>fwd_monthly_exret</th>\n",
       "      <th>quintile</th>\n",
       "      <th>fwd_quintile</th>\n",
       "      <th>fwd_quintile_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-04</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.197605</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.297250e+05</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>-0.113263</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.099415e+05</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.053263</td>\n",
       "      <td>-0.113263</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-06</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>24.375000</td>\n",
       "      <td>0.037234</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.214819e+05</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>-0.314516</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-07</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>-0.335632</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>2.044295e+05</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.049587</td>\n",
       "      <td>-0.314516</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>13242.0</td>\n",
       "      <td>2.184930e+05</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.074085</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>-0.060247</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>294.950012</td>\n",
       "      <td>-0.028375</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.797247e+08</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>-0.055658</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>302.779999</td>\n",
       "      <td>0.026547</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.871505e+08</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>312.549988</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.970275e+08</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.069506</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>311.769989</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.962862e+08</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>-0.031868</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>305.140015</td>\n",
       "      <td>-0.021266</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.899855e+08</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>-0.019232</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>-0.019856</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200393 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          permno         prc       ret    shrout        mktcap  vweights  \\\n",
       "date                                                                       \n",
       "1986-04  10104.0   25.000000  0.197605   13189.0  3.297250e+05  0.000139   \n",
       "1986-05  10104.0   23.500000 -0.060000   13189.0  3.099415e+05  0.000142   \n",
       "1986-06  10104.0   24.375000  0.037234   13189.0  3.214819e+05  0.000096   \n",
       "1986-07  10104.0   15.500000 -0.335632   13189.0  2.044295e+05  0.000096   \n",
       "1986-08  10104.0   16.500000  0.064516   13242.0  2.184930e+05  0.000090   \n",
       "...          ...         ...       ...       ...           ...       ...   \n",
       "2020-06  92655.0  294.950012 -0.028375  948380.0  2.797247e+08  0.008455   \n",
       "2020-07  92655.0  302.779999  0.026547  948380.0  2.871505e+08  0.008236   \n",
       "2020-08  92655.0  312.549988  0.032268  950336.0  2.970275e+08  0.008534   \n",
       "2020-09  92655.0  311.769989  0.001504  950336.0  2.962862e+08  0.008568   \n",
       "2020-10  92655.0  305.140015 -0.021266  950336.0  2.899855e+08  0.008495   \n",
       "\n",
       "           ret_mk  monthly_exret  fwd_monthly_exret  quintile  fwd_quintile  \\\n",
       "date                                                                          \n",
       "1986-04 -0.002530       0.200135          -0.113263         4             4   \n",
       "1986-05  0.053263      -0.113263           0.016762         4             4   \n",
       "1986-06  0.020472       0.016762          -0.314516         4             3   \n",
       "1986-07 -0.049587      -0.314516          -0.009569         3             3   \n",
       "1986-08  0.074085      -0.009569          -0.060247         3             3   \n",
       "...           ...            ...                ...       ...           ...   \n",
       "2020-06  0.027283      -0.055658          -0.033264         5             5   \n",
       "2020-07  0.059811      -0.033264          -0.037238         5             5   \n",
       "2020-08  0.069506      -0.037238           0.033372         5             5   \n",
       "2020-09 -0.031868       0.033372          -0.002034         5             5   \n",
       "2020-10 -0.019232      -0.002034          -0.019856         5             5   \n",
       "\n",
       "         fwd_quintile_adj  \n",
       "date                       \n",
       "1986-04                 3  \n",
       "1986-05                 3  \n",
       "1986-06                 2  \n",
       "1986-07                 2  \n",
       "1986-08                 2  \n",
       "...                   ...  \n",
       "2020-06                 4  \n",
       "2020-07                 4  \n",
       "2020-08                 4  \n",
       "2020-09                 4  \n",
       "2020-10                 4  \n",
       "\n",
       "[200393 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data['fwd_quintile_adj'] = monthly_stock_data['fwd_quintile']-1\n",
    "monthly_stock_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prediction months:  520\n",
      "total number of stocks:  501\n"
     ]
    }
   ],
   "source": [
    "print('total prediction months: ', len(monthly_stock_data.index.unique()))\n",
    "print('total number of stocks: ', len(monthly_stock_data.permno.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stock_data = monthly_stock_data.copy(deep=True)\n",
    "final_factor_data = monthly_factors_adj.copy(deep=True)\n",
    "final_macro_data = macro_factors.copy(deep=True)\n",
    "\n",
    "window =  252*3\n",
    "min_len = 200\n",
    "batch_size = 8\n",
    "n_factors = len(final_factor_data.columns)\n",
    "patience_value = 25\n",
    "\n",
    "start_date = ('1977-01')\n",
    "end_date = ('2020-10')\n",
    "Y_name = 'fwd_quintile_adj'\n",
    "\n",
    "\n",
    "label_names = np.sort(final_stock_data[Y_name].unique())\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all model processes in a file to continue training the model with previous weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local savings version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has:  33 Smart-beta factors,  136 Macro factors,  and 169 Total factors, \n"
     ]
    }
   ],
   "source": [
    "checkpoints_loop = 'checkpoints_loop/'\n",
    "\n",
    "checkpoints_LSTM_model_merged_1L_loop = checkpoints_loop+'checkpoints_LSTM_model_merged_1L_loop.h5'\n",
    "checkpoints_LSTM_model_merged_2L_loop = checkpoints_loop+'checkpoints_LSTM_model_merged_2L_loop.h5'\n",
    "checkpoints_LSTM_model_merged_3L_loop = checkpoints_loop+'checkpoints_LSTM_model_merged_3L_loop.h5'\n",
    "checkpoints_LSTM_model_merged_4L_loop = checkpoints_loop+'checkpoints_LSTM_model_merged_4L_loop.h5'\n",
    "\n",
    "checkpoints_LSTM_model_1L_loop = checkpoints_loop+'checkpoints_LSTM_model_1L_loop.h5'\n",
    "checkpoints_LSTM_model_2L_loop = checkpoints_loop+'checkpoints_LSTM_model_2L_loop.h5'\n",
    "checkpoints_LSTM_model_3L_loop = checkpoints_loop+'checkpoints_LSTM_model_3L_loop.h5'\n",
    "checkpoints_LSTM_model_4L_loop = checkpoints_loop+'checkpoints_LSTM_model_4L_loop.h5'\n",
    "\n",
    "checkpoints_FFN_model_L1_loop = checkpoints_loop+'checkpoints_FFN_model_L1_loop.h5'\n",
    "checkpoints_FFN_model_L2_loop = checkpoints_loop+'checkpoints_FFN_model_L2_loop.h5'\n",
    "checkpoints_FFN_model_L3_loop = checkpoints_loop+'checkpoints_FFN_model_L3_loop.h5'\n",
    "checkpoints_FFN_model_L4_loop = checkpoints_loop+'checkpoints_FFN_model_L4_loop.h5'\n",
    "\n",
    "checkpoints_model_merged_1L_loop = checkpoints_loop+'checkpoints_model_merged_1L_loop.h5'\n",
    "checkpoints_model_merged_2L_loop = checkpoints_loop+'checkpoints_model_merged_2L_loop.h5'\n",
    "checkpoints_model_merged_3L_loop = checkpoints_loop+'checkpoints_model_merged_3L_loop.h5'\n",
    "checkpoints_model_merged_4L_loop = checkpoints_loop+'checkpoints_model_merged_4L_loop.h5'\n",
    "\n",
    "checkpoints_model_1L_loop = checkpoints_loop+'checkpoints_model_1L_loop.h5'\n",
    "checkpoints_model_2L_loop = checkpoints_loop+'checkpoints_model_2L_loop.h5'\n",
    "checkpoints_model_3L_loop = checkpoints_loop+'checkpoints_model_3L_loop.h5'\n",
    "checkpoints_model_4L_loop = checkpoints_loop+'checkpoints_model_4L_loop.h5'\n",
    "\n",
    "\n",
    "\n",
    "number_of_factors = final_factor_data.shape[1]\n",
    "number_of_macro_factors = final_macro_data.shape[1] \n",
    "number_of_merged_factors = number_of_factors+number_of_macro_factors \n",
    "\n",
    "print('The dataset has: ', \\\n",
    "     number_of_factors, 'Smart-beta factors, ', \\\n",
    "     number_of_macro_factors, 'Macro factors, ', 'and',\n",
    "     number_of_merged_factors, 'Total factors, ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation: Tensor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_train(stock_data,macro_data, factor_data, batch_size, train_set_ratio, Y_name):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    permno_list = stock_data.permno.unique()\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    train_data_factors_arr = []\n",
    "    train_data_macro_arr = []\n",
    "    train_data_merged_arr = []\n",
    "    train_data_y_arr = []\n",
    "    \n",
    "    for p in tqdm(permno_list):\n",
    "        \n",
    "        stock_data_length = len(stock_data.loc[stock_data.permno==p])\n",
    "        train_data_stocks = stock_data.loc[stock_data.permno==p][0:int(stock_data_length*train_set_ratio)]\n",
    "        train_date_index = train_data_stocks.index.values.tolist() \n",
    "        \n",
    "        \n",
    "        #train data\n",
    "        train_data_factors = factor_data.loc[factor_data.index.isin(train_date_index)]\n",
    "        train_data_macro = macro_factors.loc[macro_factors.index.isin(train_date_index)]\n",
    "        train_data_macro_norm = train_data_macro.copy(deep=True)\n",
    "        \n",
    "        for c in train_data_macro_norm.columns: \n",
    "            train_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(train_data_macro_norm[c]))\n",
    "        \n",
    "        train_data_merged = pd.concat([train_data_factors, train_data_macro_norm],axis=1)\n",
    "    \n",
    "        \n",
    "        x_train_factors = []\n",
    "        x_train_macro = []\n",
    "        x_train_merged =[]\n",
    "        y_train =[]\n",
    "        \n",
    "\n",
    "        for i in range(batch_size, len(train_data_factors)):\n",
    "            x_train_factors.append(train_data_factors.values[i-batch_size:i,:])\n",
    "            x_train_macro.append(train_data_macro_norm.values[i-batch_size:i,:])\n",
    "            x_train_merged.append(train_data_merged.values[i-batch_size:i,:])\n",
    "            y_train.append(train_data_stocks[Y_name].values[i])   #y_train_encoded_arr[i]\n",
    "  \n",
    "        x_train_factors, x_train_macro, x_train_merged, y_train= np.array(x_train_factors),np.array(x_train_macro),np.array(x_train_merged), np.array(y_train)\n",
    "\n",
    "        train_data_factors_arr.append(x_train_factors)\n",
    "        train_data_macro_arr.append(x_train_macro)\n",
    "        train_data_merged_arr.append(x_train_merged)\n",
    "        train_data_y_arr.append(y_train)\n",
    "\n",
    "        \n",
    "    return train_data_factors_arr, train_data_macro_arr, train_data_merged_arr, train_data_y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [02:07<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_factors, x_train_macro, x_train_merged, y_train= batch_generator_train(final_stock_data,final_macro_data, final_factor_data, batch_size, 0.8, Y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_factors, x_train_macro, x_train_merged, y_train = np.array(x_train_factors, dtype='object'), np.array(x_train_macro, dtype='object'),np.array(x_train_merged, dtype='object'),np.array(y_train, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save train data\n"
     ]
    }
   ],
   "source": [
    "print('save train data')\n",
    "np.savez_compressed('train_arrays_seperate_sp.npz',x_train_factors=x_train_factors,x_train_macro=x_train_macro,x_train_merged=x_train_merged,y_train=y_train)\n",
    "file_saver('train_arrays_seperate_sp.npz','train_arrays_seperate_sp.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_test(stock_data,macro_data, factor_data, batch_size, train_set_ratio, Y_name):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    permno_list = stock_data.permno.unique()\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "\n",
    "    test_data_factors_arr = []\n",
    "    test_data_macro_arr = []\n",
    "    test_data_merged_arr = []\n",
    "    test_data_y_arr =[]\n",
    "    test_data_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    for p in tqdm(permno_list):\n",
    "        \n",
    "        stock_data_length = len(stock_data.loc[stock_data.permno==p])\n",
    "        test_data_stocks = stock_data.loc[stock_data.permno==p][int(stock_data_length*(train_set_ratio)):]\n",
    "        test_data_df = pd.concat([test_data_df,test_data_stocks],axis=0)\n",
    "        test_date_index = test_data_stocks.index.values.tolist()\n",
    "        \n",
    "        #test data \n",
    "        test_data_factors = factor_data.loc[factor_data.index.isin(test_date_index)]\n",
    "        test_data_macro = macro_data.loc[macro_data.index.isin(test_date_index)] \n",
    "        test_data_macro_norm = test_data_macro.copy(deep=True)\n",
    "        \n",
    "        for c in test_data_macro_norm.columns: \n",
    "            test_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(test_data_macro_norm[c]))\n",
    "        \n",
    "        test_data_merged = pd.concat([test_data_factors, test_data_macro_norm],axis=1)\n",
    "        \n",
    "        \n",
    "        x_test_factors = []\n",
    "        x_test_macro = []\n",
    "        x_test_merged =[]\n",
    "        y_test =[]\n",
    "\n",
    "        for i in range(batch_size, len(test_data_factors)):\n",
    "            x_test_factors.append(test_data_factors.values[i-batch_size:i,:])\n",
    "            x_test_macro.append(test_data_macro_norm.values[i-batch_size:i,:])\n",
    "            x_test_merged.append(test_data_merged.values[i-batch_size:i,:])\n",
    "            y_test.append(test_data_stocks[Y_name].values[i])\n",
    "\n",
    "        x_test_factors, x_test_macro,x_test_merged,y_test= np.array(x_test_factors), np.array(x_test_macro),np.array(x_test_merged), np.array(y_test) \n",
    "\n",
    "        test_data_factors_arr.append(x_test_factors)\n",
    "        test_data_macro_arr.append(x_test_macro)\n",
    "        test_data_merged_arr.append(x_test_merged)\n",
    "        test_data_y_arr.append(y_test)\n",
    "                \n",
    "    return test_data_factors_arr,test_data_macro_arr,test_data_merged_arr, test_data_y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [02:02<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test_factors, x_test_macro, x_test_merged, y_test= batch_generator_test(final_stock_data,final_macro_data, final_factor_data, batch_size, 0.8, Y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_factors, x_test_macro, x_test_merged, y_test= np.array(x_test_factors, dtype='object'), np.array(x_test_macro, dtype='object'),np.array(x_test_merged, dtype='object'), np.array(y_test, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save test data\n"
     ]
    }
   ],
   "source": [
    "print('save test data')\n",
    "np.savez_compressed('test_arrays_seperate_sp.npz',x_test_factors=x_test_factors,x_test_macro=x_test_macro,x_test_merged=x_test_merged, y_test=y_test)\n",
    "file_saver('test_arrays_seperate_sp.npz','test_arrays_seperate_sp.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for loop fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.load('train_arrays_seperate_sp.npz', allow_pickle=True)\n",
    "x_train_factors_loop = train_array['x_train_factors']\n",
    "x_train_macro_loop = train_array['x_train_macro']\n",
    "x_train_merged_loop = train_array['x_train_merged']\n",
    "y_train_loop = train_array['y_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_factors_loop.shape\n",
    "x_train_macro_loop.shape\n",
    "x_train_merged_loop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_simple_loop(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, \n",
    "                                batch_size, num_epochs, goal, class_weights_dict):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor = 'val_sparse_categorical_crossentropy',                   #'loss'\n",
    "        patience=patience,\n",
    "        mode='min')\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor= 'val_sparse_categorical_crossentropy',\n",
    "        verbose=False,\n",
    "        save_best_only=True,\n",
    "        save_freq = 'epoch',\n",
    "        mode='min')\n",
    "    \n",
    "    schedular = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-04*10**(epoch/10))\n",
    "    \n",
    "    trained_model = tf.keras.models.load_model(checkpoint_path)\n",
    "    cp_callback = cp_callback\n",
    "            \n",
    "    permno_list = stock_data.permno.unique()\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    if goal =='lr':\n",
    "        for p in range(0,x_train_factors.shape[0]):\n",
    "            print('fitting to: ', p)\n",
    "            #checkpoints \n",
    "\n",
    "            if model_type=='combined':\n",
    "\n",
    "                history = trained_model.fit(x=[x_train_macro[p],x_train_factors[p]],y=y_train[p],batch_size=batch_size, epochs=20,class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[schedular], verbose=0) #\n",
    "\n",
    "            if model_type=='merged':\n",
    "\n",
    "\n",
    "                history = trained_model.fit(x=x_train_merged[p],y=y_train[p],batch_size=batch_size, epochs=20,class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[schedular], verbose=0)  # \n",
    "            if model_type=='factors':\n",
    "\n",
    "                history = trained_model.fit(x=[x_train_factors[p]],y=y_train[p],batch_size=batch_size, epochs=20,class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[schedular], verbose=0)  #           \n",
    "        return trained_model\n",
    "    \n",
    "    if goal=='fit':\n",
    "        for p in range(0,x_train_factors.shape[0]):\n",
    "            print('fitting to: ', p)\n",
    "            #checkpoints \n",
    "\n",
    "            if model_type=='combined':\n",
    "\n",
    "                trained_model.fit(x=[x_train_macro[p],x_train_factors[p]],y=y_train[p],batch_size=batch_size, epochs=num_epochs, class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[early_stopping,cp_callback], verbose=0) #\n",
    "\n",
    "            if model_type=='merged':\n",
    "\n",
    "\n",
    "                trained_model.fit(x=x_train_merged[p],y=y_train[p],batch_size=batch_size, epochs=num_epochs,class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[early_stopping,cp_callback], verbose=0)  # \n",
    "            if model_type=='factors':\n",
    "\n",
    "                trained_model.fit(x=[x_train_factors[p]],y=y_train[p],batch_size=batch_size, epochs=num_epochs,class_weight=class_weights_dict,\n",
    "                                  validation_split=0.09, callbacks=[early_stopping,cp_callback], verbose=0)  #           \n",
    "\n",
    "        trained_model.save(f'{checkpoint_path[:-3]}'+'_final'+'.h5')\n",
    "\n",
    "    return trained_model                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_stocks_loop(stock_data,batch_size,train_set_ratio):\n",
    "    permno_list = stock_data.permno.unique()\n",
    "    test_data_df = pd.DataFrame()\n",
    "    \n",
    "    for p in tqdm(permno_list):\n",
    "        \n",
    "        stock_data_length = len(stock_data.loc[stock_data.permno==p])\n",
    "        test_data_stocks = stock_data.loc[stock_data.permno==p][int(stock_data_length*(train_set_ratio)):]\n",
    "        test_data_df = pd.concat([test_data_df,test_data_stocks],axis=0)\n",
    "    \n",
    "    test_data_df = test_data_df.groupby('permno').apply(lambda x: x.iloc[batch_size:]).reset_index(level=0,drop=True)    \n",
    "    return test_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# def nn_predict_simple(model_type,test_data_stocks, x_test_factors,x_test_macro,x_test_merged, model_path, batch_size, label_names):\n",
    "#     permnos = test_data_stocks.permno.unique()\n",
    "#     permno_df =test_data_stocks[['permno']].copy()\n",
    "#     trained_model = tf.keras.models.load_model(model_path) \n",
    "    \n",
    "#     if model_type =='combined':\n",
    "#         predictions = trained_model.predict(x_test_merged,batch_size)\n",
    "#         predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "#         predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "#         for i in range(len(predictions)):\n",
    "#             predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "            \n",
    "#     elif model_type =='factors':\n",
    "#         predictions = trained_model.predict(x_test_merged,batch_size)\n",
    "#         predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "#         predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "#         for i in range(len(predictions)):\n",
    "#             predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "\n",
    "#     elif model_type =='merged':\n",
    "#         predictions = trained_model.predict(x_test_merged,batch_size)\n",
    "#         predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "#         predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "#         for i in range(len(predictions)):\n",
    "#             predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "            \n",
    "#     predictions_df[f'pred_{model_path[12:]}'] = predictions_df[f'pred_{model_path[12:]}'].values +1\n",
    "#     permno_df = permno_df.rename(columns={0:'date', 1:'permno'}) \n",
    "#     predictions_df.reset_index(inplace=True)\n",
    "#     permno_df.reset_index(inplace=True)\n",
    "#     predictions_df = pd.concat([permno_df,predictions_df],axis=1)\n",
    "#     predictions_df = predictions_df.reset_index(drop=True) \n",
    "#     predictions_df = predictions_df.drop(columns=['index'])\n",
    "\n",
    "#     return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def nn_predict_simple_loop(model_type,test_data_stocks, x_test_factors,x_test_macro,x_test_merged, model_path, batch_size, label_names):\n",
    "    permnos = test_data_stocks.permno.unique()\n",
    "    predicted_returns = pd.DataFrame()\n",
    "    permno_df =test_data_stocks[['permno']].copy()\n",
    "    \n",
    "    trained_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    for p in range(0,(x_test_factors.shape[0])):\n",
    "        print('predicting: ', p)\n",
    "        if model_type =='combined':\n",
    "            predictions = trained_model.predict([x_test_macro[p],x_test_factors[p]],batch_size, verbose=0)\n",
    "            predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "            predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "            for i in range(len(predictions)):\n",
    "                predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "            \n",
    "            predicted_returns = pd.concat([predicted_returns,predictions_df],axis=0)\n",
    "            \n",
    "        elif model_type =='factors':\n",
    "            predictions = trained_model.predict(x_test_factors[p],batch_size, verbose=0)\n",
    "            predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "            predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "            for i in range(len(predictions)):\n",
    "                predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "            \n",
    "            predicted_returns = pd.concat([predicted_returns,predictions_df],axis=0)\n",
    "\n",
    "        elif model_type =='merged':\n",
    "            predictions = trained_model.predict(x_test_merged[p],batch_size, verbose=0)\n",
    "            predictions_df = pd.DataFrame(index=np.arange(len(predictions)))\n",
    "            predictions_df[f'pred_{model_path[12:]}'] = 0\n",
    "    \n",
    "            for i in range(len(predictions)):\n",
    "                predictions_df.iloc[i,0] = label_names[tf.argmax(predictions[i])]\n",
    "            \n",
    "            predicted_returns = pd.concat([predicted_returns,predictions_df],axis=0)\n",
    "\n",
    "    predicted_returns[f'pred_{model_path[12:]}'] = predicted_returns[f'pred_{model_path[12:]}'].values +1\n",
    "    permno_df = permno_df.rename(columns={0:'date', 1:'permno'}) \n",
    "    predicted_returns.reset_index(inplace=True)\n",
    "    permno_df.reset_index(inplace=True)\n",
    "    predicted_returns = pd.concat([permno_df,predicted_returns],axis=1)\n",
    "    predicted_returns = predicted_returns.reset_index(drop=True) \n",
    "    predicted_returns = predicted_returns.drop(columns=['index'])\n",
    "    \n",
    "    return predicted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_oos(true_values, pred_values):\n",
    "    r2 = 1 - ((sum((true_values-pred_values)**2))/(sum(true_values**2)))\n",
    "    return r2 \n",
    "\n",
    "\n",
    "def create_metrics(test_data_df, prediction_df, batch_size, n_factors, r2_oos,dataframe_name, Y_name):\n",
    "    prediction_df = prediction_df.copy()\n",
    "    pred_name = prediction_df.columns.tolist()\n",
    "    pred_name = pred_name[2]\n",
    "    \n",
    "    comparison = test_data_df[Y_name].copy(deep=True).reset_index()\n",
    "    \n",
    "    comparison = pd.concat([comparison, prediction_df.iloc[:,1:]],axis=1)\n",
    "    comparison = comparison[['date','permno', Y_name, pred_name]]\n",
    "    comparison.iloc[:,3] = np.round(comparison.iloc[:,3])\n",
    "    \n",
    "    r2= r2_score(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    r2_oos = r2_oos(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    \n",
    "    r2_adj = 1-(1-r2)*(len(comparison.iloc[:,3])-1)/((len(comparison.iloc[:,3])-n_factors-1))\n",
    "    r2_adj_oos = 1-(1-r2_oos)*(len(comparison.iloc[:,3])-1)/((len(comparison.iloc[:,3])-n_factors-1))\n",
    "    MSE = mean_squared_error(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    rss = np.sum((comparison.iloc[:,3]-comparison.iloc[:,2])**2)\n",
    "\n",
    "    df_metrics =  pd.DataFrame(data=[r2,r2_oos,r2_adj,r2_adj_oos, MSE, rss])\n",
    "    df_metrics=df_metrics.rename(index={0:'r2',1:'r2_oos', 2:'r2_adj', 3:'r2_adj_oos',4:'MSE',5:'rss'},columns={0:dataframe_name})\n",
    "    return comparison, df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(model, x, y, y_pred, sample_weights=1):\n",
    "    compute_loss = model.compute_loss(x,y,y_pred, sample_weights)\n",
    "    \n",
    "    compute_metrics = model.compute_metrics(x,y,y_pred, sample_weights)\n",
    "    \n",
    "    evaluated_model = model.evaluate(x,y,batch_size=32)\n",
    "    \n",
    "    return compute_loss, compute_metrics, evaluated_model\n",
    "\n",
    "def sharpe(r):\n",
    "    return (np.nanmean(r)/np.nanstd(r))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 16:36:52.804819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-28 16:36:52.804855: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-28 16:36:52.804876: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sagemaker-data-scien-ml-r5-2xlarge-2cbef822f8cf2a6152fa7f6847fd): /proc/driver/nvidia/version does not exist\n",
      "2022-08-28 16:36:52.805089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-3/10, momentum=0.9, nesterov=True)\n",
    "sgd_empty =SGD() \n",
    "adam = Adam(0.001)\n",
    "rmsprop = RMSprop(0.0015)\n",
    "\n",
    "config_simple = {'LOSS':[tf.keras.losses.SparseCategoricalCrossentropy()],             \n",
    "          'METRICS':[tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseCategoricalCrossentropy()],    \n",
    "         'OPTIMIZER':adam}  #specify learnig rate after having found optimal value                     #tf.keras.losses.MeanSquaredError() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged Feedforward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_merged_datasets_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 8, 128)            21760     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 128)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8, 5)              645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,405\n",
      "Trainable params: 22,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 8, 128)            21760     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 128)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8, 64)             8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 64)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8, 5)              325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,341\n",
      "Trainable params: 30,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 8, 128)            21760     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8, 128)            0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 8, 64)             8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 8, 64)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 8, 32)             2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 8, 5)              165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,261\n",
      "Trainable params: 32,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 8, 128)            21760     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 8, 128)            0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8, 64)             8256      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 8, 64)             0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8, 32)             2080      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 8, 16)             528       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8, 16)             0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8, 5)              85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,709\n",
      "Trainable params: 32,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_merged_1L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                              layers.Dense(128, activation='leaky_relu' , kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                              layers.Dropout(0.5),\n",
    "                              layers.Dense(5,activation='softmax')],name = 'model_merged_datasets_1L') \n",
    "\n",
    "model_merged_1L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_merged_1L.save(checkpoints_model_merged_1L_loop)\n",
    "model_merged_1L.summary()\n",
    "\n",
    "model_merged_2L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.Dense(128 ,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                                   layers.Dropout(0.2),\n",
    "                                   layers.Dense(64,activation='leaky_relu'),\n",
    "                                   layers.Dropout(0.2),\n",
    "                                   layers.Dense(5,activation='softmax')],name='model_merged_datasets_2L')\n",
    "\n",
    "model_merged_2L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_merged_2L.save(checkpoints_model_merged_2L_loop)\n",
    "model_merged_2L.summary()\n",
    "\n",
    "model_merged_3L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                             layers.Dense(128,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Dense(64,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Dense(32,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Dense(5, activation='softmax')],name='model_merged_datasets_3L')\n",
    "\n",
    "model_merged_3L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_merged_3L.save(checkpoints_model_merged_3L_loop)\n",
    "model_merged_3L.summary()\n",
    "\n",
    "model_merged_4L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                              layers.Dense(128,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(64,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(32,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(16,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(5,activation='softmax' )],name='model_merged_datasets_4L')\n",
    "\n",
    "model_merged_4L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_merged_4L.save(checkpoints_model_merged_4L_loop)\n",
    "model_merged_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Only Feedforward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_factors_only_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 8, 32)             1088      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 8, 5)              165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,253\n",
      "Trainable params: 1,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 8, 32)             1088      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 8, 16)             528       \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 8, 16)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 8, 5)              85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,701\n",
      "Trainable params: 1,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 8, 32)             1088      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 8, 16)             528       \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 8, 16)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 8, 8)              136       \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 8, 8)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 8, 5)              45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,797\n",
      "Trainable params: 1,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 8, 32)             1088      \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 8, 32)             0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 8, 16)             528       \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 8, 16)             0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 8, 8)              136       \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 8, 8)              0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 8, 8)              72        \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 8, 8)              0         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 8, 8)              0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 8, 5)              45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,869\n",
      "Trainable params: 1,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "\n",
    "model_input_macro = layers.Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = layers.Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#################################################### LSTM  models with only factor data ##################################################\n",
    "model_1L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.5),\n",
    "                         layers.Dense(5,activation='softmax')],name='model_factors_only_1L')\n",
    "\n",
    "model_1L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_1L.save(checkpoints_model_1L_loop)\n",
    "model_1L.summary()\n",
    "\n",
    "model_2L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(16,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(5,activation='softmax')],name='model_factors_only_2L')\n",
    "\n",
    "model_2L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_2L.save(checkpoints_model_2L_loop)\n",
    "model_2L.summary()\n",
    "\n",
    "model_3L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(8, activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(5,activation='softmax')],name='model_factors_only_3L')\n",
    "\n",
    "model_3L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_3L.save(checkpoints_model_3L_loop)\n",
    "model_3L.summary()\n",
    "\n",
    "model_4L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(8,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(8,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(5,activation='softmax')],name='model_factors_only_4L')\n",
    "\n",
    "model_4L.compile(loss=config_simple['LOSS'],optimizer=config_simple['OPTIMIZER'],metrics=config_simple['METRICS'])\n",
    "model_4L.save(checkpoints_model_4L_loop)\n",
    "model_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM neural network models:\n",
    "* models using the merged dataset including factors and macro data \n",
    "* models using only factor data as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'LOSS':[tf.keras.losses.SparseCategoricalCrossentropy()],             \n",
    "          'METRICS':[tf.keras.metrics.SparseCategoricalAccuracy(), 'accuracy',tf.keras.metrics.SparseCategoricalCrossentropy()],    \n",
    "         'OPTIMIZER':adam}  #specify learnig rate after having found optimal value                     #tf.keras.losses.MeanSquaredError() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM models with merged data set (macro & factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model_merged_datasets_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization (LayerN  (None, 8, 169)           338       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               152576    \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " layer_normalization_1 (Laye  (None, 64)               128       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163,543\n",
      "Trainable params: 163,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_2 (Laye  (None, 8, 169)           338       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_3 (Laye  (None, 8, 128)           256       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " layer_normalization_4 (Laye  (None, 64)               128       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,951\n",
      "Trainable params: 204,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_5 (Laye  (None, 8, 169)           338       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 8, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_6 (Laye  (None, 8, 128)           256       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 8, 64)             49408     \n",
      "                                                                 \n",
      " layer_normalization_7 (Laye  (None, 8, 64)            128       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " layer_normalization_8 (Laye  (None, 32)               64        \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 215,799\n",
      "Trainable params: 215,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 8, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_9 (Laye  (None, 8, 128)           256       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 8, 64)             49408     \n",
      "                                                                 \n",
      " layer_normalization_10 (Lay  (None, 8, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 8, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_11 (Lay  (None, 8, 32)            64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_12 (Lay  (None, 16)               32        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,197\n",
      "Trainable params: 218,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "LSTM_model_merged_1L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128,  return_sequences =False),\n",
    "                                   layers.Dense(64, activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                                   layers.Dense(5,activation='softmax')],name = 'LSTM_model_merged_datasets_1L') \n",
    "\n",
    "LSTM_model_merged_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_1L.save(checkpoints_LSTM_model_merged_1L_loop)\n",
    "LSTM_model_merged_1L.summary()\n",
    "\n",
    "LSTM_model_merged_2L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64, return_sequences =False),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)), \n",
    "                                   layers.Dense(5,activation='softmax')],name='LSTM_model_merged_datasets_2L')\n",
    "\n",
    "LSTM_model_merged_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_2L.save(checkpoints_LSTM_model_merged_2L_loop)\n",
    "LSTM_model_merged_2L.summary()\n",
    "\n",
    "LSTM_model_merged_3L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(32,  return_sequences = False),\n",
    "                                   layers.LayerNormalization(axis=1),\n",
    "                                   layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                                   layers.Dense(5, activation='softmax')],name='LSTM_model_merged_datasets_3L')\n",
    "\n",
    "LSTM_model_merged_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_3L.save(checkpoints_LSTM_model_merged_3L_loop)\n",
    "LSTM_model_merged_3L.summary()\n",
    "\n",
    "LSTM_model_merged_4L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LSTM(128, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(32,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(16,  return_sequences = False),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                                   layers.Dense(5,activation='softmax' )],name='LSTM_model_merged_datasets_4L')\n",
    "\n",
    "LSTM_model_merged_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_4L.save(checkpoints_LSTM_model_merged_4L_loop)\n",
    "LSTM_model_merged_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM models with only factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model_factors_only_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_13 (Lay  (None, 8, 33)            66        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 64)                25088     \n",
      "                                                                 \n",
      " layer_normalization_14 (Lay  (None, 64)               128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,527\n",
      "Trainable params: 27,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_15 (Lay  (None, 8, 33)            66        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 8, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_16 (Lay  (None, 8, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " layer_normalization_17 (Lay  (None, 32)               64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,375\n",
      "Trainable params: 38,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 8, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_18 (Lay  (None, 8, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 8, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_19 (Lay  (None, 8, 32)            64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_20 (Lay  (None, 16)               32        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,045\n",
      "Trainable params: 41,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (None, 8, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_21 (Lay  (None, 8, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 8, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_22 (Lay  (None, 8, 32)            64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (None, 8, 32)             8320      \n",
      "                                                                 \n",
      " layer_normalization_23 (Lay  (None, 8, 32)            64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_24 (Lay  (None, 16)               32        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 5)                 45        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,429\n",
      "Trainable params: 49,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "\n",
    "model_input_macro = layers.Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = layers.Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#################################################### LSTM  models with only factor data ##################################################\n",
    "LSTM_model_1L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(64, return_sequences =False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dense(5,activation='softmax')],name='LSTM_model_factors_only_1L')\n",
    "\n",
    "LSTM_model_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_1L.save(checkpoints_LSTM_model_1L_loop)\n",
    "LSTM_model_1L.summary()\n",
    "\n",
    "LSTM_model_2L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(16, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dense(5,activation='softmax')],name='LSTM_model_factors_only_2L')\n",
    "\n",
    "LSTM_model_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_2L.save(checkpoints_LSTM_model_2L_loop)\n",
    "LSTM_model_2L.summary()\n",
    "\n",
    "LSTM_model_3L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(16,  return_sequences = False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(8, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dense(5,activation='softmax')],name='LSTM_model_factors_only_3L')\n",
    "\n",
    "LSTM_model_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_3L.save(checkpoints_LSTM_model_3L_loop)\n",
    "LSTM_model_3L.summary()\n",
    "\n",
    "LSTM_model_4L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(16,   return_sequences = False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.001)),\n",
    "                            layers.Dense(5,activation='softmax')],name='LSTM_model_factors_only_4L')\n",
    "\n",
    "LSTM_model_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_4L.save(checkpoints_LSTM_model_4L_loop)\n",
    "LSTM_model_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined models (LSTM transformed macro + LSTM FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model using another macro model to pass input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"one_layer_FFN_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 8, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_22 (InputLayer)          [(None, 8, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " one-layer-macro-model (Functio  ((None, 8, 4),      138064      ['input_21[0][0]']               \n",
      " nal)                            (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 37)        0           ['input_22[0][0]',               \n",
      "                                                                  'one-layer-macro-model[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_22 (LSTM)                 (None, 32)           8960        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_26 (LayerN  (None, 32)          64          ['lstm_22[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 5)            165         ['layer_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 147,253\n",
      "Trainable params: 147,253\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_one_layer_FFN = {'input_units_macro': 128, 'output_units':4, 'outputs':0, 'input_units_FFN_1L':32}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_one_layer_FFN['input_units_macro'], return_sequences =True, return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "LSTM_model_macro_L1_output,LSTM_model_macro_L1_states_h, LSTM_model_macro_L1_states_c = layers.LSTM(specs_one_layer_FFN['output_units'],return_sequences=True, return_state=True)(normalize_1)\n",
    "\n",
    "LSTM_model_macro_1L = Model(model_input_macro, (LSTM_model_macro_L1_output,LSTM_model_macro_L1_states_h), name='one-layer-macro-model')\n",
    "macro_transformed_1L = LSTM_model_macro_1L(model_input_macro)[specs_one_layer_FFN['outputs']] \n",
    "\n",
    "# # macro_transformed_1L = tf.expand_dims(macro_transformed_1L, 1)\n",
    "merged_layers_transformed_1L = layers.Concatenate()([model_input_factors,macro_transformed_1L])\n",
    "\n",
    "#one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_one_layer_FFN['input_units_FFN_1L'],  return_sequences =False, return_state=False)(merged_layers_transformed_1L)\n",
    "normalize_2 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "returns_L1 = layers.Dense(5,activation='softmax')(normalize_2)\n",
    "\n",
    "FFN_model_L1 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L1, name='one_layer_FFN_model')\n",
    "# FFN_model_L1([macro_factors,monthly_factors])\n",
    "\n",
    "\n",
    "FFN_model_L1.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L1.save(checkpoints_FFN_model_L1_loop)\n",
    "FFN_model_L1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"two-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)          [(None, 8, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_24 (InputLayer)          [(None, 8, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " two-layer-macro-model (Functio  ((None, 8, 4),      186576      ['input_23[0][0]']               \n",
      " nal)                            (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 37)        0           ['input_24[0][0]',               \n",
      "                                                                  'two-layer-macro-model[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_26 (LSTM)                 (None, 8, 32)        8960        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_29 (LayerN  (None, 8, 32)       64          ['lstm_26[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_27 (LSTM)                 (None, 16)           3136        ['layer_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_30 (LayerN  (None, 16)          32          ['lstm_27[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 5)            85          ['layer_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 198,853\n",
      "Trainable params: 198,853\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_two_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64, 'output_units':4, 'outputs':0,\n",
    "                       'input_units_FFN_1L':32, 'input_units_FFN_2L':16}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_two_layer_FFN['input_units_macro_1L'],  return_sequences =True,  return_state=False)(model_input_macro)\n",
    "normalize_1= layers.LayerNormalization()(x_macro_L1)\n",
    "x_macro_L2 = layers.LSTM(specs_two_layer_FFN['input_units_macro_2L'], return_sequences =True,  return_state=False)(normalize_1)\n",
    "normalize_2= layers.LayerNormalization()(x_macro_L2)\n",
    "LSTM_model_macro_L2_output,LSTM_model_macro_L2_states_h, LSTM_model_macro_L2_states_c = layers.LSTM(specs_two_layer_FFN['output_units'],return_sequences=True, return_state=True)(normalize_2)\n",
    "\n",
    "LSTM_model_macro_2L = Model(model_input_macro, (LSTM_model_macro_L2_output,LSTM_model_macro_L2_states_h), name='two-layer-macro-model')\n",
    "macro_transformed_2L = LSTM_model_macro_2L(model_input_macro)[specs_two_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_2L = tf.expand_dims(macro_transformed_2L, 1)\n",
    "merged_layers_transformed_2L = layers.Concatenate()([model_input_factors,macro_transformed_2L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_two_layer_FFN['input_units_FFN_1L'],  return_sequences =True, return_state=False)(merged_layers_transformed_2L)\n",
    "normalize_3= layers.LayerNormalization()(LSTM_comb_1L)\n",
    "LSTM_comb_2L = layers.LSTM(specs_two_layer_FFN['input_units_FFN_2L'],  return_sequences =False,  return_state=False)(normalize_3)\n",
    "normalize_4 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "returns_L2 = layers.Dense(5, activation='softmax')(normalize_4)\n",
    "\n",
    "FFN_model_L2 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L2, name='two-layer-FFN-model')\n",
    "\n",
    "FFN_model_L2.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L2.save(checkpoints_FFN_model_L2_loop)\n",
    "FFN_model_L2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"three-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_25 (InputLayer)          [(None, 8, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 8, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " three-layer-macro-model (Funct  ((None, 8, 4),      198544      ['input_25[0][0]']               \n",
      " ional)                          (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8, 37)        0           ['input_26[0][0]',               \n",
      "                                                                  'three-layer-macro-model[0][0]']\n",
      "                                                                                                  \n",
      " lstm_32 (LSTM)                 (None, 8, 32)        8960        ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, 8, 32)       64          ['lstm_32[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_33 (LSTM)                 (None, 8, 16)        3136        ['layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, 8, 16)       32          ['lstm_33[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_34 (LSTM)                 (None, 8)            800         ['layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_36 (LayerN  (None, 8)           16          ['lstm_34[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 5)            45          ['layer_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 211,597\n",
      "Trainable params: 211,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_three_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64,'input_units_macro_3L': 32, \n",
    "                         'output_units':4, 'outputs':0, 'input_units_FFN_1L':32, 'input_units_FFN_2L':16,\n",
    "                         'input_units_FFN_3L':8}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_three_layer_FFN['input_units_macro_1L'],  return_sequences =True,  return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "x_macro_L2 = layers.LSTM(specs_three_layer_FFN['input_units_macro_2L'],  return_sequences =True,  return_state=False)(normalize_1)\n",
    "normalize_2 = layers.LayerNormalization()(x_macro_L2)\n",
    "x_macro_L3 = layers.LSTM(specs_three_layer_FFN['input_units_macro_3L'],  return_sequences =True,  return_state=False)(normalize_2)\n",
    "normalize_3 = layers.LayerNormalization()(x_macro_L3)\n",
    "LSTM_model_macro_L3_output,LSTM_model_macro_L3_states_h, LSTM_model_macro_L3_states_c = layers.LSTM(specs_three_layer_FFN['output_units'],return_sequences=True, return_state=True)(normalize_3)\n",
    "\n",
    "LSTM_model_macro_3L = Model(model_input_macro, (LSTM_model_macro_L3_output,LSTM_model_macro_L3_states_h), name='three-layer-macro-model')\n",
    "macro_transformed_3L = LSTM_model_macro_3L(model_input_macro)[specs_three_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_3L = tf.expand_dims(macro_transformed_3L, 1)\n",
    "merged_layers_transformed_3L = layers.Concatenate()([model_input_factors,macro_transformed_3L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_1L'], return_sequences =True, return_state=False)(merged_layers_transformed_3L)\n",
    "normalize_4 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "LSTM_comb_2L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_2L'], return_sequences =True, return_state=False)(normalize_4)\n",
    "normalize_5 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "LSTM_comb_3L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_3L'], return_sequences =False,  return_state=False)(normalize_5)\n",
    "normalize_6 = layers.LayerNormalization()(LSTM_comb_3L)\n",
    "returns_L3 = layers.Dense(5,activation='softmax')(normalize_6)\n",
    "\n",
    "FFN_model_L3 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L3, name='three-layer-FFN-model')\n",
    "\n",
    "FFN_model_L3.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L3.save(checkpoints_FFN_model_L3_loop)\n",
    "FFN_model_L3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"four-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 8, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_28 (InputLayer)          [(None, 8, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " four-layer-macro-model (Functi  ((None, 8, 4),      201456      ['input_27[0][0]']               \n",
      " onal)                           (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 8, 37)        0           ['input_28[0][0]',               \n",
      "                                                                  'four-layer-macro-model[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_40 (LSTM)                 (None, 8, 32)        8960        ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_41 (LayerN  (None, 8, 32)       64          ['lstm_40[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_41 (LSTM)                 (None, 8, 32)        8320        ['layer_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_42 (LayerN  (None, 8, 32)       64          ['lstm_41[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_42 (LSTM)                 (None, 8, 16)        3136        ['layer_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_43 (LayerN  (None, 8, 16)       32          ['lstm_42[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " lstm_43 (LSTM)                 (None, 8)            800         ['layer_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_44 (LayerN  (None, 8)           16          ['lstm_43[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 5)            45          ['layer_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 222,893\n",
      "Trainable params: 222,893\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_four_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64,'input_units_macro_3L': 32, 'input_units_macro_4L': 16,\n",
    "                        'output_units':4, 'outputs':0, 'input_units_FFN_1L':32, 'input_units_FFN_2L':32,'input_units_FFN_3L':16,\n",
    "                        'input_units_FFN_4L':8}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_four_layer_FFN['input_units_macro_1L'],  return_sequences =True, return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "x_macro_L2 = layers.LSTM(specs_four_layer_FFN['input_units_macro_2L'],  return_sequences =True, return_state=False)(normalize_1)\n",
    "normalize_2 = layers.LayerNormalization()(x_macro_L2)\n",
    "x_macro_L3 = layers.LSTM(specs_four_layer_FFN['input_units_macro_3L'],  return_sequences =True, return_state=False)(normalize_2)\n",
    "normalize_3 = layers.LayerNormalization()(x_macro_L3)\n",
    "x_macro_L4 = layers.LSTM(specs_four_layer_FFN['input_units_macro_4L'],  return_sequences =True, return_state=False)(normalize_3)\n",
    "normalize_4 = layers.LayerNormalization()(x_macro_L4)\n",
    "LSTM_model_macro_L4_output,LSTM_model_macro_L4_states_h, LSTM_model_macro_L4_states_c = layers.LSTM(specs_four_layer_FFN['output_units'],return_sequences=True, return_state=True)(normalize_4)\n",
    "\n",
    "LSTM_model_macro_4L = Model(model_input_macro, (LSTM_model_macro_L4_output,LSTM_model_macro_L4_states_h), name='four-layer-macro-model')\n",
    "macro_transformed_4L = LSTM_model_macro_4L(model_input_macro)[specs_four_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_4L = tf.expand_dims(macro_transformed_4L, 1)\n",
    "merged_layers_transformed_4L = layers.Concatenate()([model_input_factors,macro_transformed_4L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_1L'], return_sequences =True, return_state=False)(merged_layers_transformed_4L)\n",
    "normalize_5 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "LSTM_comb_2L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_2L'],  return_sequences =True, return_state=False)(normalize_5)\n",
    "normalize_6 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "LSTM_comb_3L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_3L'],  return_sequences =True, return_state=False)(normalize_6)\n",
    "normalize_7 = layers.LayerNormalization()(LSTM_comb_3L)\n",
    "LSTM_comb_4L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_4L'],  return_sequences =False,return_state=False)(normalize_7)\n",
    "normalize_8 = layers.LayerNormalization()(LSTM_comb_4L)\n",
    "returns_L4 = layers.Dense(5, activation='softmax')(normalize_8)\n",
    "\n",
    "FFN_model_L4 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L4, name='four-layer-FFN-model')\n",
    "\n",
    "FFN_model_L4.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L4.save(checkpoints_FFN_model_L4_loop)\n",
    "FFN_model_L4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data issues with multi classification\n",
    "\n",
    "Taking a look at the graph below, it can be noticed that the data set is highly imbalanced. Therefore each label class receives a weight to count the imbalance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f31cb161e50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(final_stock_data[Y_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=date\n",
      "1986-04    3\n",
      "1986-05    3\n",
      "1986-06    2\n",
      "1986-07    2\n",
      "1986-08    2\n",
      "          ..\n",
      "2020-06    4\n",
      "2020-07    4\n",
      "2020-08    4\n",
      "2020-09    4\n",
      "2020-10    4\n",
      "Name: fwd_quintile_adj, Length: 200393, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 17.68693733451015,\n",
       " 1: 4.343151278716949,\n",
       " 2: 1.85781300699949,\n",
       " 3: 0.7196215032139908,\n",
       " 4: 0.359024294109216}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights= class_weight.compute_class_weight('balanced', np.sort(final_stock_data[Y_name].unique()), final_stock_data[Y_name])\n",
    "class_weights_dict  ={0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3],4:class_weights[4]}\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find optimal adam learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"def compile_and_fit_simple(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, batch_size, num_epochs, goal):\"\"\"\n",
    "\n",
    "\n",
    "# model_history_LSTM_model_merged_1L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_1L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  2, batch_size, \n",
    "#                     100, 'lr', class_weights_dict)\n",
    "# model_history_LSTM_model_merged_2L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_2L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, 2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)\n",
    "# model_history_LSTM_model_merged_3L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_3L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, 2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)\n",
    "# model_history_LSTM_model_merged_4L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_4L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = 1e-4*(10**(tf.range(20)/20))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.semilogx(lrs, model_history_LSTM_model_merged_1L.history['loss'])\n",
    "# plt.xlabel('learningrate')\n",
    "# plt.ylable('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Feedforward models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Feedforward NNs looped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"compile_and_fit_simple_loop(model_type,checkpoint_path,config,final_stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, x_val_factors,x_val_macro,x_val_merged,y_val, patience, \n",
    "                                batch_size, num_epochs, Y_name):\"\"\"\n",
    "\n",
    "\n",
    "model_history_model_merged_1L_loop = compile_and_fit_simple_loop('merged',checkpoints_model_merged_1L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop, patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_model_merged_2L_loop = compile_and_fit_simple_loop('merged',checkpoints_model_merged_2L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_model_merged_3L_loop = compile_and_fit_simple_loop('merged',checkpoints_model_merged_3L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_model_merged_4L_loop = compile_and_fit_simple_loop('merged',checkpoints_model_merged_4L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_model_merged_1L_loop_final.h5', 'checkpoints_model_merged_1L_loop_final.h5')\n",
    "file_saver('checkpoints_model_merged_2L_loop_final.h5', 'checkpoints_model_merged_2L_loop_final.h5')\n",
    "file_saver('checkpoints_model_merged_3L_loop_final.h5', 'checkpoints_model_merged_3L_loop_final.h5')\n",
    "file_saver('checkpoints_model_merged_4L_loop_final.h5', 'checkpoints_model_merged_4L_loop_final.h5')\n",
    "\n",
    "file_saver('checkpoints_model_merged_1L_loop.h5', 'checkpoints_model_merged_1L_loop.h5')\n",
    "file_saver('checkpoints_model_merged_2L_loop.h5', 'checkpoints_model_merged_2L_loop.h5')\n",
    "file_saver('checkpoints_model_merged_3L_loop.h5', 'checkpoints_model_merged_3L_loop.h5')\n",
    "file_saver('checkpoints_model_merged_4L_loop.h5', 'checkpoints_model_merged_4L_loop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Feedforward NNs looped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nn_predict_simple_loop(model_type,test_data_stocks, x_test_factors,x_test_macro,x_test_merged, model_path, batch_size, label_names)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history_model_1L_loop= compile_and_fit_simple_loop('factors',checkpoints_model_1L_loop,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_model_2L_loop = compile_and_fit_simple_loop('factors',checkpoints_model_2L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop, patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_model_3L_loop = compile_and_fit_simple_loop('factors',checkpoints_model_3L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_model_4L_loop = compile_and_fit_simple_loop('factors',checkpoints_model_4L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop, patience_value, batch_size, \n",
    "                    100,'fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_model_1L_loop_final.h5', 'checkpoints_model_1L_loop_final.h5')\n",
    "file_saver('checkpoints_model_1L_loop_final.h5', 'checkpoints_model_1L_loop_final.h5')\n",
    "file_saver('checkpoints_model_1L_loop_final.h5', 'checkpoints_model_1L_loop_final.h5')\n",
    "file_saver('checkpoints_model_1L_loop_final.h5', 'checkpoints_model_1L_loop_final.h5')\n",
    "\n",
    "file_saver('checkpoints_model_1L_loop.h5', 'checkpoints_model_1L_loop.h5')\n",
    "file_saver('checkpoints_model_1L_loop.h5', 'checkpoints_model_1L_loop.h5')\n",
    "file_saver('checkpoints_model_1L_loop.h5', 'checkpoints_model_1L_loop.h5')\n",
    "file_saver('checkpoints_model_1L_loop.h5', 'checkpoints_model_1L_loop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM merged looped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting to:  0\n",
      "fitting to:  1\n",
      "fitting to:  2\n",
      "fitting to:  3\n",
      "fitting to:  4\n",
      "fitting to:  5\n",
      "fitting to:  6\n",
      "fitting to:  7\n",
      "fitting to:  8\n",
      "fitting to:  9\n",
      "fitting to:  10\n",
      "fitting to:  11\n",
      "fitting to:  12\n",
      "fitting to:  13\n",
      "fitting to:  14\n",
      "fitting to:  15\n",
      "fitting to:  16\n",
      "fitting to:  17\n",
      "fitting to:  18\n",
      "fitting to:  19\n",
      "fitting to:  20\n",
      "fitting to:  21\n",
      "fitting to:  22\n",
      "fitting to:  23\n",
      "fitting to:  24\n",
      "fitting to:  25\n",
      "fitting to:  26\n",
      "fitting to:  27\n",
      "fitting to:  28\n",
      "fitting to:  29\n",
      "fitting to:  30\n",
      "fitting to:  31\n",
      "fitting to:  32\n",
      "fitting to:  33\n",
      "fitting to:  34\n",
      "fitting to:  35\n",
      "fitting to:  36\n",
      "fitting to:  37\n",
      "fitting to:  38\n",
      "fitting to:  39\n",
      "fitting to:  40\n",
      "fitting to:  41\n",
      "fitting to:  42\n",
      "fitting to:  43\n",
      "fitting to:  44\n",
      "fitting to:  45\n",
      "fitting to:  46\n",
      "fitting to:  47\n",
      "fitting to:  48\n",
      "fitting to:  49\n",
      "fitting to:  50\n",
      "fitting to:  51\n",
      "fitting to:  52\n",
      "fitting to:  53\n",
      "fitting to:  54\n",
      "fitting to:  55\n",
      "fitting to:  56\n",
      "fitting to:  57\n",
      "fitting to:  58\n",
      "fitting to:  59\n",
      "fitting to:  60\n",
      "fitting to:  61\n",
      "fitting to:  62\n",
      "fitting to:  63\n",
      "fitting to:  64\n",
      "fitting to:  65\n",
      "fitting to:  66\n",
      "fitting to:  67\n",
      "fitting to:  68\n",
      "fitting to:  69\n",
      "fitting to:  70\n",
      "fitting to:  71\n",
      "fitting to:  72\n",
      "fitting to:  73\n",
      "fitting to:  74\n",
      "fitting to:  75\n",
      "fitting to:  76\n",
      "fitting to:  77\n",
      "fitting to:  78\n",
      "fitting to:  79\n",
      "fitting to:  80\n",
      "fitting to:  81\n",
      "fitting to:  82\n",
      "fitting to:  83\n",
      "fitting to:  84\n",
      "fitting to:  85\n",
      "fitting to:  86\n",
      "fitting to:  87\n",
      "fitting to:  88\n",
      "fitting to:  89\n",
      "fitting to:  90\n",
      "fitting to:  91\n",
      "fitting to:  92\n",
      "fitting to:  93\n",
      "fitting to:  94\n",
      "fitting to:  95\n",
      "fitting to:  96\n",
      "fitting to:  97\n",
      "fitting to:  98\n",
      "fitting to:  99\n",
      "fitting to:  100\n",
      "fitting to:  101\n",
      "fitting to:  102\n",
      "fitting to:  103\n",
      "fitting to:  104\n",
      "fitting to:  105\n",
      "fitting to:  106\n",
      "fitting to:  107\n",
      "fitting to:  108\n",
      "fitting to:  109\n",
      "fitting to:  110\n",
      "fitting to:  111\n",
      "fitting to:  112\n",
      "fitting to:  113\n",
      "fitting to:  114\n",
      "fitting to:  115\n",
      "fitting to:  116\n",
      "fitting to:  117\n",
      "fitting to:  118\n",
      "fitting to:  119\n",
      "fitting to:  120\n",
      "fitting to:  121\n",
      "fitting to:  122\n",
      "fitting to:  123\n",
      "fitting to:  124\n",
      "fitting to:  125\n",
      "fitting to:  126\n",
      "fitting to:  127\n",
      "fitting to:  128\n",
      "fitting to:  129\n",
      "fitting to:  130\n",
      "fitting to:  131\n",
      "fitting to:  132\n",
      "fitting to:  133\n",
      "fitting to:  134\n",
      "fitting to:  135\n",
      "fitting to:  136\n",
      "fitting to:  137\n",
      "fitting to:  138\n",
      "fitting to:  139\n",
      "fitting to:  140\n",
      "fitting to:  141\n",
      "fitting to:  142\n",
      "fitting to:  143\n",
      "fitting to:  144\n",
      "fitting to:  145\n",
      "fitting to:  146\n",
      "fitting to:  147\n",
      "fitting to:  148\n",
      "fitting to:  149\n",
      "fitting to:  150\n",
      "fitting to:  151\n",
      "fitting to:  152\n",
      "fitting to:  153\n",
      "fitting to:  154\n",
      "fitting to:  155\n",
      "fitting to:  156\n",
      "fitting to:  157\n",
      "fitting to:  158\n",
      "fitting to:  159\n",
      "fitting to:  160\n",
      "fitting to:  161\n",
      "fitting to:  162\n",
      "fitting to:  163\n",
      "fitting to:  164\n",
      "fitting to:  165\n",
      "fitting to:  166\n",
      "fitting to:  167\n",
      "fitting to:  168\n",
      "fitting to:  169\n",
      "fitting to:  170\n",
      "fitting to:  171\n",
      "fitting to:  172\n",
      "fitting to:  173\n",
      "fitting to:  174\n",
      "fitting to:  175\n",
      "fitting to:  176\n",
      "fitting to:  177\n",
      "fitting to:  178\n",
      "fitting to:  179\n",
      "fitting to:  180\n",
      "fitting to:  181\n",
      "fitting to:  182\n",
      "fitting to:  183\n",
      "fitting to:  184\n",
      "fitting to:  185\n",
      "fitting to:  186\n",
      "fitting to:  187\n",
      "fitting to:  188\n",
      "fitting to:  189\n",
      "fitting to:  190\n",
      "fitting to:  191\n",
      "fitting to:  192\n",
      "fitting to:  193\n",
      "fitting to:  194\n",
      "fitting to:  195\n",
      "fitting to:  196\n",
      "fitting to:  197\n",
      "fitting to:  198\n",
      "fitting to:  199\n",
      "fitting to:  200\n",
      "fitting to:  201\n",
      "fitting to:  202\n",
      "fitting to:  203\n",
      "fitting to:  204\n",
      "fitting to:  205\n",
      "fitting to:  206\n",
      "fitting to:  207\n",
      "fitting to:  208\n",
      "fitting to:  209\n",
      "fitting to:  210\n",
      "fitting to:  211\n",
      "fitting to:  212\n",
      "fitting to:  213\n",
      "fitting to:  214\n",
      "fitting to:  215\n",
      "fitting to:  216\n",
      "fitting to:  217\n",
      "fitting to:  218\n",
      "fitting to:  219\n",
      "fitting to:  220\n",
      "fitting to:  221\n",
      "fitting to:  222\n",
      "fitting to:  223\n",
      "fitting to:  224\n",
      "fitting to:  225\n",
      "fitting to:  226\n",
      "fitting to:  227\n",
      "fitting to:  228\n",
      "fitting to:  229\n",
      "fitting to:  230\n",
      "fitting to:  231\n",
      "fitting to:  232\n",
      "fitting to:  233\n",
      "fitting to:  234\n",
      "fitting to:  235\n",
      "fitting to:  236\n",
      "fitting to:  237\n",
      "fitting to:  238\n",
      "fitting to:  239\n",
      "fitting to:  240\n",
      "fitting to:  241\n",
      "fitting to:  242\n",
      "fitting to:  243\n",
      "fitting to:  244\n",
      "fitting to:  245\n",
      "fitting to:  246\n",
      "fitting to:  247\n",
      "fitting to:  248\n",
      "fitting to:  249\n",
      "fitting to:  250\n",
      "fitting to:  251\n",
      "fitting to:  252\n",
      "fitting to:  253\n",
      "fitting to:  254\n",
      "fitting to:  255\n",
      "fitting to:  256\n",
      "fitting to:  257\n",
      "fitting to:  258\n",
      "fitting to:  259\n",
      "fitting to:  260\n",
      "fitting to:  261\n",
      "fitting to:  262\n",
      "fitting to:  263\n",
      "fitting to:  264\n",
      "fitting to:  265\n",
      "fitting to:  266\n",
      "fitting to:  267\n",
      "fitting to:  268\n",
      "fitting to:  269\n",
      "fitting to:  270\n",
      "fitting to:  271\n",
      "fitting to:  272\n",
      "fitting to:  273\n",
      "fitting to:  274\n",
      "fitting to:  275\n",
      "fitting to:  276\n",
      "fitting to:  277\n",
      "fitting to:  278\n",
      "fitting to:  279\n",
      "fitting to:  280\n",
      "fitting to:  281\n",
      "fitting to:  282\n",
      "fitting to:  283\n",
      "fitting to:  284\n",
      "fitting to:  285\n",
      "fitting to:  286\n",
      "fitting to:  287\n",
      "fitting to:  288\n",
      "fitting to:  289\n",
      "fitting to:  290\n",
      "fitting to:  291\n",
      "fitting to:  292\n",
      "fitting to:  293\n",
      "fitting to:  294\n",
      "fitting to:  295\n",
      "fitting to:  296\n",
      "fitting to:  297\n",
      "fitting to:  298\n",
      "fitting to:  299\n",
      "fitting to:  300\n",
      "fitting to:  301\n",
      "fitting to:  302\n",
      "fitting to:  303\n",
      "fitting to:  304\n",
      "fitting to:  305\n",
      "fitting to:  306\n",
      "fitting to:  307\n",
      "fitting to:  308\n",
      "fitting to:  309\n",
      "fitting to:  310\n",
      "fitting to:  311\n",
      "fitting to:  312\n",
      "fitting to:  313\n",
      "fitting to:  314\n",
      "fitting to:  315\n",
      "fitting to:  316\n",
      "fitting to:  317\n",
      "fitting to:  318\n",
      "fitting to:  319\n",
      "fitting to:  320\n",
      "fitting to:  321\n",
      "fitting to:  322\n",
      "fitting to:  323\n",
      "fitting to:  324\n",
      "fitting to:  325\n",
      "fitting to:  326\n",
      "fitting to:  327\n",
      "fitting to:  328\n",
      "fitting to:  329\n",
      "fitting to:  330\n",
      "fitting to:  331\n",
      "fitting to:  332\n",
      "fitting to:  333\n",
      "fitting to:  334\n",
      "fitting to:  335\n",
      "fitting to:  336\n",
      "fitting to:  337\n",
      "fitting to:  338\n",
      "fitting to:  339\n",
      "fitting to:  340\n",
      "fitting to:  341\n",
      "fitting to:  342\n",
      "fitting to:  343\n",
      "fitting to:  344\n",
      "fitting to:  345\n",
      "fitting to:  346\n",
      "fitting to:  347\n",
      "fitting to:  348\n",
      "fitting to:  349\n",
      "fitting to:  350\n",
      "fitting to:  351\n",
      "fitting to:  352\n",
      "fitting to:  353\n",
      "fitting to:  354\n",
      "fitting to:  355\n",
      "fitting to:  356\n",
      "fitting to:  357\n",
      "fitting to:  358\n",
      "fitting to:  359\n",
      "fitting to:  360\n",
      "fitting to:  361\n",
      "fitting to:  362\n",
      "fitting to:  363\n",
      "fitting to:  364\n",
      "fitting to:  365\n",
      "fitting to:  366\n",
      "fitting to:  367\n",
      "fitting to:  368\n",
      "fitting to:  369\n",
      "fitting to:  370\n",
      "fitting to:  371\n",
      "fitting to:  372\n",
      "fitting to:  373\n",
      "fitting to:  374\n",
      "fitting to:  375\n",
      "fitting to:  376\n",
      "fitting to:  377\n",
      "fitting to:  378\n",
      "fitting to:  379\n",
      "fitting to:  380\n",
      "fitting to:  381\n",
      "fitting to:  382\n",
      "fitting to:  383\n",
      "fitting to:  384\n",
      "fitting to:  385\n",
      "fitting to:  386\n",
      "fitting to:  387\n",
      "fitting to:  388\n",
      "fitting to:  389\n",
      "fitting to:  390\n",
      "fitting to:  391\n",
      "fitting to:  392\n",
      "fitting to:  393\n",
      "fitting to:  394\n",
      "fitting to:  395\n",
      "fitting to:  396\n",
      "fitting to:  397\n",
      "fitting to:  398\n",
      "fitting to:  399\n",
      "fitting to:  400\n",
      "fitting to:  401\n",
      "fitting to:  402\n",
      "fitting to:  403\n",
      "fitting to:  404\n",
      "fitting to:  405\n",
      "fitting to:  406\n",
      "fitting to:  407\n",
      "fitting to:  408\n",
      "fitting to:  409\n",
      "fitting to:  410\n",
      "fitting to:  411\n",
      "fitting to:  412\n",
      "fitting to:  413\n",
      "fitting to:  414\n",
      "fitting to:  415\n",
      "fitting to:  416\n",
      "fitting to:  417\n",
      "fitting to:  418\n",
      "fitting to:  419\n",
      "fitting to:  420\n",
      "fitting to:  421\n",
      "fitting to:  422\n",
      "fitting to:  423\n",
      "fitting to:  424\n",
      "fitting to:  425\n",
      "fitting to:  426\n",
      "fitting to:  427\n",
      "fitting to:  428\n",
      "fitting to:  429\n",
      "fitting to:  430\n",
      "fitting to:  431\n",
      "fitting to:  432\n",
      "fitting to:  433\n",
      "fitting to:  434\n",
      "fitting to:  435\n",
      "fitting to:  436\n",
      "fitting to:  437\n",
      "fitting to:  438\n",
      "fitting to:  439\n",
      "fitting to:  440\n",
      "fitting to:  441\n",
      "fitting to:  442\n",
      "fitting to:  443\n",
      "fitting to:  444\n",
      "fitting to:  445\n",
      "fitting to:  446\n",
      "fitting to:  447\n",
      "fitting to:  448\n",
      "fitting to:  449\n",
      "fitting to:  450\n",
      "fitting to:  451\n",
      "fitting to:  452\n",
      "fitting to:  453\n",
      "fitting to:  454\n",
      "fitting to:  455\n",
      "fitting to:  456\n",
      "fitting to:  457\n",
      "fitting to:  458\n",
      "fitting to:  459\n",
      "fitting to:  460\n",
      "fitting to:  461\n",
      "fitting to:  462\n",
      "fitting to:  463\n",
      "fitting to:  464\n",
      "fitting to:  465\n",
      "fitting to:  466\n",
      "fitting to:  467\n",
      "fitting to:  468\n",
      "fitting to:  469\n",
      "fitting to:  470\n",
      "fitting to:  471\n",
      "fitting to:  472\n",
      "fitting to:  473\n",
      "fitting to:  474\n",
      "fitting to:  475\n",
      "fitting to:  476\n",
      "fitting to:  477\n",
      "fitting to:  478\n",
      "fitting to:  479\n",
      "fitting to:  480\n",
      "fitting to:  481\n",
      "fitting to:  482\n",
      "fitting to:  483\n",
      "fitting to:  484\n",
      "fitting to:  485\n",
      "fitting to:  486\n",
      "fitting to:  487\n",
      "fitting to:  488\n",
      "fitting to:  489\n",
      "fitting to:  490\n",
      "fitting to:  491\n",
      "fitting to:  492\n",
      "fitting to:  493\n",
      "fitting to:  494\n",
      "fitting to:  495\n",
      "fitting to:  496\n",
      "fitting to:  497\n",
      "fitting to:  498\n",
      "fitting to:  499\n",
      "fitting to:  500\n",
      "fitting to:  0\n",
      "fitting to:  1\n",
      "fitting to:  2\n",
      "fitting to:  3\n",
      "fitting to:  4\n",
      "fitting to:  5\n",
      "fitting to:  6\n",
      "fitting to:  7\n",
      "fitting to:  8\n",
      "fitting to:  9\n",
      "fitting to:  10\n",
      "fitting to:  11\n",
      "fitting to:  12\n",
      "fitting to:  13\n",
      "fitting to:  14\n",
      "fitting to:  15\n",
      "fitting to:  16\n",
      "fitting to:  17\n",
      "fitting to:  18\n",
      "fitting to:  19\n",
      "fitting to:  20\n",
      "fitting to:  21\n",
      "fitting to:  22\n",
      "fitting to:  23\n",
      "fitting to:  24\n",
      "fitting to:  25\n",
      "fitting to:  26\n",
      "fitting to:  27\n",
      "fitting to:  28\n",
      "fitting to:  29\n",
      "fitting to:  30\n",
      "fitting to:  31\n",
      "fitting to:  32\n",
      "fitting to:  33\n",
      "fitting to:  34\n",
      "fitting to:  35\n",
      "fitting to:  36\n",
      "fitting to:  37\n",
      "fitting to:  38\n",
      "fitting to:  39\n",
      "fitting to:  40\n",
      "fitting to:  41\n",
      "fitting to:  42\n",
      "fitting to:  43\n",
      "fitting to:  44\n",
      "fitting to:  45\n",
      "fitting to:  46\n",
      "fitting to:  47\n",
      "fitting to:  48\n",
      "fitting to:  49\n",
      "fitting to:  50\n",
      "fitting to:  51\n",
      "fitting to:  52\n",
      "fitting to:  53\n",
      "fitting to:  54\n",
      "fitting to:  55\n",
      "fitting to:  56\n",
      "fitting to:  57\n",
      "fitting to:  58\n",
      "fitting to:  59\n",
      "fitting to:  60\n",
      "fitting to:  61\n",
      "fitting to:  62\n",
      "fitting to:  63\n",
      "fitting to:  64\n",
      "fitting to:  65\n",
      "fitting to:  66\n",
      "fitting to:  67\n",
      "fitting to:  68\n",
      "fitting to:  69\n",
      "fitting to:  70\n",
      "fitting to:  71\n",
      "fitting to:  72\n",
      "fitting to:  73\n",
      "fitting to:  74\n",
      "fitting to:  75\n",
      "fitting to:  76\n",
      "fitting to:  77\n",
      "fitting to:  78\n",
      "fitting to:  79\n",
      "fitting to:  80\n",
      "fitting to:  81\n",
      "fitting to:  82\n",
      "fitting to:  83\n",
      "fitting to:  84\n",
      "fitting to:  85\n",
      "fitting to:  86\n",
      "fitting to:  87\n",
      "fitting to:  88\n",
      "fitting to:  89\n",
      "fitting to:  90\n",
      "fitting to:  91\n",
      "fitting to:  92\n",
      "fitting to:  93\n",
      "fitting to:  94\n",
      "fitting to:  95\n",
      "fitting to:  96\n",
      "fitting to:  97\n",
      "fitting to:  98\n",
      "fitting to:  99\n",
      "fitting to:  100\n",
      "fitting to:  101\n",
      "fitting to:  102\n",
      "fitting to:  103\n",
      "fitting to:  104\n",
      "fitting to:  105\n",
      "fitting to:  106\n",
      "fitting to:  107\n",
      "fitting to:  108\n",
      "fitting to:  109\n",
      "fitting to:  110\n",
      "fitting to:  111\n",
      "fitting to:  112\n",
      "fitting to:  113\n",
      "fitting to:  114\n",
      "fitting to:  115\n",
      "fitting to:  116\n",
      "fitting to:  117\n",
      "fitting to:  118\n",
      "fitting to:  119\n",
      "fitting to:  120\n",
      "fitting to:  121\n",
      "fitting to:  122\n",
      "fitting to:  123\n",
      "fitting to:  124\n",
      "fitting to:  125\n",
      "fitting to:  126\n",
      "fitting to:  127\n",
      "fitting to:  128\n",
      "fitting to:  129\n",
      "fitting to:  130\n",
      "fitting to:  131\n",
      "fitting to:  132\n",
      "fitting to:  133\n",
      "fitting to:  134\n",
      "fitting to:  135\n",
      "fitting to:  136\n",
      "fitting to:  137\n",
      "fitting to:  138\n",
      "fitting to:  139\n",
      "fitting to:  140\n",
      "fitting to:  141\n",
      "fitting to:  142\n",
      "fitting to:  143\n",
      "fitting to:  144\n",
      "fitting to:  145\n",
      "fitting to:  146\n",
      "fitting to:  147\n",
      "fitting to:  148\n",
      "fitting to:  149\n",
      "fitting to:  150\n",
      "fitting to:  151\n",
      "fitting to:  152\n",
      "fitting to:  153\n",
      "fitting to:  154\n",
      "fitting to:  155\n",
      "fitting to:  156\n",
      "fitting to:  157\n",
      "fitting to:  158\n",
      "fitting to:  159\n",
      "fitting to:  160\n",
      "fitting to:  161\n",
      "fitting to:  162\n",
      "fitting to:  163\n",
      "fitting to:  164\n",
      "fitting to:  165\n",
      "fitting to:  166\n",
      "fitting to:  167\n",
      "fitting to:  168\n",
      "fitting to:  169\n",
      "fitting to:  170\n",
      "fitting to:  171\n",
      "fitting to:  172\n",
      "fitting to:  173\n",
      "fitting to:  174\n",
      "fitting to:  175\n",
      "fitting to:  176\n",
      "fitting to:  177\n",
      "fitting to:  178\n",
      "fitting to:  179\n",
      "fitting to:  180\n",
      "fitting to:  181\n",
      "fitting to:  182\n",
      "fitting to:  183\n",
      "fitting to:  184\n",
      "fitting to:  185\n",
      "fitting to:  186\n",
      "fitting to:  187\n",
      "fitting to:  188\n",
      "fitting to:  189\n",
      "fitting to:  190\n",
      "fitting to:  191\n",
      "fitting to:  192\n",
      "fitting to:  193\n",
      "fitting to:  194\n",
      "fitting to:  195\n",
      "fitting to:  196\n",
      "fitting to:  197\n",
      "fitting to:  198\n",
      "fitting to:  199\n",
      "fitting to:  200\n",
      "fitting to:  201\n",
      "fitting to:  202\n",
      "fitting to:  203\n",
      "fitting to:  204\n",
      "fitting to:  205\n",
      "fitting to:  206\n",
      "fitting to:  207\n",
      "fitting to:  208\n",
      "fitting to:  209\n",
      "fitting to:  210\n",
      "fitting to:  211\n",
      "fitting to:  212\n",
      "fitting to:  213\n",
      "fitting to:  214\n",
      "fitting to:  215\n",
      "fitting to:  216\n",
      "fitting to:  217\n",
      "fitting to:  218\n",
      "fitting to:  219\n",
      "fitting to:  220\n",
      "fitting to:  221\n",
      "fitting to:  222\n",
      "fitting to:  223\n",
      "fitting to:  224\n",
      "fitting to:  225\n",
      "fitting to:  226\n",
      "fitting to:  227\n",
      "fitting to:  228\n",
      "fitting to:  229\n",
      "fitting to:  230\n",
      "fitting to:  231\n",
      "fitting to:  232\n",
      "fitting to:  233\n",
      "fitting to:  234\n",
      "fitting to:  235\n",
      "fitting to:  236\n",
      "fitting to:  237\n",
      "fitting to:  238\n",
      "fitting to:  239\n",
      "fitting to:  240\n",
      "fitting to:  241\n",
      "fitting to:  242\n",
      "fitting to:  243\n",
      "fitting to:  244\n",
      "fitting to:  245\n",
      "fitting to:  246\n",
      "fitting to:  247\n",
      "fitting to:  248\n",
      "fitting to:  249\n",
      "fitting to:  250\n",
      "fitting to:  251\n",
      "fitting to:  252\n",
      "fitting to:  253\n",
      "fitting to:  254\n",
      "fitting to:  255\n",
      "fitting to:  256\n",
      "fitting to:  257\n",
      "fitting to:  258\n",
      "fitting to:  259\n",
      "fitting to:  260\n",
      "fitting to:  261\n",
      "fitting to:  262\n",
      "fitting to:  263\n",
      "fitting to:  264\n",
      "fitting to:  265\n",
      "fitting to:  266\n",
      "fitting to:  267\n",
      "fitting to:  268\n",
      "fitting to:  269\n",
      "fitting to:  270\n",
      "fitting to:  271\n",
      "fitting to:  272\n",
      "fitting to:  273\n",
      "fitting to:  274\n",
      "fitting to:  275\n",
      "fitting to:  276\n",
      "fitting to:  277\n",
      "fitting to:  278\n",
      "fitting to:  279\n",
      "fitting to:  280\n",
      "fitting to:  281\n",
      "fitting to:  282\n",
      "fitting to:  283\n",
      "fitting to:  284\n",
      "fitting to:  285\n",
      "fitting to:  286\n",
      "fitting to:  287\n",
      "fitting to:  288\n",
      "fitting to:  289\n",
      "fitting to:  290\n",
      "fitting to:  291\n",
      "fitting to:  292\n",
      "fitting to:  293\n",
      "fitting to:  294\n",
      "fitting to:  295\n",
      "fitting to:  296\n",
      "fitting to:  297\n",
      "fitting to:  298\n",
      "fitting to:  299\n",
      "fitting to:  300\n",
      "fitting to:  301\n",
      "fitting to:  302\n",
      "fitting to:  303\n",
      "fitting to:  304\n",
      "fitting to:  305\n",
      "fitting to:  306\n",
      "fitting to:  307\n",
      "fitting to:  308\n",
      "fitting to:  309\n",
      "fitting to:  310\n",
      "fitting to:  311\n",
      "fitting to:  312\n",
      "fitting to:  313\n",
      "fitting to:  314\n",
      "fitting to:  315\n",
      "fitting to:  316\n",
      "fitting to:  317\n",
      "fitting to:  318\n",
      "fitting to:  319\n",
      "fitting to:  320\n",
      "fitting to:  321\n",
      "fitting to:  322\n",
      "fitting to:  323\n",
      "fitting to:  324\n",
      "fitting to:  325\n",
      "fitting to:  326\n",
      "fitting to:  327\n",
      "fitting to:  328\n",
      "fitting to:  329\n",
      "fitting to:  330\n",
      "fitting to:  331\n",
      "fitting to:  332\n",
      "fitting to:  333\n",
      "fitting to:  334\n",
      "fitting to:  335\n",
      "fitting to:  336\n",
      "fitting to:  337\n",
      "fitting to:  338\n",
      "fitting to:  339\n",
      "fitting to:  340\n",
      "fitting to:  341\n",
      "fitting to:  342\n",
      "fitting to:  343\n",
      "fitting to:  344\n",
      "fitting to:  345\n",
      "fitting to:  346\n",
      "fitting to:  347\n",
      "fitting to:  348\n",
      "fitting to:  349\n",
      "fitting to:  350\n",
      "fitting to:  351\n",
      "fitting to:  352\n",
      "fitting to:  353\n",
      "fitting to:  354\n",
      "fitting to:  355\n",
      "fitting to:  356\n",
      "fitting to:  357\n",
      "fitting to:  358\n",
      "fitting to:  359\n",
      "fitting to:  360\n",
      "fitting to:  361\n",
      "fitting to:  362\n",
      "fitting to:  363\n",
      "fitting to:  364\n",
      "fitting to:  365\n",
      "fitting to:  366\n",
      "fitting to:  367\n",
      "fitting to:  368\n",
      "fitting to:  369\n",
      "fitting to:  370\n",
      "fitting to:  371\n",
      "fitting to:  372\n",
      "fitting to:  373\n",
      "fitting to:  374\n",
      "fitting to:  375\n",
      "fitting to:  376\n",
      "fitting to:  377\n",
      "fitting to:  378\n",
      "fitting to:  379\n",
      "fitting to:  380\n",
      "fitting to:  381\n",
      "fitting to:  382\n",
      "fitting to:  383\n",
      "fitting to:  384\n",
      "fitting to:  385\n",
      "fitting to:  386\n",
      "fitting to:  387\n",
      "fitting to:  388\n",
      "fitting to:  389\n",
      "fitting to:  390\n",
      "fitting to:  391\n",
      "fitting to:  392\n",
      "fitting to:  393\n",
      "fitting to:  394\n",
      "fitting to:  395\n",
      "fitting to:  396\n",
      "fitting to:  397\n",
      "fitting to:  398\n",
      "fitting to:  399\n",
      "fitting to:  400\n",
      "fitting to:  401\n",
      "fitting to:  402\n",
      "fitting to:  403\n",
      "fitting to:  404\n",
      "fitting to:  405\n",
      "fitting to:  406\n",
      "fitting to:  407\n",
      "fitting to:  408\n",
      "fitting to:  409\n",
      "fitting to:  410\n",
      "fitting to:  411\n",
      "fitting to:  412\n",
      "fitting to:  413\n",
      "fitting to:  414\n",
      "fitting to:  415\n",
      "fitting to:  416\n",
      "fitting to:  417\n",
      "fitting to:  418\n",
      "fitting to:  419\n",
      "fitting to:  420\n",
      "fitting to:  421\n",
      "fitting to:  422\n",
      "fitting to:  423\n",
      "fitting to:  424\n",
      "fitting to:  425\n",
      "fitting to:  426\n",
      "fitting to:  427\n",
      "fitting to:  428\n",
      "fitting to:  429\n",
      "fitting to:  430\n",
      "fitting to:  431\n",
      "fitting to:  432\n",
      "fitting to:  433\n",
      "fitting to:  434\n",
      "fitting to:  435\n",
      "fitting to:  436\n",
      "fitting to:  437\n",
      "fitting to:  438\n",
      "fitting to:  439\n",
      "fitting to:  440\n",
      "fitting to:  441\n",
      "fitting to:  442\n",
      "fitting to:  443\n",
      "fitting to:  444\n",
      "fitting to:  445\n",
      "fitting to:  446\n",
      "fitting to:  447\n",
      "fitting to:  448\n",
      "fitting to:  449\n",
      "fitting to:  450\n",
      "fitting to:  451\n",
      "fitting to:  452\n",
      "fitting to:  453\n",
      "fitting to:  454\n",
      "fitting to:  455\n",
      "fitting to:  456\n",
      "fitting to:  457\n",
      "fitting to:  458\n",
      "fitting to:  459\n",
      "fitting to:  460\n",
      "fitting to:  461\n",
      "fitting to:  462\n",
      "fitting to:  463\n",
      "fitting to:  464\n",
      "fitting to:  465\n",
      "fitting to:  466\n",
      "fitting to:  467\n",
      "fitting to:  468\n",
      "fitting to:  469\n",
      "fitting to:  470\n",
      "fitting to:  471\n",
      "fitting to:  472\n",
      "fitting to:  473\n",
      "fitting to:  474\n",
      "fitting to:  475\n",
      "fitting to:  476\n",
      "fitting to:  477\n",
      "fitting to:  478\n",
      "fitting to:  479\n",
      "fitting to:  480\n",
      "fitting to:  481\n",
      "fitting to:  482\n",
      "fitting to:  483\n",
      "fitting to:  484\n",
      "fitting to:  485\n",
      "fitting to:  486\n",
      "fitting to:  487\n",
      "fitting to:  488\n",
      "fitting to:  489\n",
      "fitting to:  490\n",
      "fitting to:  491\n",
      "fitting to:  492\n",
      "fitting to:  493\n",
      "fitting to:  494\n",
      "fitting to:  495\n",
      "fitting to:  496\n",
      "fitting to:  497\n",
      "fitting to:  498\n",
      "fitting to:  499\n",
      "fitting to:  500\n",
      "fitting to:  0\n",
      "fitting to:  1\n",
      "fitting to:  2\n",
      "fitting to:  3\n",
      "fitting to:  4\n",
      "fitting to:  5\n",
      "fitting to:  6\n",
      "fitting to:  7\n",
      "fitting to:  8\n",
      "fitting to:  9\n",
      "fitting to:  10\n",
      "fitting to:  11\n",
      "fitting to:  12\n",
      "fitting to:  13\n",
      "fitting to:  14\n",
      "fitting to:  15\n",
      "fitting to:  16\n",
      "fitting to:  17\n",
      "fitting to:  18\n",
      "fitting to:  19\n",
      "fitting to:  20\n",
      "fitting to:  21\n",
      "fitting to:  22\n",
      "fitting to:  23\n",
      "fitting to:  24\n",
      "fitting to:  25\n",
      "fitting to:  26\n",
      "fitting to:  27\n",
      "fitting to:  28\n",
      "fitting to:  29\n",
      "fitting to:  30\n",
      "fitting to:  31\n",
      "fitting to:  32\n",
      "fitting to:  33\n",
      "fitting to:  34\n",
      "fitting to:  35\n",
      "fitting to:  36\n",
      "fitting to:  37\n",
      "fitting to:  38\n",
      "fitting to:  39\n",
      "fitting to:  40\n",
      "fitting to:  41\n",
      "fitting to:  42\n",
      "fitting to:  43\n",
      "fitting to:  44\n",
      "fitting to:  45\n",
      "fitting to:  46\n",
      "fitting to:  47\n",
      "fitting to:  48\n",
      "fitting to:  49\n",
      "fitting to:  50\n",
      "fitting to:  51\n",
      "fitting to:  52\n",
      "fitting to:  53\n",
      "fitting to:  54\n",
      "fitting to:  55\n",
      "fitting to:  56\n",
      "fitting to:  57\n",
      "fitting to:  58\n",
      "fitting to:  59\n",
      "fitting to:  60\n",
      "fitting to:  61\n",
      "fitting to:  62\n",
      "fitting to:  63\n",
      "fitting to:  64\n",
      "fitting to:  65\n",
      "fitting to:  66\n",
      "fitting to:  67\n",
      "fitting to:  68\n",
      "fitting to:  69\n",
      "fitting to:  70\n",
      "fitting to:  71\n",
      "fitting to:  72\n",
      "fitting to:  73\n",
      "fitting to:  74\n",
      "fitting to:  75\n",
      "fitting to:  76\n",
      "fitting to:  77\n",
      "fitting to:  78\n",
      "fitting to:  79\n",
      "fitting to:  80\n",
      "fitting to:  81\n",
      "fitting to:  82\n",
      "fitting to:  83\n",
      "fitting to:  84\n",
      "fitting to:  85\n",
      "fitting to:  86\n",
      "fitting to:  87\n",
      "fitting to:  88\n",
      "fitting to:  89\n",
      "fitting to:  90\n",
      "fitting to:  91\n",
      "fitting to:  92\n",
      "fitting to:  93\n",
      "fitting to:  94\n",
      "fitting to:  95\n",
      "fitting to:  96\n",
      "fitting to:  97\n",
      "fitting to:  98\n",
      "fitting to:  99\n",
      "fitting to:  100\n",
      "fitting to:  101\n",
      "fitting to:  102\n",
      "fitting to:  103\n",
      "fitting to:  104\n",
      "fitting to:  105\n",
      "fitting to:  106\n",
      "fitting to:  107\n",
      "fitting to:  108\n",
      "fitting to:  109\n",
      "fitting to:  110\n",
      "fitting to:  111\n",
      "fitting to:  112\n",
      "fitting to:  113\n",
      "fitting to:  114\n",
      "fitting to:  115\n",
      "fitting to:  116\n",
      "fitting to:  117\n",
      "fitting to:  118\n",
      "fitting to:  119\n",
      "fitting to:  120\n",
      "fitting to:  121\n",
      "fitting to:  122\n",
      "fitting to:  123\n",
      "fitting to:  124\n",
      "fitting to:  125\n",
      "fitting to:  126\n",
      "fitting to:  127\n",
      "fitting to:  128\n",
      "fitting to:  129\n",
      "fitting to:  130\n",
      "fitting to:  131\n",
      "fitting to:  132\n",
      "fitting to:  133\n",
      "fitting to:  134\n",
      "fitting to:  135\n",
      "fitting to:  136\n",
      "fitting to:  137\n",
      "fitting to:  138\n",
      "fitting to:  139\n",
      "fitting to:  140\n",
      "fitting to:  141\n",
      "fitting to:  142\n",
      "fitting to:  143\n",
      "fitting to:  144\n",
      "fitting to:  145\n",
      "fitting to:  146\n",
      "fitting to:  147\n",
      "fitting to:  148\n",
      "fitting to:  149\n",
      "fitting to:  150\n",
      "fitting to:  151\n",
      "fitting to:  152\n",
      "fitting to:  153\n",
      "fitting to:  154\n",
      "fitting to:  155\n",
      "fitting to:  156\n",
      "fitting to:  157\n",
      "fitting to:  158\n",
      "fitting to:  159\n",
      "fitting to:  160\n",
      "fitting to:  161\n",
      "fitting to:  162\n",
      "fitting to:  163\n",
      "fitting to:  164\n",
      "fitting to:  165\n",
      "fitting to:  166\n",
      "fitting to:  167\n",
      "fitting to:  168\n",
      "fitting to:  169\n",
      "fitting to:  170\n",
      "fitting to:  171\n",
      "fitting to:  172\n",
      "fitting to:  173\n",
      "fitting to:  174\n",
      "fitting to:  175\n",
      "fitting to:  176\n",
      "fitting to:  177\n",
      "fitting to:  178\n",
      "fitting to:  179\n",
      "fitting to:  180\n",
      "fitting to:  181\n",
      "fitting to:  182\n",
      "fitting to:  183\n",
      "fitting to:  184\n",
      "fitting to:  185\n",
      "fitting to:  186\n",
      "fitting to:  187\n",
      "fitting to:  188\n",
      "fitting to:  189\n",
      "fitting to:  190\n",
      "fitting to:  191\n",
      "fitting to:  192\n",
      "fitting to:  193\n",
      "fitting to:  194\n",
      "fitting to:  195\n",
      "fitting to:  196\n",
      "fitting to:  197\n",
      "fitting to:  198\n",
      "fitting to:  199\n",
      "fitting to:  200\n",
      "fitting to:  201\n",
      "fitting to:  202\n",
      "fitting to:  203\n",
      "fitting to:  204\n",
      "fitting to:  205\n",
      "fitting to:  206\n",
      "fitting to:  207\n",
      "fitting to:  208\n",
      "fitting to:  209\n",
      "fitting to:  210\n",
      "fitting to:  211\n",
      "fitting to:  212\n",
      "fitting to:  213\n",
      "fitting to:  214\n",
      "fitting to:  215\n",
      "fitting to:  216\n",
      "fitting to:  217\n",
      "fitting to:  218\n",
      "fitting to:  219\n",
      "fitting to:  220\n",
      "fitting to:  221\n",
      "fitting to:  222\n",
      "fitting to:  223\n",
      "fitting to:  224\n",
      "fitting to:  225\n",
      "fitting to:  226\n",
      "fitting to:  227\n",
      "fitting to:  228\n",
      "fitting to:  229\n",
      "fitting to:  230\n",
      "fitting to:  231\n",
      "fitting to:  232\n",
      "fitting to:  233\n",
      "fitting to:  234\n",
      "fitting to:  235\n",
      "fitting to:  236\n",
      "fitting to:  237\n",
      "fitting to:  238\n",
      "fitting to:  239\n",
      "fitting to:  240\n",
      "fitting to:  241\n",
      "fitting to:  242\n",
      "fitting to:  243\n",
      "fitting to:  244\n",
      "fitting to:  245\n",
      "fitting to:  246\n",
      "fitting to:  247\n",
      "fitting to:  248\n",
      "fitting to:  249\n",
      "fitting to:  250\n",
      "fitting to:  251\n",
      "fitting to:  252\n",
      "fitting to:  253\n",
      "fitting to:  254\n",
      "fitting to:  255\n",
      "fitting to:  256\n",
      "fitting to:  257\n",
      "fitting to:  258\n",
      "fitting to:  259\n",
      "fitting to:  260\n",
      "fitting to:  261\n",
      "fitting to:  262\n",
      "fitting to:  263\n",
      "fitting to:  264\n",
      "fitting to:  265\n",
      "fitting to:  266\n",
      "fitting to:  267\n",
      "fitting to:  268\n",
      "fitting to:  269\n",
      "fitting to:  270\n",
      "fitting to:  271\n",
      "fitting to:  272\n",
      "fitting to:  273\n",
      "fitting to:  274\n",
      "fitting to:  275\n",
      "fitting to:  276\n",
      "fitting to:  277\n",
      "fitting to:  278\n",
      "fitting to:  279\n",
      "fitting to:  280\n",
      "fitting to:  281\n",
      "fitting to:  282\n",
      "fitting to:  283\n",
      "fitting to:  284\n",
      "fitting to:  285\n",
      "fitting to:  286\n",
      "fitting to:  287\n",
      "fitting to:  288\n",
      "fitting to:  289\n",
      "fitting to:  290\n",
      "fitting to:  291\n",
      "fitting to:  292\n",
      "fitting to:  293\n",
      "fitting to:  294\n",
      "fitting to:  295\n",
      "fitting to:  296\n",
      "fitting to:  297\n",
      "fitting to:  298\n",
      "fitting to:  299\n",
      "fitting to:  300\n",
      "fitting to:  301\n",
      "fitting to:  302\n",
      "fitting to:  303\n",
      "fitting to:  304\n",
      "fitting to:  305\n",
      "fitting to:  306\n",
      "fitting to:  307\n",
      "fitting to:  308\n",
      "fitting to:  309\n",
      "fitting to:  310\n",
      "fitting to:  311\n",
      "fitting to:  312\n",
      "fitting to:  313\n",
      "fitting to:  314\n",
      "fitting to:  315\n",
      "fitting to:  316\n",
      "fitting to:  317\n",
      "fitting to:  318\n",
      "fitting to:  319\n",
      "fitting to:  320\n",
      "fitting to:  321\n",
      "fitting to:  322\n",
      "fitting to:  323\n",
      "fitting to:  324\n",
      "fitting to:  325\n",
      "fitting to:  326\n",
      "fitting to:  327\n",
      "fitting to:  328\n",
      "fitting to:  329\n",
      "fitting to:  330\n",
      "fitting to:  331\n",
      "fitting to:  332\n",
      "fitting to:  333\n",
      "fitting to:  334\n",
      "fitting to:  335\n",
      "fitting to:  336\n",
      "fitting to:  337\n",
      "fitting to:  338\n",
      "fitting to:  339\n",
      "fitting to:  340\n",
      "fitting to:  341\n",
      "fitting to:  342\n",
      "fitting to:  343\n",
      "fitting to:  344\n",
      "fitting to:  345\n",
      "fitting to:  346\n",
      "fitting to:  347\n",
      "fitting to:  348\n",
      "fitting to:  349\n",
      "fitting to:  350\n",
      "fitting to:  351\n",
      "fitting to:  352\n",
      "fitting to:  353\n",
      "fitting to:  354\n",
      "fitting to:  355\n",
      "fitting to:  356\n",
      "fitting to:  357\n",
      "fitting to:  358\n",
      "fitting to:  359\n",
      "fitting to:  360\n",
      "fitting to:  361\n",
      "fitting to:  362\n",
      "fitting to:  363\n",
      "fitting to:  364\n",
      "fitting to:  365\n",
      "fitting to:  366\n",
      "fitting to:  367\n",
      "fitting to:  368\n",
      "fitting to:  369\n",
      "fitting to:  370\n",
      "fitting to:  371\n",
      "fitting to:  372\n",
      "fitting to:  373\n",
      "fitting to:  374\n",
      "fitting to:  375\n",
      "fitting to:  376\n",
      "fitting to:  377\n",
      "fitting to:  378\n",
      "fitting to:  379\n",
      "fitting to:  380\n",
      "fitting to:  381\n",
      "fitting to:  382\n",
      "fitting to:  383\n",
      "fitting to:  384\n",
      "fitting to:  385\n",
      "fitting to:  386\n",
      "fitting to:  387\n",
      "fitting to:  388\n",
      "fitting to:  389\n",
      "fitting to:  390\n",
      "fitting to:  391\n",
      "fitting to:  392\n",
      "fitting to:  393\n",
      "fitting to:  394\n",
      "fitting to:  395\n",
      "fitting to:  396\n",
      "fitting to:  397\n",
      "fitting to:  398\n",
      "fitting to:  399\n",
      "fitting to:  400\n",
      "fitting to:  401\n",
      "fitting to:  402\n",
      "fitting to:  403\n",
      "fitting to:  404\n",
      "fitting to:  405\n",
      "fitting to:  406\n",
      "fitting to:  407\n",
      "fitting to:  408\n",
      "fitting to:  409\n",
      "fitting to:  410\n",
      "fitting to:  411\n",
      "fitting to:  412\n",
      "fitting to:  413\n",
      "fitting to:  414\n",
      "fitting to:  415\n",
      "fitting to:  416\n",
      "fitting to:  417\n",
      "fitting to:  418\n",
      "fitting to:  419\n",
      "fitting to:  420\n",
      "fitting to:  421\n",
      "fitting to:  422\n",
      "fitting to:  423\n",
      "fitting to:  424\n",
      "fitting to:  425\n",
      "fitting to:  426\n",
      "fitting to:  427\n",
      "fitting to:  428\n",
      "fitting to:  429\n",
      "fitting to:  430\n",
      "fitting to:  431\n",
      "fitting to:  432\n",
      "fitting to:  433\n",
      "fitting to:  434\n",
      "fitting to:  435\n",
      "fitting to:  436\n",
      "fitting to:  437\n",
      "fitting to:  438\n",
      "fitting to:  439\n",
      "fitting to:  440\n",
      "fitting to:  441\n",
      "fitting to:  442\n",
      "fitting to:  443\n",
      "fitting to:  444\n",
      "fitting to:  445\n",
      "fitting to:  446\n",
      "fitting to:  447\n",
      "fitting to:  448\n",
      "fitting to:  449\n",
      "fitting to:  450\n",
      "fitting to:  451\n",
      "fitting to:  452\n",
      "fitting to:  453\n",
      "fitting to:  454\n",
      "fitting to:  455\n",
      "fitting to:  456\n",
      "fitting to:  457\n",
      "fitting to:  458\n",
      "fitting to:  459\n",
      "fitting to:  460\n",
      "fitting to:  461\n",
      "fitting to:  462\n",
      "fitting to:  463\n",
      "fitting to:  464\n",
      "fitting to:  465\n",
      "fitting to:  466\n",
      "fitting to:  467\n",
      "fitting to:  468\n",
      "fitting to:  469\n",
      "fitting to:  470\n",
      "fitting to:  471\n",
      "fitting to:  472\n",
      "fitting to:  473\n",
      "fitting to:  474\n",
      "fitting to:  475\n",
      "fitting to:  476\n",
      "fitting to:  477\n",
      "fitting to:  478\n",
      "fitting to:  479\n",
      "fitting to:  480\n",
      "fitting to:  481\n",
      "fitting to:  482\n",
      "fitting to:  483\n",
      "fitting to:  484\n",
      "fitting to:  485\n",
      "fitting to:  486\n",
      "fitting to:  487\n",
      "fitting to:  488\n",
      "fitting to:  489\n",
      "fitting to:  490\n",
      "fitting to:  491\n",
      "fitting to:  492\n",
      "fitting to:  493\n",
      "fitting to:  494\n",
      "fitting to:  495\n",
      "fitting to:  496\n",
      "fitting to:  497\n",
      "fitting to:  498\n",
      "fitting to:  499\n",
      "fitting to:  500\n",
      "fitting to:  0\n",
      "fitting to:  1\n",
      "fitting to:  2\n",
      "fitting to:  3\n",
      "fitting to:  4\n",
      "fitting to:  5\n",
      "fitting to:  6\n",
      "fitting to:  7\n",
      "fitting to:  8\n",
      "fitting to:  9\n",
      "fitting to:  10\n",
      "fitting to:  11\n",
      "fitting to:  12\n",
      "fitting to:  13\n",
      "fitting to:  14\n",
      "fitting to:  15\n",
      "fitting to:  16\n",
      "fitting to:  17\n",
      "fitting to:  18\n",
      "fitting to:  19\n",
      "fitting to:  20\n",
      "fitting to:  21\n",
      "fitting to:  22\n",
      "fitting to:  23\n",
      "fitting to:  24\n",
      "fitting to:  25\n",
      "fitting to:  26\n",
      "fitting to:  27\n",
      "fitting to:  28\n",
      "fitting to:  29\n",
      "fitting to:  30\n",
      "fitting to:  31\n",
      "fitting to:  32\n",
      "fitting to:  33\n",
      "fitting to:  34\n",
      "fitting to:  35\n",
      "fitting to:  36\n",
      "fitting to:  37\n",
      "fitting to:  38\n",
      "fitting to:  39\n",
      "fitting to:  40\n",
      "fitting to:  41\n",
      "fitting to:  42\n",
      "fitting to:  43\n",
      "fitting to:  44\n",
      "fitting to:  45\n",
      "fitting to:  46\n",
      "fitting to:  47\n",
      "fitting to:  48\n",
      "fitting to:  49\n",
      "fitting to:  50\n",
      "fitting to:  51\n",
      "fitting to:  52\n",
      "fitting to:  53\n",
      "fitting to:  54\n",
      "fitting to:  55\n",
      "fitting to:  56\n",
      "fitting to:  57\n",
      "fitting to:  58\n",
      "fitting to:  59\n",
      "fitting to:  60\n",
      "fitting to:  61\n",
      "fitting to:  62\n",
      "fitting to:  63\n",
      "fitting to:  64\n",
      "fitting to:  65\n",
      "fitting to:  66\n",
      "fitting to:  67\n",
      "fitting to:  68\n",
      "fitting to:  69\n",
      "fitting to:  70\n",
      "fitting to:  71\n",
      "fitting to:  72\n",
      "fitting to:  73\n",
      "fitting to:  74\n",
      "fitting to:  75\n",
      "fitting to:  76\n",
      "fitting to:  77\n",
      "fitting to:  78\n",
      "fitting to:  79\n",
      "fitting to:  80\n",
      "fitting to:  81\n",
      "fitting to:  82\n",
      "fitting to:  83\n",
      "fitting to:  84\n",
      "fitting to:  85\n",
      "fitting to:  86\n",
      "fitting to:  87\n",
      "fitting to:  88\n",
      "fitting to:  89\n",
      "fitting to:  90\n",
      "fitting to:  91\n",
      "fitting to:  92\n",
      "fitting to:  93\n",
      "fitting to:  94\n",
      "fitting to:  95\n",
      "fitting to:  96\n",
      "fitting to:  97\n",
      "fitting to:  98\n",
      "fitting to:  99\n",
      "fitting to:  100\n",
      "fitting to:  101\n",
      "fitting to:  102\n",
      "fitting to:  103\n",
      "fitting to:  104\n",
      "fitting to:  105\n",
      "fitting to:  106\n",
      "fitting to:  107\n",
      "fitting to:  108\n",
      "fitting to:  109\n",
      "fitting to:  110\n",
      "fitting to:  111\n",
      "fitting to:  112\n",
      "fitting to:  113\n",
      "fitting to:  114\n",
      "fitting to:  115\n",
      "fitting to:  116\n",
      "fitting to:  117\n",
      "fitting to:  118\n",
      "fitting to:  119\n",
      "fitting to:  120\n",
      "fitting to:  121\n",
      "fitting to:  122\n",
      "fitting to:  123\n",
      "fitting to:  124\n",
      "fitting to:  125\n",
      "fitting to:  126\n",
      "fitting to:  127\n",
      "fitting to:  128\n",
      "fitting to:  129\n",
      "fitting to:  130\n",
      "fitting to:  131\n",
      "fitting to:  132\n",
      "fitting to:  133\n",
      "fitting to:  134\n",
      "fitting to:  135\n",
      "fitting to:  136\n",
      "fitting to:  137\n",
      "fitting to:  138\n",
      "fitting to:  139\n",
      "fitting to:  140\n",
      "fitting to:  141\n",
      "fitting to:  142\n",
      "fitting to:  143\n",
      "fitting to:  144\n",
      "fitting to:  145\n",
      "fitting to:  146\n",
      "fitting to:  147\n",
      "fitting to:  148\n",
      "fitting to:  149\n",
      "fitting to:  150\n",
      "fitting to:  151\n",
      "fitting to:  152\n",
      "fitting to:  153\n",
      "fitting to:  154\n",
      "fitting to:  155\n",
      "fitting to:  156\n",
      "fitting to:  157\n",
      "fitting to:  158\n",
      "fitting to:  159\n",
      "fitting to:  160\n",
      "fitting to:  161\n",
      "fitting to:  162\n",
      "fitting to:  163\n",
      "fitting to:  164\n",
      "fitting to:  165\n",
      "fitting to:  166\n",
      "fitting to:  167\n",
      "fitting to:  168\n",
      "fitting to:  169\n",
      "fitting to:  170\n",
      "fitting to:  171\n",
      "fitting to:  172\n",
      "fitting to:  173\n",
      "fitting to:  174\n",
      "fitting to:  175\n",
      "fitting to:  176\n",
      "fitting to:  177\n",
      "fitting to:  178\n",
      "fitting to:  179\n",
      "fitting to:  180\n",
      "fitting to:  181\n",
      "fitting to:  182\n",
      "fitting to:  183\n",
      "fitting to:  184\n",
      "fitting to:  185\n",
      "fitting to:  186\n",
      "fitting to:  187\n",
      "fitting to:  188\n",
      "fitting to:  189\n",
      "fitting to:  190\n",
      "fitting to:  191\n",
      "fitting to:  192\n",
      "fitting to:  193\n",
      "fitting to:  194\n",
      "fitting to:  195\n",
      "fitting to:  196\n",
      "fitting to:  197\n",
      "fitting to:  198\n",
      "fitting to:  199\n",
      "fitting to:  200\n",
      "fitting to:  201\n",
      "fitting to:  202\n",
      "fitting to:  203\n",
      "fitting to:  204\n",
      "fitting to:  205\n",
      "fitting to:  206\n",
      "fitting to:  207\n",
      "fitting to:  208\n",
      "fitting to:  209\n",
      "fitting to:  210\n",
      "fitting to:  211\n",
      "fitting to:  212\n",
      "fitting to:  213\n",
      "fitting to:  214\n",
      "fitting to:  215\n",
      "fitting to:  216\n",
      "fitting to:  217\n",
      "fitting to:  218\n",
      "fitting to:  219\n",
      "fitting to:  220\n",
      "fitting to:  221\n",
      "fitting to:  222\n",
      "fitting to:  223\n",
      "fitting to:  224\n",
      "fitting to:  225\n",
      "fitting to:  226\n",
      "fitting to:  227\n",
      "fitting to:  228\n",
      "fitting to:  229\n",
      "fitting to:  230\n",
      "fitting to:  231\n",
      "fitting to:  232\n",
      "fitting to:  233\n",
      "fitting to:  234\n",
      "fitting to:  235\n",
      "fitting to:  236\n",
      "fitting to:  237\n",
      "fitting to:  238\n",
      "fitting to:  239\n",
      "fitting to:  240\n",
      "fitting to:  241\n",
      "fitting to:  242\n",
      "fitting to:  243\n",
      "fitting to:  244\n",
      "fitting to:  245\n",
      "fitting to:  246\n",
      "fitting to:  247\n",
      "fitting to:  248\n",
      "fitting to:  249\n",
      "fitting to:  250\n",
      "fitting to:  251\n",
      "fitting to:  252\n",
      "fitting to:  253\n",
      "fitting to:  254\n",
      "fitting to:  255\n",
      "fitting to:  256\n",
      "fitting to:  257\n",
      "fitting to:  258\n",
      "fitting to:  259\n",
      "fitting to:  260\n",
      "fitting to:  261\n",
      "fitting to:  262\n",
      "fitting to:  263\n",
      "fitting to:  264\n",
      "fitting to:  265\n",
      "fitting to:  266\n",
      "fitting to:  267\n",
      "fitting to:  268\n",
      "fitting to:  269\n",
      "fitting to:  270\n",
      "fitting to:  271\n",
      "fitting to:  272\n",
      "fitting to:  273\n",
      "fitting to:  274\n",
      "fitting to:  275\n",
      "fitting to:  276\n",
      "fitting to:  277\n",
      "fitting to:  278\n",
      "fitting to:  279\n",
      "fitting to:  280\n",
      "fitting to:  281\n",
      "fitting to:  282\n",
      "fitting to:  283\n",
      "fitting to:  284\n",
      "fitting to:  285\n",
      "fitting to:  286\n",
      "fitting to:  287\n",
      "fitting to:  288\n",
      "fitting to:  289\n",
      "fitting to:  290\n",
      "fitting to:  291\n",
      "fitting to:  292\n",
      "fitting to:  293\n",
      "fitting to:  294\n",
      "fitting to:  295\n",
      "fitting to:  296\n",
      "fitting to:  297\n",
      "fitting to:  298\n",
      "fitting to:  299\n",
      "fitting to:  300\n",
      "fitting to:  301\n",
      "fitting to:  302\n",
      "fitting to:  303\n",
      "fitting to:  304\n",
      "fitting to:  305\n",
      "fitting to:  306\n",
      "fitting to:  307\n",
      "fitting to:  308\n",
      "fitting to:  309\n",
      "fitting to:  310\n",
      "fitting to:  311\n",
      "fitting to:  312\n",
      "fitting to:  313\n",
      "fitting to:  314\n",
      "fitting to:  315\n",
      "fitting to:  316\n",
      "fitting to:  317\n",
      "fitting to:  318\n",
      "fitting to:  319\n",
      "fitting to:  320\n",
      "fitting to:  321\n",
      "fitting to:  322\n",
      "fitting to:  323\n",
      "fitting to:  324\n",
      "fitting to:  325\n",
      "fitting to:  326\n",
      "fitting to:  327\n",
      "fitting to:  328\n",
      "fitting to:  329\n",
      "fitting to:  330\n",
      "fitting to:  331\n",
      "fitting to:  332\n",
      "fitting to:  333\n",
      "fitting to:  334\n",
      "fitting to:  335\n",
      "fitting to:  336\n",
      "fitting to:  337\n",
      "fitting to:  338\n",
      "fitting to:  339\n",
      "fitting to:  340\n",
      "fitting to:  341\n",
      "fitting to:  342\n",
      "fitting to:  343\n",
      "fitting to:  344\n",
      "fitting to:  345\n",
      "fitting to:  346\n",
      "fitting to:  347\n",
      "fitting to:  348\n",
      "fitting to:  349\n",
      "fitting to:  350\n",
      "fitting to:  351\n",
      "fitting to:  352\n",
      "fitting to:  353\n",
      "fitting to:  354\n",
      "fitting to:  355\n",
      "fitting to:  356\n",
      "fitting to:  357\n",
      "fitting to:  358\n",
      "fitting to:  359\n",
      "fitting to:  360\n",
      "fitting to:  361\n",
      "fitting to:  362\n",
      "fitting to:  363\n",
      "fitting to:  364\n",
      "fitting to:  365\n",
      "fitting to:  366\n",
      "fitting to:  367\n",
      "fitting to:  368\n",
      "fitting to:  369\n",
      "fitting to:  370\n",
      "fitting to:  371\n",
      "fitting to:  372\n",
      "fitting to:  373\n",
      "fitting to:  374\n",
      "fitting to:  375\n",
      "fitting to:  376\n",
      "fitting to:  377\n",
      "fitting to:  378\n",
      "fitting to:  379\n",
      "fitting to:  380\n",
      "fitting to:  381\n",
      "fitting to:  382\n",
      "fitting to:  383\n",
      "fitting to:  384\n",
      "fitting to:  385\n",
      "fitting to:  386\n",
      "fitting to:  387\n",
      "fitting to:  388\n",
      "fitting to:  389\n",
      "fitting to:  390\n",
      "fitting to:  391\n",
      "fitting to:  392\n",
      "fitting to:  393\n",
      "fitting to:  394\n",
      "fitting to:  395\n",
      "fitting to:  396\n",
      "fitting to:  397\n",
      "fitting to:  398\n",
      "fitting to:  399\n",
      "fitting to:  400\n",
      "fitting to:  401\n",
      "fitting to:  402\n",
      "fitting to:  403\n",
      "fitting to:  404\n",
      "fitting to:  405\n",
      "fitting to:  406\n",
      "fitting to:  407\n",
      "fitting to:  408\n",
      "fitting to:  409\n",
      "fitting to:  410\n",
      "fitting to:  411\n",
      "fitting to:  412\n",
      "fitting to:  413\n",
      "fitting to:  414\n",
      "fitting to:  415\n",
      "fitting to:  416\n",
      "fitting to:  417\n",
      "fitting to:  418\n",
      "fitting to:  419\n",
      "fitting to:  420\n",
      "fitting to:  421\n",
      "fitting to:  422\n",
      "fitting to:  423\n",
      "fitting to:  424\n",
      "fitting to:  425\n",
      "fitting to:  426\n",
      "fitting to:  427\n",
      "fitting to:  428\n",
      "fitting to:  429\n",
      "fitting to:  430\n",
      "fitting to:  431\n",
      "fitting to:  432\n",
      "fitting to:  433\n",
      "fitting to:  434\n",
      "fitting to:  435\n",
      "fitting to:  436\n",
      "fitting to:  437\n",
      "fitting to:  438\n",
      "fitting to:  439\n",
      "fitting to:  440\n",
      "fitting to:  441\n",
      "fitting to:  442\n",
      "fitting to:  443\n",
      "fitting to:  444\n",
      "fitting to:  445\n",
      "fitting to:  446\n",
      "fitting to:  447\n",
      "fitting to:  448\n",
      "fitting to:  449\n",
      "fitting to:  450\n",
      "fitting to:  451\n",
      "fitting to:  452\n",
      "fitting to:  453\n",
      "fitting to:  454\n",
      "fitting to:  455\n",
      "fitting to:  456\n",
      "fitting to:  457\n",
      "fitting to:  458\n",
      "fitting to:  459\n",
      "fitting to:  460\n",
      "fitting to:  461\n",
      "fitting to:  462\n",
      "fitting to:  463\n",
      "fitting to:  464\n",
      "fitting to:  465\n",
      "fitting to:  466\n",
      "fitting to:  467\n",
      "fitting to:  468\n",
      "fitting to:  469\n",
      "fitting to:  470\n",
      "fitting to:  471\n",
      "fitting to:  472\n",
      "fitting to:  473\n",
      "fitting to:  474\n",
      "fitting to:  475\n",
      "fitting to:  476\n",
      "fitting to:  477\n",
      "fitting to:  478\n",
      "fitting to:  479\n",
      "fitting to:  480\n",
      "fitting to:  481\n",
      "fitting to:  482\n",
      "fitting to:  483\n",
      "fitting to:  484\n",
      "fitting to:  485\n",
      "fitting to:  486\n",
      "fitting to:  487\n",
      "fitting to:  488\n",
      "fitting to:  489\n",
      "fitting to:  490\n",
      "fitting to:  491\n",
      "fitting to:  492\n",
      "fitting to:  493\n",
      "fitting to:  494\n",
      "fitting to:  495\n",
      "fitting to:  496\n",
      "fitting to:  497\n",
      "fitting to:  498\n",
      "fitting to:  499\n",
      "fitting to:  500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"compile_and_fit_simple_loop(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, \n",
    "                                batch_size, num_epochs, goal, class_weights_dict):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "model_history_LSTM_model_merged_1L_loop = compile_and_fit_simple_loop('merged',checkpoints_LSTM_model_merged_1L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_2L_loop = compile_and_fit_simple_loop('merged',checkpoints_LSTM_model_merged_2L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_3L_loop = compile_and_fit_simple_loop('merged',checkpoints_LSTM_model_merged_3L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_4L_loop = compile_and_fit_simple_loop('merged',checkpoints_LSTM_model_merged_4L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop,  patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_1L_loop_final.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_1L_loop_final.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_2L_loop_final.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_2L_loop_final.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_3L_loop_final.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_3L_loop_final.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_4L_loop_final.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_4L_loop_final.h5')\n",
    "\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_1L_loop.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_1L_loop.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_2L_loop.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_2L_loop.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_3L_loop.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_3L_loop.h5')\n",
    "# file_saver('checkpoints_loop/checkpoints_LSTM_model_merged_4L_loop.h5', 'checkpoints_loop/checkpoints_LSTM_model_merged_4L_loop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor LSTM looped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compile_and_fit_simple_loop(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, \n",
    "#                                 batch_size, num_epochs, goal, class_weights_dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_history_LSTM_model_1L\u001b[38;5;241m=\u001b[39m compile_and_fit_simple_loop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactors\u001b[39m\u001b[38;5;124m'\u001b[39m,checkpoints_LSTM_model_1L_loop,\u001b[43mconfig\u001b[49m,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop,  patience_value, batch_size, \n\u001b[1;32m      2\u001b[0m                     \u001b[38;5;241m100\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weights_dict)\n\u001b[1;32m      3\u001b[0m model_history_LSTM_model_2L \u001b[38;5;241m=\u001b[39m compile_and_fit_simple_loop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactors\u001b[39m\u001b[38;5;124m'\u001b[39m,checkpoints_LSTM_model_2L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop, y_train_loop,patience_value, batch_size, \n\u001b[1;32m      4\u001b[0m                     \u001b[38;5;241m100\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weights_dict)\n\u001b[1;32m      5\u001b[0m model_history_LSTM_model_3L \u001b[38;5;241m=\u001b[39m compile_and_fit_simple_loop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfactors\u001b[39m\u001b[38;5;124m'\u001b[39m,checkpoints_LSTM_model_3L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop, y_train_loop, patience_value, batch_size, \n\u001b[1;32m      6\u001b[0m                     \u001b[38;5;241m100\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m,class_weights_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "model_history_LSTM_model_1L= compile_and_fit_simple_loop('factors',checkpoints_LSTM_model_1L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop,  patience_value, batch_size, \n",
    "                    100,'fit', class_weights_dict)\n",
    "model_history_LSTM_model_2L = compile_and_fit_simple_loop('factors',checkpoints_LSTM_model_2L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop, y_train_loop,patience_value, batch_size, \n",
    "                    100,'fit', class_weights_dict)\n",
    "model_history_LSTM_model_3L = compile_and_fit_simple_loop('factors',checkpoints_LSTM_model_3L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop, y_train_loop, patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_4L = compile_and_fit_simple_loop('factors',checkpoints_LSTM_model_4L_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, patience_value, batch_size, \n",
    "                    100,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_LSTM_model_1L_loop_final.h5', 'checkpoints_LSTM_model_1L_loop_final.h5')\n",
    "file_saver('checkpoints_LSTM_model_2L_loop_final.h5', 'checkpoints_LSTM_model_2L_loop_final.h5')\n",
    "file_saver('checkpoints_LSTM_model_3L_loop_final.h5', 'checkpoints_LSTM_model_3L_loop_final.h5')\n",
    "file_saver('checkpoints_LSTM_model_4L_loop_final.h5', 'checkpoints_LSTM_model_4L_loop_final.h5')\n",
    "\n",
    "file_saver('checkpoints_LSTM_model_1L_loop.h5', 'checkpoints_LSTM_model_1L_loop.h5')\n",
    "file_saver('checkpoints_LSTM_model_2L_loop.h5', 'checkpoints_LSTM_model_2L_loop.h5')\n",
    "file_saver('checkpoints_LSTM_model_3L_loop.h5', 'checkpoints_LSTM_model_3L_loop.h5')\n",
    "file_saver('checkpoints_LSTM_model_4L_loop.h5', 'checkpoints_LSTM_model_4L_loop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined FFN & LSTM looped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_history_FFN_model_L1 = compile_and_fit_simple_loop('combined',checkpoints_FFN_model_L1_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_FFN_model_L2 = compile_and_fit_simple_loop('combined',checkpoints_FFN_model_L2_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop,  patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_FFN_model_L3 = compile_and_fit_simple_loop('combined',checkpoints_FFN_model_L3_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop, patience_value, batch_size, \n",
    "                    100,'fit')\n",
    "model_history_FFN_model_L4 = compile_and_fit_simple_loop('combined',checkpoints_FFN_model_L4_loop,config,final_stock_data,x_train_factors_loop,x_train_macro_loop,x_train_merged_loop,y_train_loop, x_val_factors_loop,x_val_macro_loop,x_val_merged_loop,y_val_loop, patience_value, batch_size, \n",
    "                    100,'fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_FFN_model_L1_loop_final.h5', 'checkpoints_FFN_model_L1_loop_final.h5')\n",
    "file_saver('checkpoints_FFN_model_L2_loop_final.h5', 'checkpoints_FFN_model_L2_loop_final.h5')\n",
    "file_saver('checkpoints_FFN_model_L3_loop_final.h5', 'checkpoints_FFN_model_L3_loop_final.h5')\n",
    "file_saver('checkpoints_FFN_model_L4_loop_final.h5', 'checkpoints_FFN_model_L4_loop_final.h5')\n",
    "\n",
    "file_saver('checkpoints_FFN_model_L1_loop.h5', 'checkpoints_FFN_model_L1_loop.h5')\n",
    "file_saver('checkpoints_FFN_model_L2_loop.h5', 'checkpoints_FFN_model_L2_loop.h5')\n",
    "file_saver('checkpoints_FFN_model_L3_loop.h5', 'checkpoints_FFN_model_L3_loop.h5')\n",
    "file_saver('checkpoints_FFN_model_L4_loop.h5', 'checkpoints_FFN_model_L4_loop.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:00<00:00, 605.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>mktcap</th>\n",
       "      <th>vweights</th>\n",
       "      <th>ret_mk</th>\n",
       "      <th>monthly_exret</th>\n",
       "      <th>fwd_monthly_exret</th>\n",
       "      <th>quintile</th>\n",
       "      <th>fwd_quintile</th>\n",
       "      <th>fwd_quintile_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-08</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>41.529999</td>\n",
       "      <td>0.028225</td>\n",
       "      <td>4454889.0</td>\n",
       "      <td>1.850115e+08</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.041170</td>\n",
       "      <td>-0.012945</td>\n",
       "      <td>-0.056919</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>38.279999</td>\n",
       "      <td>-0.078257</td>\n",
       "      <td>4431304.0</td>\n",
       "      <td>1.696303e+08</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>-0.021337</td>\n",
       "      <td>-0.056919</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>39.049999</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>4431304.0</td>\n",
       "      <td>1.730424e+08</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.027391</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>0.058629</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-11</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>42.410000</td>\n",
       "      <td>0.086044</td>\n",
       "      <td>4431304.0</td>\n",
       "      <td>1.879316e+08</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.027415</td>\n",
       "      <td>0.058629</td>\n",
       "      <td>0.061811</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>44.970001</td>\n",
       "      <td>0.060363</td>\n",
       "      <td>4391367.0</td>\n",
       "      <td>1.974798e+08</td>\n",
       "      <td>0.007340</td>\n",
       "      <td>-0.001448</td>\n",
       "      <td>0.061811</td>\n",
       "      <td>-0.044355</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>294.950012</td>\n",
       "      <td>-0.028375</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.797247e+08</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>-0.055658</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>302.779999</td>\n",
       "      <td>0.026547</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.871505e+08</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>312.549988</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.970275e+08</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.069506</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>311.769989</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.962862e+08</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>-0.031868</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>305.140015</td>\n",
       "      <td>-0.021266</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.899855e+08</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>-0.019232</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>-0.019856</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36190 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          permno         prc       ret     shrout        mktcap  vweights  \\\n",
       "date                                                                        \n",
       "2014-08  10104.0   41.529999  0.028225  4454889.0  1.850115e+08  0.006780   \n",
       "2014-09  10104.0   38.279999 -0.078257  4431304.0  1.696303e+08  0.006779   \n",
       "2014-10  10104.0   39.049999  0.023250  4431304.0  1.730424e+08  0.007223   \n",
       "2014-11  10104.0   42.410000  0.086044  4431304.0  1.879316e+08  0.007638   \n",
       "2014-12  10104.0   44.970001  0.060363  4391367.0  1.974798e+08  0.007340   \n",
       "...          ...         ...       ...        ...           ...       ...   \n",
       "2020-06  92655.0  294.950012 -0.028375   948380.0  2.797247e+08  0.008455   \n",
       "2020-07  92655.0  302.779999  0.026547   948380.0  2.871505e+08  0.008236   \n",
       "2020-08  92655.0  312.549988  0.032268   950336.0  2.970275e+08  0.008534   \n",
       "2020-09  92655.0  311.769989  0.001504   950336.0  2.962862e+08  0.008568   \n",
       "2020-10  92655.0  305.140015 -0.021266   950336.0  2.899855e+08  0.008495   \n",
       "\n",
       "           ret_mk  monthly_exret  fwd_monthly_exret  quintile  fwd_quintile  \\\n",
       "date                                                                          \n",
       "2014-08  0.041170      -0.012945          -0.056919         4             4   \n",
       "2014-09 -0.021337      -0.056919          -0.004142         4             4   \n",
       "2014-10  0.027391      -0.004142           0.058629         4             4   \n",
       "2014-11  0.027415       0.058629           0.061811         4             4   \n",
       "2014-12 -0.001448       0.061811          -0.044355         4             4   \n",
       "...           ...            ...                ...       ...           ...   \n",
       "2020-06  0.027283      -0.055658          -0.033264         5             5   \n",
       "2020-07  0.059811      -0.033264          -0.037238         5             5   \n",
       "2020-08  0.069506      -0.037238           0.033372         5             5   \n",
       "2020-09 -0.031868       0.033372          -0.002034         5             5   \n",
       "2020-10 -0.019232      -0.002034          -0.019856         5             5   \n",
       "\n",
       "         fwd_quintile_adj  \n",
       "date                       \n",
       "2014-08                 3  \n",
       "2014-09                 3  \n",
       "2014-10                 3  \n",
       "2014-11                 3  \n",
       "2014-12                 3  \n",
       "...                   ...  \n",
       "2020-06                 4  \n",
       "2020-07                 4  \n",
       "2020-08                 4  \n",
       "2020-09                 4  \n",
       "2020-10                 4  \n",
       "\n",
       "[36190 rows x 12 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_loop  = get_test_data_stocks_loop(final_stock_data, batch_size, 0.8)\n",
    "test_data_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.load('test_arrays_seperate_sp.npz',allow_pickle=True)\n",
    "x_test_merged_loop = test_array['x_test_merged']\n",
    "x_test_factors_loop =test_array['x_test_factors']\n",
    "x_test_macro_loop = test_array['x_test_macro']\n",
    "y_test_loop = test_array['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nn_predict(model_type,data_stocks, factor_data,macro_data, merged_data, model_path, batch_size)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permno               9.265500e+04\n",
       "prc                  4.168340e+03\n",
       "ret                  4.849498e-01\n",
       "shrout               1.710254e+07\n",
       "mktcap               2.206911e+09\n",
       "vweights             6.119655e-02\n",
       "ret_mk               1.284342e-01\n",
       "monthly_exret        4.504670e-01\n",
       "fwd_monthly_exret    4.508749e-01\n",
       "quintile             5.000000e+00\n",
       "fwd_quintile         5.000000e+00\n",
       "fwd_quintile_adj     4.000000e+00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_loop.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                                                  2020-10\n",
       "permno                                                92655.0\n",
       "pred_loop/checkpoints_LSTM_model_merged_1L_loop.h5          5\n",
       "dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_LSTM_model_merged_1L_loop.max()\n",
    "\n",
    "# r2_score(test_data_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting:  0\n",
      "predicting:  1\n",
      "predicting:  2\n",
      "predicting:  3\n",
      "predicting:  4\n",
      "predicting:  5\n",
      "predicting:  6\n",
      "predicting:  7\n",
      "predicting:  8\n",
      "predicting:  9\n",
      "predicting:  10\n",
      "predicting:  11\n",
      "predicting:  12\n",
      "predicting:  13\n",
      "predicting:  14\n",
      "predicting:  15\n",
      "predicting:  16\n",
      "predicting:  17\n",
      "predicting:  18\n",
      "predicting:  19\n",
      "predicting:  20\n",
      "predicting:  21\n",
      "predicting:  22\n",
      "predicting:  23\n",
      "predicting:  24\n",
      "predicting:  25\n",
      "predicting:  26\n",
      "predicting:  27\n",
      "predicting:  28\n",
      "predicting:  29\n",
      "predicting:  30\n",
      "predicting:  31\n",
      "predicting:  32\n",
      "predicting:  33\n",
      "predicting:  34\n",
      "predicting:  35\n",
      "predicting:  36\n",
      "predicting:  37\n",
      "predicting:  38\n",
      "predicting:  39\n",
      "predicting:  40\n",
      "predicting:  41\n",
      "predicting:  42\n",
      "predicting:  43\n",
      "predicting:  44\n",
      "predicting:  45\n",
      "predicting:  46\n",
      "predicting:  47\n",
      "predicting:  48\n",
      "predicting:  49\n",
      "predicting:  50\n",
      "predicting:  51\n",
      "predicting:  52\n",
      "predicting:  53\n",
      "predicting:  54\n",
      "predicting:  55\n",
      "predicting:  56\n",
      "predicting:  57\n",
      "predicting:  58\n",
      "predicting:  59\n",
      "predicting:  60\n",
      "predicting:  61\n",
      "predicting:  62\n",
      "predicting:  63\n",
      "predicting:  64\n",
      "predicting:  65\n",
      "predicting:  66\n",
      "predicting:  67\n",
      "predicting:  68\n",
      "predicting:  69\n",
      "predicting:  70\n",
      "predicting:  71\n",
      "predicting:  72\n",
      "predicting:  73\n",
      "predicting:  74\n",
      "predicting:  75\n",
      "predicting:  76\n",
      "predicting:  77\n",
      "predicting:  78\n",
      "predicting:  79\n",
      "predicting:  80\n",
      "predicting:  81\n",
      "predicting:  82\n",
      "predicting:  83\n",
      "predicting:  84\n",
      "predicting:  85\n",
      "predicting:  86\n",
      "predicting:  87\n",
      "predicting:  88\n",
      "predicting:  89\n",
      "predicting:  90\n",
      "predicting:  91\n",
      "predicting:  92\n",
      "predicting:  93\n",
      "predicting:  94\n",
      "predicting:  95\n",
      "predicting:  96\n",
      "predicting:  97\n",
      "predicting:  98\n",
      "predicting:  99\n",
      "predicting:  100\n",
      "predicting:  101\n",
      "predicting:  102\n",
      "predicting:  103\n",
      "predicting:  104\n",
      "predicting:  105\n",
      "predicting:  106\n",
      "predicting:  107\n",
      "predicting:  108\n",
      "predicting:  109\n",
      "predicting:  110\n",
      "predicting:  111\n",
      "predicting:  112\n",
      "predicting:  113\n",
      "predicting:  114\n",
      "predicting:  115\n",
      "predicting:  116\n",
      "predicting:  117\n",
      "predicting:  118\n",
      "predicting:  119\n",
      "predicting:  120\n",
      "predicting:  121\n",
      "predicting:  122\n",
      "predicting:  123\n",
      "predicting:  124\n",
      "predicting:  125\n",
      "predicting:  126\n",
      "predicting:  127\n",
      "predicting:  128\n",
      "predicting:  129\n",
      "predicting:  130\n",
      "predicting:  131\n",
      "predicting:  132\n",
      "predicting:  133\n",
      "predicting:  134\n",
      "predicting:  135\n",
      "predicting:  136\n",
      "predicting:  137\n",
      "predicting:  138\n",
      "predicting:  139\n",
      "predicting:  140\n",
      "predicting:  141\n",
      "predicting:  142\n",
      "predicting:  143\n",
      "predicting:  144\n",
      "predicting:  145\n",
      "predicting:  146\n",
      "predicting:  147\n",
      "predicting:  148\n",
      "predicting:  149\n",
      "predicting:  150\n",
      "predicting:  151\n",
      "predicting:  152\n",
      "predicting:  153\n",
      "predicting:  154\n",
      "predicting:  155\n",
      "predicting:  156\n",
      "predicting:  157\n",
      "predicting:  158\n",
      "predicting:  159\n",
      "predicting:  160\n",
      "predicting:  161\n",
      "predicting:  162\n",
      "predicting:  163\n",
      "predicting:  164\n",
      "predicting:  165\n",
      "predicting:  166\n",
      "predicting:  167\n",
      "predicting:  168\n",
      "predicting:  169\n",
      "predicting:  170\n",
      "predicting:  171\n",
      "predicting:  172\n",
      "predicting:  173\n",
      "predicting:  174\n",
      "predicting:  175\n",
      "predicting:  176\n",
      "predicting:  177\n",
      "predicting:  178\n",
      "predicting:  179\n",
      "predicting:  180\n",
      "predicting:  181\n",
      "predicting:  182\n",
      "predicting:  183\n",
      "predicting:  184\n",
      "predicting:  185\n",
      "predicting:  186\n",
      "predicting:  187\n",
      "predicting:  188\n",
      "predicting:  189\n",
      "predicting:  190\n",
      "predicting:  191\n",
      "predicting:  192\n",
      "predicting:  193\n",
      "predicting:  194\n",
      "predicting:  195\n",
      "predicting:  196\n",
      "predicting:  197\n",
      "predicting:  198\n",
      "predicting:  199\n",
      "predicting:  200\n",
      "predicting:  201\n",
      "predicting:  202\n",
      "predicting:  203\n",
      "predicting:  204\n",
      "predicting:  205\n",
      "predicting:  206\n",
      "predicting:  207\n",
      "predicting:  208\n",
      "predicting:  209\n",
      "predicting:  210\n",
      "predicting:  211\n",
      "predicting:  212\n",
      "predicting:  213\n",
      "predicting:  214\n",
      "predicting:  215\n",
      "predicting:  216\n",
      "predicting:  217\n",
      "predicting:  218\n",
      "predicting:  219\n",
      "predicting:  220\n",
      "predicting:  221\n",
      "predicting:  222\n",
      "predicting:  223\n",
      "predicting:  224\n",
      "predicting:  225\n",
      "predicting:  226\n",
      "predicting:  227\n",
      "predicting:  228\n",
      "predicting:  229\n",
      "predicting:  230\n",
      "predicting:  231\n",
      "predicting:  232\n",
      "predicting:  233\n",
      "predicting:  234\n",
      "predicting:  235\n",
      "predicting:  236\n",
      "predicting:  237\n",
      "predicting:  238\n",
      "predicting:  239\n",
      "predicting:  240\n",
      "predicting:  241\n",
      "predicting:  242\n",
      "predicting:  243\n",
      "predicting:  244\n",
      "predicting:  245\n",
      "predicting:  246\n",
      "predicting:  247\n",
      "predicting:  248\n",
      "predicting:  249\n",
      "predicting:  250\n",
      "predicting:  251\n",
      "predicting:  252\n",
      "predicting:  253\n",
      "predicting:  254\n",
      "predicting:  255\n",
      "predicting:  256\n",
      "predicting:  257\n",
      "predicting:  258\n",
      "predicting:  259\n",
      "predicting:  260\n",
      "predicting:  261\n",
      "predicting:  262\n",
      "predicting:  263\n",
      "predicting:  264\n",
      "predicting:  265\n",
      "predicting:  266\n",
      "predicting:  267\n",
      "predicting:  268\n",
      "predicting:  269\n",
      "predicting:  270\n",
      "predicting:  271\n",
      "predicting:  272\n",
      "predicting:  273\n",
      "predicting:  274\n",
      "predicting:  275\n",
      "predicting:  276\n",
      "predicting:  277\n",
      "predicting:  278\n",
      "predicting:  279\n",
      "predicting:  280\n",
      "predicting:  281\n",
      "predicting:  282\n",
      "predicting:  283\n",
      "predicting:  284\n",
      "predicting:  285\n",
      "predicting:  286\n",
      "predicting:  287\n",
      "predicting:  288\n",
      "predicting:  289\n",
      "predicting:  290\n",
      "predicting:  291\n",
      "predicting:  292\n",
      "predicting:  293\n",
      "predicting:  294\n",
      "predicting:  295\n",
      "predicting:  296\n",
      "predicting:  297\n",
      "predicting:  298\n",
      "predicting:  299\n",
      "predicting:  300\n",
      "predicting:  301\n",
      "predicting:  302\n",
      "predicting:  303\n",
      "predicting:  304\n",
      "predicting:  305\n",
      "predicting:  306\n",
      "predicting:  307\n",
      "predicting:  308\n",
      "predicting:  309\n",
      "predicting:  310\n",
      "predicting:  311\n",
      "predicting:  312\n",
      "predicting:  313\n",
      "predicting:  314\n",
      "predicting:  315\n",
      "predicting:  316\n",
      "predicting:  317\n",
      "predicting:  318\n",
      "predicting:  319\n",
      "predicting:  320\n",
      "predicting:  321\n",
      "predicting:  322\n",
      "predicting:  323\n",
      "predicting:  324\n",
      "predicting:  325\n",
      "predicting:  326\n",
      "predicting:  327\n",
      "predicting:  328\n",
      "predicting:  329\n",
      "predicting:  330\n",
      "predicting:  331\n",
      "predicting:  332\n",
      "predicting:  333\n",
      "predicting:  334\n",
      "predicting:  335\n",
      "predicting:  336\n",
      "predicting:  337\n",
      "predicting:  338\n",
      "predicting:  339\n",
      "predicting:  340\n",
      "predicting:  341\n",
      "predicting:  342\n",
      "predicting:  343\n",
      "predicting:  344\n",
      "predicting:  345\n",
      "predicting:  346\n",
      "predicting:  347\n",
      "predicting:  348\n",
      "predicting:  349\n",
      "predicting:  350\n",
      "predicting:  351\n",
      "predicting:  352\n",
      "predicting:  353\n",
      "predicting:  354\n",
      "predicting:  355\n",
      "predicting:  356\n",
      "predicting:  357\n",
      "predicting:  358\n",
      "predicting:  359\n",
      "predicting:  360\n",
      "predicting:  361\n",
      "predicting:  362\n",
      "predicting:  363\n",
      "predicting:  364\n",
      "predicting:  365\n",
      "predicting:  366\n",
      "predicting:  367\n",
      "predicting:  368\n",
      "predicting:  369\n",
      "predicting:  370\n",
      "predicting:  371\n",
      "predicting:  372\n",
      "predicting:  373\n",
      "predicting:  374\n",
      "predicting:  375\n",
      "predicting:  376\n",
      "predicting:  377\n",
      "predicting:  378\n",
      "predicting:  379\n",
      "predicting:  380\n",
      "predicting:  381\n",
      "predicting:  382\n",
      "predicting:  383\n",
      "predicting:  384\n",
      "predicting:  385\n",
      "predicting:  386\n",
      "predicting:  387\n",
      "predicting:  388\n",
      "predicting:  389\n",
      "predicting:  390\n",
      "predicting:  391\n",
      "predicting:  392\n",
      "predicting:  393\n",
      "predicting:  394\n",
      "predicting:  395\n",
      "predicting:  396\n",
      "predicting:  397\n",
      "predicting:  398\n",
      "predicting:  399\n",
      "predicting:  400\n",
      "predicting:  401\n",
      "predicting:  402\n",
      "predicting:  403\n",
      "predicting:  404\n",
      "predicting:  405\n",
      "predicting:  406\n",
      "predicting:  407\n",
      "predicting:  408\n",
      "predicting:  409\n",
      "predicting:  410\n",
      "predicting:  411\n",
      "predicting:  412\n",
      "predicting:  413\n",
      "predicting:  414\n",
      "predicting:  415\n",
      "predicting:  416\n",
      "predicting:  417\n",
      "predicting:  418\n",
      "predicting:  419\n",
      "predicting:  420\n",
      "predicting:  421\n",
      "predicting:  422\n",
      "predicting:  423\n",
      "predicting:  424\n",
      "predicting:  425\n",
      "predicting:  426\n",
      "predicting:  427\n",
      "predicting:  428\n",
      "predicting:  429\n",
      "predicting:  430\n",
      "predicting:  431\n",
      "predicting:  432\n",
      "predicting:  433\n",
      "predicting:  434\n",
      "predicting:  435\n",
      "predicting:  436\n",
      "predicting:  437\n",
      "predicting:  438\n",
      "predicting:  439\n",
      "predicting:  440\n",
      "predicting:  441\n",
      "predicting:  442\n",
      "predicting:  443\n",
      "predicting:  444\n",
      "predicting:  445\n",
      "predicting:  446\n",
      "predicting:  447\n",
      "predicting:  448\n",
      "predicting:  449\n",
      "predicting:  450\n",
      "predicting:  451\n",
      "predicting:  452\n",
      "predicting:  453\n",
      "predicting:  454\n",
      "predicting:  455\n",
      "predicting:  456\n",
      "predicting:  457\n",
      "predicting:  458\n",
      "predicting:  459\n",
      "predicting:  460\n",
      "predicting:  461\n",
      "predicting:  462\n",
      "predicting:  463\n",
      "predicting:  464\n",
      "predicting:  465\n",
      "predicting:  466\n",
      "predicting:  467\n",
      "predicting:  468\n",
      "predicting:  469\n",
      "predicting:  470\n",
      "predicting:  471\n",
      "predicting:  472\n",
      "predicting:  473\n",
      "predicting:  474\n",
      "predicting:  475\n",
      "predicting:  476\n",
      "predicting:  477\n",
      "predicting:  478\n",
      "predicting:  479\n",
      "predicting:  480\n",
      "predicting:  481\n",
      "predicting:  482\n",
      "predicting:  483\n",
      "predicting:  484\n",
      "predicting:  485\n",
      "predicting:  486\n",
      "predicting:  487\n",
      "predicting:  488\n",
      "predicting:  489\n",
      "predicting:  490\n",
      "predicting:  491\n",
      "predicting:  492\n",
      "predicting:  493\n",
      "predicting:  494\n",
      "predicting:  495\n",
      "predicting:  496\n",
      "predicting:  497\n",
      "predicting:  498\n",
      "predicting:  499\n",
      "predicting:  500\n",
      "predicting:  0\n",
      "predicting:  1\n",
      "predicting:  2\n",
      "predicting:  3\n",
      "predicting:  4\n",
      "predicting:  5\n",
      "predicting:  6\n",
      "predicting:  7\n",
      "predicting:  8\n",
      "predicting:  9\n",
      "predicting:  10\n",
      "predicting:  11\n",
      "predicting:  12\n",
      "predicting:  13\n",
      "predicting:  14\n",
      "predicting:  15\n",
      "predicting:  16\n",
      "predicting:  17\n",
      "predicting:  18\n",
      "predicting:  19\n",
      "predicting:  20\n",
      "predicting:  21\n",
      "predicting:  22\n",
      "predicting:  23\n",
      "predicting:  24\n",
      "predicting:  25\n",
      "predicting:  26\n",
      "predicting:  27\n",
      "predicting:  28\n",
      "predicting:  29\n",
      "predicting:  30\n",
      "predicting:  31\n",
      "predicting:  32\n",
      "predicting:  33\n",
      "predicting:  34\n",
      "predicting:  35\n",
      "predicting:  36\n",
      "predicting:  37\n",
      "predicting:  38\n",
      "predicting:  39\n",
      "predicting:  40\n",
      "predicting:  41\n",
      "predicting:  42\n",
      "predicting:  43\n",
      "predicting:  44\n",
      "predicting:  45\n",
      "predicting:  46\n",
      "predicting:  47\n",
      "predicting:  48\n",
      "predicting:  49\n",
      "predicting:  50\n",
      "predicting:  51\n",
      "predicting:  52\n",
      "predicting:  53\n",
      "predicting:  54\n",
      "predicting:  55\n",
      "predicting:  56\n",
      "predicting:  57\n",
      "predicting:  58\n",
      "predicting:  59\n",
      "predicting:  60\n",
      "predicting:  61\n",
      "predicting:  62\n",
      "predicting:  63\n",
      "predicting:  64\n",
      "predicting:  65\n",
      "predicting:  66\n",
      "predicting:  67\n",
      "predicting:  68\n",
      "predicting:  69\n",
      "predicting:  70\n",
      "predicting:  71\n",
      "predicting:  72\n",
      "predicting:  73\n",
      "predicting:  74\n",
      "predicting:  75\n",
      "predicting:  76\n",
      "predicting:  77\n",
      "predicting:  78\n",
      "predicting:  79\n",
      "predicting:  80\n",
      "predicting:  81\n",
      "predicting:  82\n",
      "predicting:  83\n",
      "predicting:  84\n",
      "predicting:  85\n",
      "predicting:  86\n",
      "predicting:  87\n",
      "predicting:  88\n",
      "predicting:  89\n",
      "predicting:  90\n",
      "predicting:  91\n",
      "predicting:  92\n",
      "predicting:  93\n",
      "predicting:  94\n",
      "predicting:  95\n",
      "predicting:  96\n",
      "predicting:  97\n",
      "predicting:  98\n",
      "predicting:  99\n",
      "predicting:  100\n",
      "predicting:  101\n",
      "predicting:  102\n",
      "predicting:  103\n",
      "predicting:  104\n",
      "predicting:  105\n",
      "predicting:  106\n",
      "predicting:  107\n",
      "predicting:  108\n",
      "predicting:  109\n",
      "predicting:  110\n",
      "predicting:  111\n",
      "predicting:  112\n",
      "predicting:  113\n",
      "predicting:  114\n",
      "predicting:  115\n",
      "predicting:  116\n",
      "predicting:  117\n",
      "predicting:  118\n",
      "predicting:  119\n",
      "predicting:  120\n",
      "predicting:  121\n",
      "predicting:  122\n",
      "predicting:  123\n",
      "predicting:  124\n",
      "predicting:  125\n",
      "predicting:  126\n",
      "predicting:  127\n",
      "predicting:  128\n",
      "predicting:  129\n",
      "predicting:  130\n",
      "predicting:  131\n",
      "predicting:  132\n",
      "predicting:  133\n",
      "predicting:  134\n",
      "predicting:  135\n",
      "predicting:  136\n",
      "predicting:  137\n",
      "predicting:  138\n",
      "predicting:  139\n",
      "predicting:  140\n",
      "predicting:  141\n",
      "predicting:  142\n",
      "predicting:  143\n",
      "predicting:  144\n",
      "predicting:  145\n",
      "predicting:  146\n",
      "predicting:  147\n",
      "predicting:  148\n",
      "predicting:  149\n",
      "predicting:  150\n",
      "predicting:  151\n",
      "predicting:  152\n",
      "predicting:  153\n",
      "predicting:  154\n",
      "predicting:  155\n",
      "predicting:  156\n",
      "predicting:  157\n",
      "predicting:  158\n",
      "predicting:  159\n",
      "predicting:  160\n",
      "predicting:  161\n",
      "predicting:  162\n",
      "predicting:  163\n",
      "predicting:  164\n",
      "predicting:  165\n",
      "predicting:  166\n",
      "predicting:  167\n",
      "predicting:  168\n",
      "predicting:  169\n",
      "predicting:  170\n",
      "predicting:  171\n",
      "predicting:  172\n",
      "predicting:  173\n",
      "predicting:  174\n",
      "predicting:  175\n",
      "predicting:  176\n",
      "predicting:  177\n",
      "predicting:  178\n",
      "predicting:  179\n",
      "predicting:  180\n",
      "predicting:  181\n",
      "predicting:  182\n",
      "predicting:  183\n",
      "predicting:  184\n",
      "predicting:  185\n",
      "predicting:  186\n",
      "predicting:  187\n",
      "predicting:  188\n",
      "predicting:  189\n",
      "predicting:  190\n",
      "predicting:  191\n",
      "predicting:  192\n",
      "predicting:  193\n",
      "predicting:  194\n",
      "predicting:  195\n",
      "predicting:  196\n",
      "predicting:  197\n",
      "predicting:  198\n",
      "predicting:  199\n",
      "predicting:  200\n",
      "predicting:  201\n",
      "predicting:  202\n",
      "predicting:  203\n",
      "predicting:  204\n",
      "predicting:  205\n",
      "predicting:  206\n",
      "predicting:  207\n",
      "predicting:  208\n",
      "predicting:  209\n",
      "predicting:  210\n",
      "predicting:  211\n",
      "predicting:  212\n",
      "predicting:  213\n",
      "predicting:  214\n",
      "predicting:  215\n",
      "predicting:  216\n",
      "predicting:  217\n",
      "predicting:  218\n",
      "predicting:  219\n",
      "predicting:  220\n",
      "predicting:  221\n",
      "predicting:  222\n",
      "predicting:  223\n",
      "predicting:  224\n",
      "predicting:  225\n",
      "predicting:  226\n",
      "predicting:  227\n",
      "predicting:  228\n",
      "predicting:  229\n",
      "predicting:  230\n",
      "predicting:  231\n",
      "predicting:  232\n",
      "predicting:  233\n",
      "predicting:  234\n",
      "predicting:  235\n",
      "predicting:  236\n",
      "predicting:  237\n",
      "predicting:  238\n",
      "predicting:  239\n",
      "predicting:  240\n",
      "predicting:  241\n",
      "predicting:  242\n",
      "predicting:  243\n",
      "predicting:  244\n",
      "predicting:  245\n",
      "predicting:  246\n",
      "predicting:  247\n",
      "predicting:  248\n",
      "predicting:  249\n",
      "predicting:  250\n",
      "predicting:  251\n",
      "predicting:  252\n",
      "predicting:  253\n",
      "predicting:  254\n",
      "predicting:  255\n",
      "predicting:  256\n",
      "predicting:  257\n",
      "predicting:  258\n",
      "predicting:  259\n",
      "predicting:  260\n",
      "predicting:  261\n",
      "predicting:  262\n",
      "predicting:  263\n",
      "predicting:  264\n",
      "predicting:  265\n",
      "predicting:  266\n",
      "predicting:  267\n",
      "predicting:  268\n",
      "predicting:  269\n",
      "predicting:  270\n",
      "predicting:  271\n",
      "predicting:  272\n",
      "predicting:  273\n",
      "predicting:  274\n",
      "predicting:  275\n",
      "predicting:  276\n",
      "predicting:  277\n",
      "predicting:  278\n",
      "predicting:  279\n",
      "predicting:  280\n",
      "predicting:  281\n",
      "predicting:  282\n",
      "predicting:  283\n",
      "predicting:  284\n",
      "predicting:  285\n",
      "predicting:  286\n",
      "predicting:  287\n",
      "predicting:  288\n",
      "predicting:  289\n",
      "predicting:  290\n",
      "predicting:  291\n",
      "predicting:  292\n",
      "predicting:  293\n",
      "predicting:  294\n",
      "predicting:  295\n",
      "predicting:  296\n",
      "predicting:  297\n",
      "predicting:  298\n",
      "predicting:  299\n",
      "predicting:  300\n",
      "predicting:  301\n",
      "predicting:  302\n",
      "predicting:  303\n",
      "predicting:  304\n",
      "predicting:  305\n",
      "predicting:  306\n",
      "predicting:  307\n",
      "predicting:  308\n",
      "predicting:  309\n",
      "predicting:  310\n",
      "predicting:  311\n",
      "predicting:  312\n",
      "predicting:  313\n",
      "predicting:  314\n",
      "predicting:  315\n",
      "predicting:  316\n",
      "predicting:  317\n",
      "predicting:  318\n",
      "predicting:  319\n",
      "predicting:  320\n",
      "predicting:  321\n",
      "predicting:  322\n",
      "predicting:  323\n",
      "predicting:  324\n",
      "predicting:  325\n",
      "predicting:  326\n",
      "predicting:  327\n",
      "predicting:  328\n",
      "predicting:  329\n",
      "predicting:  330\n",
      "predicting:  331\n",
      "predicting:  332\n",
      "predicting:  333\n",
      "predicting:  334\n",
      "predicting:  335\n",
      "predicting:  336\n",
      "predicting:  337\n",
      "predicting:  338\n",
      "predicting:  339\n",
      "predicting:  340\n",
      "predicting:  341\n",
      "predicting:  342\n",
      "predicting:  343\n",
      "predicting:  344\n",
      "predicting:  345\n",
      "predicting:  346\n",
      "predicting:  347\n",
      "predicting:  348\n",
      "predicting:  349\n",
      "predicting:  350\n",
      "predicting:  351\n",
      "predicting:  352\n",
      "predicting:  353\n",
      "predicting:  354\n",
      "predicting:  355\n",
      "predicting:  356\n",
      "predicting:  357\n",
      "predicting:  358\n",
      "predicting:  359\n",
      "predicting:  360\n",
      "predicting:  361\n",
      "predicting:  362\n",
      "predicting:  363\n",
      "predicting:  364\n",
      "predicting:  365\n",
      "predicting:  366\n",
      "predicting:  367\n",
      "predicting:  368\n",
      "predicting:  369\n",
      "predicting:  370\n",
      "predicting:  371\n",
      "predicting:  372\n",
      "predicting:  373\n",
      "predicting:  374\n",
      "predicting:  375\n",
      "predicting:  376\n",
      "predicting:  377\n",
      "predicting:  378\n",
      "predicting:  379\n",
      "predicting:  380\n",
      "predicting:  381\n",
      "predicting:  382\n",
      "predicting:  383\n",
      "predicting:  384\n",
      "predicting:  385\n",
      "predicting:  386\n",
      "predicting:  387\n",
      "predicting:  388\n",
      "predicting:  389\n",
      "predicting:  390\n",
      "predicting:  391\n",
      "predicting:  392\n",
      "predicting:  393\n",
      "predicting:  394\n",
      "predicting:  395\n",
      "predicting:  396\n",
      "predicting:  397\n",
      "predicting:  398\n",
      "predicting:  399\n",
      "predicting:  400\n",
      "predicting:  401\n",
      "predicting:  402\n",
      "predicting:  403\n",
      "predicting:  404\n",
      "predicting:  405\n",
      "predicting:  406\n",
      "predicting:  407\n",
      "predicting:  408\n",
      "predicting:  409\n",
      "predicting:  410\n",
      "predicting:  411\n",
      "predicting:  412\n",
      "predicting:  413\n",
      "predicting:  414\n",
      "predicting:  415\n",
      "predicting:  416\n",
      "predicting:  417\n",
      "predicting:  418\n",
      "predicting:  419\n",
      "predicting:  420\n",
      "predicting:  421\n",
      "predicting:  422\n",
      "predicting:  423\n",
      "predicting:  424\n",
      "predicting:  425\n",
      "predicting:  426\n",
      "predicting:  427\n",
      "predicting:  428\n",
      "predicting:  429\n",
      "predicting:  430\n",
      "predicting:  431\n",
      "predicting:  432\n",
      "predicting:  433\n",
      "predicting:  434\n",
      "predicting:  435\n",
      "predicting:  436\n",
      "predicting:  437\n",
      "predicting:  438\n",
      "predicting:  439\n",
      "predicting:  440\n",
      "predicting:  441\n",
      "predicting:  442\n",
      "predicting:  443\n",
      "predicting:  444\n",
      "predicting:  445\n",
      "predicting:  446\n",
      "predicting:  447\n",
      "predicting:  448\n",
      "predicting:  449\n",
      "predicting:  450\n",
      "predicting:  451\n",
      "predicting:  452\n",
      "predicting:  453\n",
      "predicting:  454\n",
      "predicting:  455\n",
      "predicting:  456\n",
      "predicting:  457\n",
      "predicting:  458\n",
      "predicting:  459\n",
      "predicting:  460\n",
      "predicting:  461\n",
      "predicting:  462\n",
      "predicting:  463\n",
      "predicting:  464\n",
      "predicting:  465\n",
      "predicting:  466\n",
      "predicting:  467\n",
      "predicting:  468\n",
      "predicting:  469\n",
      "predicting:  470\n",
      "predicting:  471\n",
      "predicting:  472\n",
      "predicting:  473\n",
      "predicting:  474\n",
      "predicting:  475\n",
      "predicting:  476\n",
      "predicting:  477\n",
      "predicting:  478\n",
      "predicting:  479\n",
      "predicting:  480\n",
      "predicting:  481\n",
      "predicting:  482\n",
      "predicting:  483\n",
      "predicting:  484\n",
      "predicting:  485\n",
      "predicting:  486\n",
      "predicting:  487\n",
      "predicting:  488\n",
      "predicting:  489\n",
      "predicting:  490\n",
      "predicting:  491\n",
      "predicting:  492\n",
      "predicting:  493\n",
      "predicting:  494\n",
      "predicting:  495\n",
      "predicting:  496\n",
      "predicting:  497\n",
      "predicting:  498\n",
      "predicting:  499\n",
      "predicting:  500\n",
      "predicting:  0\n",
      "predicting:  1\n",
      "predicting:  2\n",
      "predicting:  3\n",
      "predicting:  4\n",
      "predicting:  5\n",
      "predicting:  6\n",
      "predicting:  7\n",
      "predicting:  8\n",
      "predicting:  9\n",
      "predicting:  10\n",
      "predicting:  11\n",
      "predicting:  12\n",
      "predicting:  13\n",
      "predicting:  14\n",
      "predicting:  15\n",
      "predicting:  16\n",
      "predicting:  17\n",
      "predicting:  18\n",
      "predicting:  19\n",
      "predicting:  20\n",
      "predicting:  21\n",
      "predicting:  22\n",
      "predicting:  23\n",
      "predicting:  24\n",
      "predicting:  25\n",
      "predicting:  26\n",
      "predicting:  27\n",
      "predicting:  28\n",
      "predicting:  29\n",
      "predicting:  30\n",
      "predicting:  31\n",
      "predicting:  32\n",
      "predicting:  33\n",
      "predicting:  34\n",
      "predicting:  35\n",
      "predicting:  36\n",
      "predicting:  37\n",
      "predicting:  38\n",
      "predicting:  39\n",
      "predicting:  40\n",
      "predicting:  41\n",
      "predicting:  42\n",
      "predicting:  43\n",
      "predicting:  44\n",
      "predicting:  45\n",
      "predicting:  46\n",
      "predicting:  47\n",
      "predicting:  48\n",
      "predicting:  49\n",
      "predicting:  50\n",
      "predicting:  51\n",
      "predicting:  52\n",
      "predicting:  53\n",
      "predicting:  54\n",
      "predicting:  55\n",
      "predicting:  56\n",
      "predicting:  57\n",
      "predicting:  58\n",
      "predicting:  59\n",
      "predicting:  60\n",
      "predicting:  61\n",
      "predicting:  62\n",
      "predicting:  63\n",
      "predicting:  64\n",
      "predicting:  65\n",
      "predicting:  66\n",
      "predicting:  67\n",
      "predicting:  68\n",
      "predicting:  69\n",
      "predicting:  70\n",
      "predicting:  71\n",
      "predicting:  72\n",
      "predicting:  73\n",
      "predicting:  74\n",
      "predicting:  75\n",
      "predicting:  76\n",
      "predicting:  77\n",
      "predicting:  78\n",
      "predicting:  79\n",
      "predicting:  80\n",
      "predicting:  81\n",
      "predicting:  82\n",
      "predicting:  83\n",
      "predicting:  84\n",
      "predicting:  85\n",
      "predicting:  86\n",
      "predicting:  87\n",
      "predicting:  88\n",
      "predicting:  89\n",
      "predicting:  90\n",
      "predicting:  91\n",
      "predicting:  92\n",
      "predicting:  93\n",
      "predicting:  94\n",
      "predicting:  95\n",
      "predicting:  96\n",
      "predicting:  97\n",
      "predicting:  98\n",
      "predicting:  99\n",
      "predicting:  100\n",
      "predicting:  101\n",
      "predicting:  102\n",
      "predicting:  103\n",
      "predicting:  104\n",
      "predicting:  105\n",
      "predicting:  106\n",
      "predicting:  107\n",
      "predicting:  108\n",
      "predicting:  109\n",
      "predicting:  110\n",
      "predicting:  111\n",
      "predicting:  112\n",
      "predicting:  113\n",
      "predicting:  114\n",
      "predicting:  115\n",
      "predicting:  116\n",
      "predicting:  117\n",
      "predicting:  118\n",
      "predicting:  119\n",
      "predicting:  120\n",
      "predicting:  121\n",
      "predicting:  122\n",
      "predicting:  123\n",
      "predicting:  124\n",
      "predicting:  125\n",
      "predicting:  126\n",
      "predicting:  127\n",
      "predicting:  128\n",
      "predicting:  129\n",
      "predicting:  130\n",
      "predicting:  131\n",
      "predicting:  132\n",
      "predicting:  133\n",
      "predicting:  134\n",
      "predicting:  135\n",
      "predicting:  136\n",
      "predicting:  137\n",
      "predicting:  138\n",
      "predicting:  139\n",
      "predicting:  140\n",
      "predicting:  141\n",
      "predicting:  142\n",
      "predicting:  143\n",
      "predicting:  144\n",
      "predicting:  145\n",
      "predicting:  146\n",
      "predicting:  147\n",
      "predicting:  148\n",
      "predicting:  149\n",
      "predicting:  150\n",
      "predicting:  151\n",
      "predicting:  152\n",
      "predicting:  153\n",
      "predicting:  154\n",
      "predicting:  155\n",
      "predicting:  156\n",
      "predicting:  157\n",
      "predicting:  158\n",
      "predicting:  159\n",
      "predicting:  160\n",
      "predicting:  161\n",
      "predicting:  162\n",
      "predicting:  163\n",
      "predicting:  164\n",
      "predicting:  165\n",
      "predicting:  166\n",
      "predicting:  167\n",
      "predicting:  168\n",
      "predicting:  169\n",
      "predicting:  170\n",
      "predicting:  171\n",
      "predicting:  172\n",
      "predicting:  173\n",
      "predicting:  174\n",
      "predicting:  175\n",
      "predicting:  176\n",
      "predicting:  177\n",
      "predicting:  178\n",
      "predicting:  179\n",
      "predicting:  180\n",
      "predicting:  181\n",
      "predicting:  182\n",
      "predicting:  183\n",
      "predicting:  184\n",
      "predicting:  185\n",
      "predicting:  186\n",
      "predicting:  187\n",
      "predicting:  188\n",
      "predicting:  189\n",
      "predicting:  190\n",
      "predicting:  191\n",
      "predicting:  192\n",
      "predicting:  193\n",
      "predicting:  194\n",
      "predicting:  195\n",
      "predicting:  196\n",
      "predicting:  197\n",
      "predicting:  198\n",
      "predicting:  199\n",
      "predicting:  200\n",
      "predicting:  201\n",
      "predicting:  202\n",
      "predicting:  203\n",
      "predicting:  204\n",
      "predicting:  205\n",
      "predicting:  206\n",
      "predicting:  207\n",
      "predicting:  208\n",
      "predicting:  209\n",
      "predicting:  210\n",
      "predicting:  211\n",
      "predicting:  212\n",
      "predicting:  213\n",
      "predicting:  214\n",
      "predicting:  215\n",
      "predicting:  216\n",
      "predicting:  217\n",
      "predicting:  218\n",
      "predicting:  219\n",
      "predicting:  220\n",
      "predicting:  221\n",
      "predicting:  222\n",
      "predicting:  223\n",
      "predicting:  224\n",
      "predicting:  225\n",
      "predicting:  226\n",
      "predicting:  227\n",
      "predicting:  228\n",
      "predicting:  229\n",
      "predicting:  230\n",
      "predicting:  231\n",
      "predicting:  232\n",
      "predicting:  233\n",
      "predicting:  234\n",
      "predicting:  235\n",
      "predicting:  236\n",
      "predicting:  237\n",
      "predicting:  238\n",
      "predicting:  239\n",
      "predicting:  240\n",
      "predicting:  241\n",
      "predicting:  242\n",
      "predicting:  243\n",
      "predicting:  244\n",
      "predicting:  245\n",
      "predicting:  246\n",
      "predicting:  247\n",
      "predicting:  248\n",
      "predicting:  249\n",
      "predicting:  250\n",
      "predicting:  251\n",
      "predicting:  252\n",
      "predicting:  253\n",
      "predicting:  254\n",
      "predicting:  255\n",
      "predicting:  256\n",
      "predicting:  257\n",
      "predicting:  258\n",
      "predicting:  259\n",
      "predicting:  260\n",
      "predicting:  261\n",
      "predicting:  262\n",
      "predicting:  263\n",
      "predicting:  264\n",
      "predicting:  265\n",
      "predicting:  266\n",
      "predicting:  267\n",
      "predicting:  268\n",
      "predicting:  269\n",
      "predicting:  270\n",
      "predicting:  271\n",
      "predicting:  272\n",
      "predicting:  273\n",
      "predicting:  274\n",
      "predicting:  275\n",
      "predicting:  276\n",
      "predicting:  277\n",
      "predicting:  278\n",
      "predicting:  279\n",
      "predicting:  280\n",
      "predicting:  281\n",
      "predicting:  282\n",
      "predicting:  283\n",
      "predicting:  284\n",
      "predicting:  285\n",
      "predicting:  286\n",
      "predicting:  287\n",
      "predicting:  288\n",
      "predicting:  289\n",
      "predicting:  290\n",
      "predicting:  291\n",
      "predicting:  292\n",
      "predicting:  293\n",
      "predicting:  294\n",
      "predicting:  295\n",
      "predicting:  296\n",
      "predicting:  297\n",
      "predicting:  298\n",
      "predicting:  299\n",
      "predicting:  300\n",
      "predicting:  301\n",
      "predicting:  302\n",
      "predicting:  303\n",
      "predicting:  304\n",
      "predicting:  305\n",
      "predicting:  306\n",
      "predicting:  307\n",
      "predicting:  308\n",
      "predicting:  309\n",
      "predicting:  310\n",
      "predicting:  311\n",
      "predicting:  312\n",
      "predicting:  313\n",
      "predicting:  314\n",
      "predicting:  315\n",
      "predicting:  316\n",
      "predicting:  317\n",
      "predicting:  318\n",
      "predicting:  319\n",
      "predicting:  320\n",
      "predicting:  321\n",
      "predicting:  322\n",
      "predicting:  323\n",
      "predicting:  324\n",
      "predicting:  325\n",
      "predicting:  326\n",
      "predicting:  327\n",
      "predicting:  328\n",
      "predicting:  329\n",
      "predicting:  330\n",
      "predicting:  331\n",
      "predicting:  332\n",
      "predicting:  333\n",
      "predicting:  334\n",
      "predicting:  335\n",
      "predicting:  336\n",
      "predicting:  337\n",
      "predicting:  338\n",
      "predicting:  339\n",
      "predicting:  340\n",
      "predicting:  341\n",
      "predicting:  342\n",
      "predicting:  343\n",
      "predicting:  344\n",
      "predicting:  345\n",
      "predicting:  346\n",
      "predicting:  347\n",
      "predicting:  348\n",
      "predicting:  349\n",
      "predicting:  350\n",
      "predicting:  351\n",
      "predicting:  352\n",
      "predicting:  353\n",
      "predicting:  354\n",
      "predicting:  355\n",
      "predicting:  356\n",
      "predicting:  357\n",
      "predicting:  358\n",
      "predicting:  359\n",
      "predicting:  360\n",
      "predicting:  361\n",
      "predicting:  362\n",
      "predicting:  363\n",
      "predicting:  364\n",
      "predicting:  365\n",
      "predicting:  366\n",
      "predicting:  367\n",
      "predicting:  368\n",
      "predicting:  369\n",
      "predicting:  370\n",
      "predicting:  371\n",
      "predicting:  372\n",
      "predicting:  373\n",
      "predicting:  374\n",
      "predicting:  375\n",
      "predicting:  376\n",
      "predicting:  377\n",
      "predicting:  378\n",
      "predicting:  379\n",
      "predicting:  380\n",
      "predicting:  381\n",
      "predicting:  382\n",
      "predicting:  383\n",
      "predicting:  384\n",
      "predicting:  385\n",
      "predicting:  386\n",
      "predicting:  387\n",
      "predicting:  388\n",
      "predicting:  389\n",
      "predicting:  390\n",
      "predicting:  391\n",
      "predicting:  392\n",
      "predicting:  393\n",
      "predicting:  394\n",
      "predicting:  395\n",
      "predicting:  396\n",
      "predicting:  397\n",
      "predicting:  398\n",
      "predicting:  399\n",
      "predicting:  400\n",
      "predicting:  401\n",
      "predicting:  402\n",
      "predicting:  403\n",
      "predicting:  404\n",
      "predicting:  405\n",
      "predicting:  406\n",
      "predicting:  407\n",
      "predicting:  408\n",
      "predicting:  409\n",
      "predicting:  410\n",
      "predicting:  411\n",
      "predicting:  412\n",
      "predicting:  413\n",
      "predicting:  414\n",
      "predicting:  415\n",
      "predicting:  416\n",
      "predicting:  417\n",
      "predicting:  418\n",
      "predicting:  419\n",
      "predicting:  420\n",
      "predicting:  421\n",
      "predicting:  422\n",
      "predicting:  423\n",
      "predicting:  424\n",
      "predicting:  425\n",
      "predicting:  426\n",
      "predicting:  427\n",
      "predicting:  428\n",
      "predicting:  429\n",
      "predicting:  430\n",
      "predicting:  431\n",
      "predicting:  432\n",
      "predicting:  433\n",
      "predicting:  434\n",
      "predicting:  435\n",
      "predicting:  436\n",
      "predicting:  437\n",
      "predicting:  438\n",
      "predicting:  439\n",
      "predicting:  440\n",
      "predicting:  441\n",
      "predicting:  442\n",
      "predicting:  443\n",
      "predicting:  444\n",
      "predicting:  445\n",
      "predicting:  446\n",
      "predicting:  447\n",
      "predicting:  448\n",
      "predicting:  449\n",
      "predicting:  450\n",
      "predicting:  451\n",
      "predicting:  452\n",
      "predicting:  453\n",
      "predicting:  454\n",
      "predicting:  455\n",
      "predicting:  456\n",
      "predicting:  457\n",
      "predicting:  458\n",
      "predicting:  459\n",
      "predicting:  460\n",
      "predicting:  461\n",
      "predicting:  462\n",
      "predicting:  463\n",
      "predicting:  464\n",
      "predicting:  465\n",
      "predicting:  466\n",
      "predicting:  467\n",
      "predicting:  468\n",
      "predicting:  469\n",
      "predicting:  470\n",
      "predicting:  471\n",
      "predicting:  472\n",
      "predicting:  473\n",
      "predicting:  474\n",
      "predicting:  475\n",
      "predicting:  476\n",
      "predicting:  477\n",
      "predicting:  478\n",
      "predicting:  479\n",
      "predicting:  480\n",
      "predicting:  481\n",
      "predicting:  482\n",
      "predicting:  483\n",
      "predicting:  484\n",
      "predicting:  485\n",
      "predicting:  486\n",
      "predicting:  487\n",
      "predicting:  488\n",
      "predicting:  489\n",
      "predicting:  490\n",
      "predicting:  491\n",
      "predicting:  492\n",
      "predicting:  493\n",
      "predicting:  494\n",
      "predicting:  495\n",
      "predicting:  496\n",
      "predicting:  497\n",
      "predicting:  498\n",
      "predicting:  499\n",
      "predicting:  500\n",
      "predicting:  0\n",
      "predicting:  1\n",
      "predicting:  2\n",
      "predicting:  3\n",
      "predicting:  4\n",
      "predicting:  5\n",
      "predicting:  6\n",
      "predicting:  7\n",
      "predicting:  8\n",
      "predicting:  9\n",
      "predicting:  10\n",
      "predicting:  11\n",
      "predicting:  12\n",
      "predicting:  13\n",
      "predicting:  14\n",
      "predicting:  15\n",
      "predicting:  16\n",
      "predicting:  17\n",
      "predicting:  18\n",
      "predicting:  19\n",
      "predicting:  20\n",
      "predicting:  21\n",
      "predicting:  22\n",
      "predicting:  23\n",
      "predicting:  24\n",
      "predicting:  25\n",
      "predicting:  26\n",
      "predicting:  27\n",
      "predicting:  28\n",
      "predicting:  29\n",
      "predicting:  30\n",
      "predicting:  31\n",
      "predicting:  32\n",
      "predicting:  33\n",
      "predicting:  34\n",
      "predicting:  35\n",
      "predicting:  36\n",
      "predicting:  37\n",
      "predicting:  38\n",
      "predicting:  39\n",
      "predicting:  40\n",
      "predicting:  41\n",
      "predicting:  42\n",
      "predicting:  43\n",
      "predicting:  44\n",
      "predicting:  45\n",
      "predicting:  46\n",
      "predicting:  47\n",
      "predicting:  48\n",
      "predicting:  49\n",
      "predicting:  50\n",
      "predicting:  51\n",
      "predicting:  52\n",
      "predicting:  53\n",
      "predicting:  54\n",
      "predicting:  55\n",
      "predicting:  56\n",
      "predicting:  57\n",
      "predicting:  58\n",
      "predicting:  59\n",
      "predicting:  60\n",
      "predicting:  61\n",
      "predicting:  62\n",
      "predicting:  63\n",
      "predicting:  64\n",
      "predicting:  65\n",
      "predicting:  66\n",
      "predicting:  67\n",
      "predicting:  68\n",
      "predicting:  69\n",
      "predicting:  70\n",
      "predicting:  71\n",
      "predicting:  72\n",
      "predicting:  73\n",
      "predicting:  74\n",
      "predicting:  75\n",
      "predicting:  76\n",
      "predicting:  77\n",
      "predicting:  78\n",
      "predicting:  79\n",
      "predicting:  80\n",
      "predicting:  81\n",
      "predicting:  82\n",
      "predicting:  83\n",
      "predicting:  84\n",
      "predicting:  85\n",
      "predicting:  86\n",
      "predicting:  87\n",
      "predicting:  88\n",
      "predicting:  89\n",
      "predicting:  90\n",
      "predicting:  91\n",
      "predicting:  92\n",
      "predicting:  93\n",
      "predicting:  94\n",
      "predicting:  95\n",
      "predicting:  96\n",
      "predicting:  97\n",
      "predicting:  98\n",
      "predicting:  99\n",
      "predicting:  100\n",
      "predicting:  101\n",
      "predicting:  102\n",
      "predicting:  103\n",
      "predicting:  104\n",
      "predicting:  105\n",
      "predicting:  106\n",
      "predicting:  107\n",
      "predicting:  108\n",
      "predicting:  109\n",
      "predicting:  110\n",
      "predicting:  111\n",
      "predicting:  112\n",
      "predicting:  113\n",
      "predicting:  114\n",
      "predicting:  115\n",
      "predicting:  116\n",
      "predicting:  117\n",
      "predicting:  118\n",
      "predicting:  119\n",
      "predicting:  120\n",
      "predicting:  121\n",
      "predicting:  122\n",
      "predicting:  123\n",
      "predicting:  124\n",
      "predicting:  125\n",
      "predicting:  126\n",
      "predicting:  127\n",
      "predicting:  128\n",
      "predicting:  129\n",
      "predicting:  130\n",
      "predicting:  131\n",
      "predicting:  132\n",
      "predicting:  133\n",
      "predicting:  134\n",
      "predicting:  135\n",
      "predicting:  136\n",
      "predicting:  137\n",
      "predicting:  138\n",
      "predicting:  139\n",
      "predicting:  140\n",
      "predicting:  141\n",
      "predicting:  142\n",
      "predicting:  143\n",
      "predicting:  144\n",
      "predicting:  145\n",
      "predicting:  146\n",
      "predicting:  147\n",
      "predicting:  148\n",
      "predicting:  149\n",
      "predicting:  150\n",
      "predicting:  151\n",
      "predicting:  152\n",
      "predicting:  153\n",
      "predicting:  154\n",
      "predicting:  155\n",
      "predicting:  156\n",
      "predicting:  157\n",
      "predicting:  158\n",
      "predicting:  159\n",
      "predicting:  160\n",
      "predicting:  161\n",
      "predicting:  162\n",
      "predicting:  163\n",
      "predicting:  164\n",
      "predicting:  165\n",
      "predicting:  166\n",
      "predicting:  167\n",
      "predicting:  168\n",
      "predicting:  169\n",
      "predicting:  170\n",
      "predicting:  171\n",
      "predicting:  172\n",
      "predicting:  173\n",
      "predicting:  174\n",
      "predicting:  175\n",
      "predicting:  176\n",
      "predicting:  177\n",
      "predicting:  178\n",
      "predicting:  179\n",
      "predicting:  180\n",
      "predicting:  181\n",
      "predicting:  182\n",
      "predicting:  183\n",
      "predicting:  184\n",
      "predicting:  185\n",
      "predicting:  186\n",
      "predicting:  187\n",
      "predicting:  188\n",
      "predicting:  189\n",
      "predicting:  190\n",
      "predicting:  191\n",
      "predicting:  192\n",
      "predicting:  193\n",
      "predicting:  194\n",
      "predicting:  195\n",
      "predicting:  196\n",
      "predicting:  197\n",
      "predicting:  198\n",
      "predicting:  199\n",
      "predicting:  200\n",
      "predicting:  201\n",
      "predicting:  202\n",
      "predicting:  203\n",
      "predicting:  204\n",
      "predicting:  205\n",
      "predicting:  206\n",
      "predicting:  207\n",
      "predicting:  208\n",
      "predicting:  209\n",
      "predicting:  210\n",
      "predicting:  211\n",
      "predicting:  212\n",
      "predicting:  213\n",
      "predicting:  214\n",
      "predicting:  215\n",
      "predicting:  216\n",
      "predicting:  217\n",
      "predicting:  218\n",
      "predicting:  219\n",
      "predicting:  220\n",
      "predicting:  221\n",
      "predicting:  222\n",
      "predicting:  223\n",
      "predicting:  224\n",
      "predicting:  225\n",
      "predicting:  226\n",
      "predicting:  227\n",
      "predicting:  228\n",
      "predicting:  229\n",
      "predicting:  230\n",
      "predicting:  231\n",
      "predicting:  232\n",
      "predicting:  233\n",
      "predicting:  234\n",
      "predicting:  235\n",
      "predicting:  236\n",
      "predicting:  237\n",
      "predicting:  238\n",
      "predicting:  239\n",
      "predicting:  240\n",
      "predicting:  241\n",
      "predicting:  242\n",
      "predicting:  243\n",
      "predicting:  244\n",
      "predicting:  245\n",
      "predicting:  246\n",
      "predicting:  247\n",
      "predicting:  248\n",
      "predicting:  249\n",
      "predicting:  250\n",
      "predicting:  251\n",
      "predicting:  252\n",
      "predicting:  253\n",
      "predicting:  254\n",
      "predicting:  255\n",
      "predicting:  256\n",
      "predicting:  257\n",
      "predicting:  258\n",
      "predicting:  259\n",
      "predicting:  260\n",
      "predicting:  261\n",
      "predicting:  262\n",
      "predicting:  263\n",
      "predicting:  264\n",
      "predicting:  265\n",
      "predicting:  266\n",
      "predicting:  267\n",
      "predicting:  268\n",
      "predicting:  269\n",
      "predicting:  270\n",
      "predicting:  271\n",
      "predicting:  272\n",
      "predicting:  273\n",
      "predicting:  274\n",
      "predicting:  275\n",
      "predicting:  276\n",
      "predicting:  277\n",
      "predicting:  278\n",
      "predicting:  279\n",
      "predicting:  280\n",
      "predicting:  281\n",
      "predicting:  282\n",
      "predicting:  283\n",
      "predicting:  284\n",
      "predicting:  285\n",
      "predicting:  286\n",
      "predicting:  287\n",
      "predicting:  288\n",
      "predicting:  289\n",
      "predicting:  290\n",
      "predicting:  291\n",
      "predicting:  292\n",
      "predicting:  293\n",
      "predicting:  294\n",
      "predicting:  295\n",
      "predicting:  296\n",
      "predicting:  297\n",
      "predicting:  298\n",
      "predicting:  299\n",
      "predicting:  300\n",
      "predicting:  301\n",
      "predicting:  302\n",
      "predicting:  303\n",
      "predicting:  304\n",
      "predicting:  305\n",
      "predicting:  306\n",
      "predicting:  307\n",
      "predicting:  308\n",
      "predicting:  309\n",
      "predicting:  310\n",
      "predicting:  311\n",
      "predicting:  312\n",
      "predicting:  313\n",
      "predicting:  314\n",
      "predicting:  315\n",
      "predicting:  316\n",
      "predicting:  317\n",
      "predicting:  318\n",
      "predicting:  319\n",
      "predicting:  320\n",
      "predicting:  321\n",
      "predicting:  322\n",
      "predicting:  323\n",
      "predicting:  324\n",
      "predicting:  325\n",
      "predicting:  326\n",
      "predicting:  327\n",
      "predicting:  328\n",
      "predicting:  329\n",
      "predicting:  330\n",
      "predicting:  331\n",
      "predicting:  332\n",
      "predicting:  333\n",
      "predicting:  334\n",
      "predicting:  335\n",
      "predicting:  336\n",
      "predicting:  337\n",
      "predicting:  338\n",
      "predicting:  339\n",
      "predicting:  340\n",
      "predicting:  341\n",
      "predicting:  342\n",
      "predicting:  343\n",
      "predicting:  344\n",
      "predicting:  345\n",
      "predicting:  346\n",
      "predicting:  347\n",
      "predicting:  348\n",
      "predicting:  349\n",
      "predicting:  350\n",
      "predicting:  351\n",
      "predicting:  352\n",
      "predicting:  353\n",
      "predicting:  354\n",
      "predicting:  355\n",
      "predicting:  356\n",
      "predicting:  357\n",
      "predicting:  358\n",
      "predicting:  359\n",
      "predicting:  360\n",
      "predicting:  361\n",
      "predicting:  362\n",
      "predicting:  363\n",
      "predicting:  364\n",
      "predicting:  365\n",
      "predicting:  366\n",
      "predicting:  367\n",
      "predicting:  368\n",
      "predicting:  369\n",
      "predicting:  370\n",
      "predicting:  371\n",
      "predicting:  372\n",
      "predicting:  373\n",
      "predicting:  374\n",
      "predicting:  375\n",
      "predicting:  376\n",
      "predicting:  377\n",
      "predicting:  378\n",
      "predicting:  379\n",
      "predicting:  380\n",
      "predicting:  381\n",
      "predicting:  382\n",
      "predicting:  383\n",
      "predicting:  384\n",
      "predicting:  385\n",
      "predicting:  386\n",
      "predicting:  387\n",
      "predicting:  388\n",
      "predicting:  389\n",
      "predicting:  390\n",
      "predicting:  391\n",
      "predicting:  392\n",
      "predicting:  393\n",
      "predicting:  394\n",
      "predicting:  395\n",
      "predicting:  396\n",
      "predicting:  397\n",
      "predicting:  398\n",
      "predicting:  399\n",
      "predicting:  400\n",
      "predicting:  401\n",
      "predicting:  402\n",
      "predicting:  403\n",
      "predicting:  404\n",
      "predicting:  405\n",
      "predicting:  406\n",
      "predicting:  407\n",
      "predicting:  408\n",
      "predicting:  409\n",
      "predicting:  410\n",
      "predicting:  411\n",
      "predicting:  412\n",
      "predicting:  413\n",
      "predicting:  414\n",
      "predicting:  415\n",
      "predicting:  416\n",
      "predicting:  417\n",
      "predicting:  418\n",
      "predicting:  419\n",
      "predicting:  420\n",
      "predicting:  421\n",
      "predicting:  422\n",
      "predicting:  423\n",
      "predicting:  424\n",
      "predicting:  425\n",
      "predicting:  426\n",
      "predicting:  427\n",
      "predicting:  428\n",
      "predicting:  429\n",
      "predicting:  430\n",
      "predicting:  431\n",
      "predicting:  432\n",
      "predicting:  433\n",
      "predicting:  434\n",
      "predicting:  435\n",
      "predicting:  436\n",
      "predicting:  437\n",
      "predicting:  438\n",
      "predicting:  439\n",
      "predicting:  440\n",
      "predicting:  441\n",
      "predicting:  442\n",
      "predicting:  443\n",
      "predicting:  444\n",
      "predicting:  445\n",
      "predicting:  446\n",
      "predicting:  447\n",
      "predicting:  448\n",
      "predicting:  449\n",
      "predicting:  450\n",
      "predicting:  451\n",
      "predicting:  452\n",
      "predicting:  453\n",
      "predicting:  454\n",
      "predicting:  455\n",
      "predicting:  456\n",
      "predicting:  457\n",
      "predicting:  458\n",
      "predicting:  459\n",
      "predicting:  460\n",
      "predicting:  461\n",
      "predicting:  462\n",
      "predicting:  463\n",
      "predicting:  464\n",
      "predicting:  465\n",
      "predicting:  466\n",
      "predicting:  467\n",
      "predicting:  468\n",
      "predicting:  469\n",
      "predicting:  470\n",
      "predicting:  471\n",
      "predicting:  472\n",
      "predicting:  473\n",
      "predicting:  474\n",
      "predicting:  475\n",
      "predicting:  476\n",
      "predicting:  477\n",
      "predicting:  478\n",
      "predicting:  479\n",
      "predicting:  480\n",
      "predicting:  481\n",
      "predicting:  482\n",
      "predicting:  483\n",
      "predicting:  484\n",
      "predicting:  485\n",
      "predicting:  486\n",
      "predicting:  487\n",
      "predicting:  488\n",
      "predicting:  489\n",
      "predicting:  490\n",
      "predicting:  491\n",
      "predicting:  492\n",
      "predicting:  493\n",
      "predicting:  494\n",
      "predicting:  495\n",
      "predicting:  496\n",
      "predicting:  497\n",
      "predicting:  498\n",
      "predicting:  499\n",
      "predicting:  500\n"
     ]
    }
   ],
   "source": [
    "# predictions using merged dataset of factor and macro data and LSTM\n",
    "predictions_LSTM_model_merged_1L_loop = nn_predict_simple_loop('merged',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop, checkpoints_LSTM_model_merged_1L_loop, batch_size, label_names)\n",
    "predictions_LSTM_model_merged_2L_loop = nn_predict_simple_loop('merged',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop, checkpoints_LSTM_model_merged_2L_loop, batch_size, label_names)\n",
    "predictions_LSTM_model_merged_3L_loop = nn_predict_simple_loop('merged',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop, checkpoints_LSTM_model_merged_3L_loop, batch_size, label_names)\n",
    "predictions_LSTM_model_merged_4L_loop = nn_predict_simple_loop('merged',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,checkpoints_LSTM_model_merged_4L_loop, batch_size, label_names)\n",
    "\n",
    "# #predictions only using factor data and LSTM\n",
    "# predictions_LSTM_model_1L_loop = nn_predict_simple_loop('factors',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,  checkpoints_LSTM_model_1L_loop, batch_size)\n",
    "# predictions_LSTM_model_2L_loop = nn_predict_simple_loop('factors',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,  checkpoints_LSTM_model_2L_loop, batch_size)\n",
    "# predictions_LSTM_model_3L_loop = nn_predict_simple_loop('factors',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,  checkpoints_LSTM_model_3L_loop, batch_size)\n",
    "# predictions_LSTM_model_4L_loop = nn_predict_simple_loop('factors',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,  checkpoints_LSTM_model_4L_loop, batch_size)\n",
    "\n",
    "# #predictions Feedforward + LSTM\n",
    "# predictions_FFN_model_L1_loop = nn_predict_simple_loop('combined',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,  checkpoints_FFN_model_L1_loop, batch_size)\n",
    "# predictions_FFN_model_L2_loop = nn_predict_simple_loop('combined',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,checkpoints_FFN_model_L2_loop, batch_size)\n",
    "# predictions_FFN_model_L3_loop = nn_predict_simple_loop('combined',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,checkpoints_FFN_model_L3_loop, batch_size)\n",
    "# predictions_FFN_model_L4_loop = nn_predict_simple_loop('combined',test_data_loop, x_test_factors_loop,x_test_macro_loop,x_test_merged_loop,checkpoints_FFN_model_L4_loop, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_LSTM_model_merged_1L_loop['pred_LSTM_model_merged_1L_loop'] = np.round(predictions_LSTM_model_merged_1L_loop['pred_LSTM_model_merged_1L_loop'])\n",
    "predictions_LSTM_model_merged_2L_loop['pred_LSTM_model_merged_2L_loop'] = np.round(predictions_LSTM_model_merged_2L_loop['pred_LSTM_model_merged_2L_loop'])\n",
    "predictions_LSTM_model_merged_3L_loop['pred_LSTM_model_merged_3L_loop'] = np.round(predictions_LSTM_model_merged_3L_loop['pred_LSTM_model_merged_3L_loop'])\n",
    "predictions_LSTM_model_merged_4L_loop['pred_LSTM_model_merged_4L_loop'] = np.round(predictions_LSTM_model_merged_4L_loop['pred_LSTM_model_merged_4L_loop'])\n",
    "\n",
    "predictions_LSTM_model_1L_loop['pred_LSTM_model_1L_loop'] = np.round(predictions_LSTM_model_1L_loop['pred_LSTM_model_1L_loop'])\n",
    "predictions_LSTM_model_2L_loop['pred_LSTM_model_2L_loop'] = np.round(predictions_LSTM_model_2L_loop['pred_LSTM_model_2L_loop'])\n",
    "predictions_LSTM_model_3L_loop['pred_LSTM_model_3L_loop'] = np.round(predictions_LSTM_model_3L_loop['pred_LSTM_model_3L_loop'])\n",
    "predictions_LSTM_model_4L_loop['pred_LSTM_model_4L_loop'] = np.round(predictions_LSTM_model_4L_loop['pred_LSTM_model_4L_loop'])\n",
    "\n",
    "predictions_FFN_model_L1_loop['pred_FFN_model_L1_loop'] = np.round(predictions_FFN_model_L1_loop['pred_FFN_model_L1_loop'])\n",
    "predictions_FFN_model_L2_loop['pred_FFN_model_L2_loop']= np.round(predictions_FFN_model_L2_loop['pred_FFN_model_L2_loop'])\n",
    "predictions_FFN_model_L3_loop['pred_FFN_model_L3_loop'] = np.round(predictions_FFN_model_L3_loop['pred_FFN_model_L3_loop'])\n",
    "predictions_FFN_model_L4_loop['pred_FFN_model_L4_loop'] = np.round(predictions_FFN_model_L4_loop['pred_FFN_model_L4_loop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" create_metrics(test_data_df, prediction_df, batch_size, n_factors, r2_oos,dataframe_name, Y_name) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_merged_1L_results_loop, LSTM_model_merged_1L_metrics_loop = create_metrics(test_data_loop, predictions_LSTM_model_merged_1L_loop, batch_size,n_factors, r2_oos, 'LSTM_merged_1L_metrics', Y_name, scaler)\n",
    "LSTM_model_merged_2L_results_loop, LSTM_model_merged_2L_metrics_loop = create_metrics(test_data_loop, predictions_LSTM_model_merged_2L_loop, batch_size,n_factors, r2_oos, 'LSTM_merged_2L_metrics', Y_name, scaler)\n",
    "LSTM_model_merged_3L_results_loop, LSTM_model_merged_3L_metrics_loop = create_metrics(test_data_loop, predictions_LSTM_model_merged_3L_loop, batch_size,n_factors, r2_oos, 'LSTM_merged_3L_metrics', Y_name, scaler)\n",
    "LSTM_model_merged_4L_results_loop, LSTM_model_merged_4L_metrics_loop = create_metrics(test_data_loop, predictions_LSTM_model_merged_4L_loop, batch_size,n_factors, r2_oos, 'LSTM_merged_4L_metrics', Y_name, scaler)\n",
    "\n",
    "LSTM_model_1L_results_loop, LSTM_model_1L_metrics = create_metrics(test_data_loop, predictions_LSTM_model_1L_loop, batch_size,n_factors, r2_oos, 'LSTM_1L_metrics', Y_name, scaler)\n",
    "LSTM_model_2L_results_loop, LSTM_model_2L_metrics = create_metrics(test_data_loop, predictions_LSTM_model_2L_loop, batch_size,n_factors, r2_oos, 'LSTM_2L_metrics', Y_name, scaler)\n",
    "LSTM_model_3L_results_loop, LSTM_model_3L_metrics = create_metrics(test_data_loop, predictions_LSTM_model_3L_loop, batch_size,n_factors, r2_oos, 'LSTM_3L_metrics', Y_name, scaler)\n",
    "LSTM_model_4L_results_loop, LSTM_model_4L_metrics = create_metrics(test_data_loop, predictions_LSTM_model_4L_loop, batch_size,n_factors, r2_oos, 'LSTM_4L_metrics', Y_name, scaler)\n",
    "\n",
    "FFN_model_L1_results_loop, FFN_model_L1_metrics = create_metrics(test_data_loop, predictions_FFN_model_L1_loop, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics', Y_name, scaler)\n",
    "FFN_model_L2_results_loop, FFN_model_L2_metrics = create_metrics(test_data_loop, predictions_FFN_model_L2_loop, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics', Y_name, scaler)\n",
    "FFN_model_L3_results_loop, FFN_model_L3_metrics = create_metrics(test_data_loop, predictions_FFN_model_L3_loop, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics', Y_name, scaler)\n",
    "FFN_model_L4_results_loop, FFN_model_L4_metrics = create_metrics(test_data_loop, predictions_FFN_model_L4_loop, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics', Y_name, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"merged model save\"\"\"\n",
    "############################################################################################\n",
    "LSTM_model_merged_1L_results_loop.to_csv('LSTM_model_merged_1L_results_loop.csv')\n",
    "file_saver('LSTM_model_merged_1L_results_loop.csv', 'LSTM_model_merged_1L_results_loop.csv')\n",
    "\n",
    "LSTM_model_merged_1L_metrics_loop.to_csv('LSTM_model_merged_1L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_merged_1L_metrics_loop.csv', 'LSTM_model_merged_1L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_2L_results_loop.to_csv('LSTM_model_merged_2L_results_loop.csv')\n",
    "file_saver('LSTM_model_merged_2L_results_loop.csv', 'LSTM_model_merged_2L_results_loop.csv')\n",
    "\n",
    "LSTM_model_merged_2L_metrics_loop.to_csv('LSTM_model_merged_2L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_merged_2L_metrics_loop.csv', 'LSTM_model_merged_2L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_3L_results_loop.to_csv('LSTM_model_merged_3L_results_loop.csv')\n",
    "file_saver('LSTM_model_merged_3L_results_loop.csv', 'LSTM_model_merged_3L_results_loop.csv')\n",
    "\n",
    "LSTM_model_merged_3L_metrics_loop.to_csv('LSTM_model_merged_3L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_merged_3L_metrics_loop.csv', 'LSTM_model_merged_3L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_4L_results_loop.to_csv('LSTM_model_merged_4L_results_loop.csv')\n",
    "file_saver('LSTM_model_merged_4L_results_loop.csv', 'LSTM_model_merged_4L_results_loop.csv')\n",
    "\n",
    "LSTM_model_merged_4L_metrics_loop.to_csv('LSTM_model_merged_4L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_merged_4L_metrics_loop.csv', 'LSTM_model_merged_4L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "\"\"\" factor models only save\"\"\"\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_1L_results_loop.to_csv('LSTM_model_1L_results_loop.csv')\n",
    "file_saver('LSTM_model_1L_results_loop.csv', 'LSTM_model_1L_results_loop.csv')\n",
    "\n",
    "LSTM_model_1L_metrics_loop.to_csv('LSTM_model_1L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_1L_metrics_loop.csv', 'LSTM_model_1L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_2L_results_loop.to_csv('LSTM_model_2L_results_loop.csv')\n",
    "file_saver('LSTM_model_2L_results_loop.csv', 'LSTM_model_2L_results_loop.csv')\n",
    "\n",
    "LSTM_model_2L_metrics_loop.to_csv('LSTM_model_2L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_2L_metrics_loop.csv', 'LSTM_model_2L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_3L_results_loop.to_csv('LSTM_model_3L_results_loop.csv')\n",
    "file_saver('LSTM_model_3L_results_loop.csv', 'LSTM_model_3L_results_loop.csv')\n",
    "\n",
    "LSTM_model_3L_metrics_loop.to_csv('LSTM_model_3L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_3L_metrics_loop.csv', 'LSTM_model_3L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_4L_results_loop.to_csv('LSTM_model_4L_results_loop.csv')\n",
    "file_saver('LSTM_model_4L_results_loop.csv', 'LSTM_model_4L_results_loop.csv')\n",
    "\n",
    "LSTM_model_4L_metrics_loop.to_csv('LSTM_model_4L_metrics_loop.csv')\n",
    "file_saver('LSTM_model_4L_metrics_loop.csv', 'LSTM_model_4L_metrics_loop.csv')\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "\"\"\"combined model save \"\"\"\n",
    "############################################################################################\n",
    "FFN_model_L1_results_loop.to_csv('FFN_model_L1_results_loop.csv')\n",
    "file_saver('FFN_model_L1_results_loop.csv', 'FFN_model_L1_results_loop.csv')\n",
    "\n",
    "FFN_model_L1_metrics_loop.to_csv('FFN_model_L1_metrics_loop.csv')\n",
    "file_saver('FFN_model_L1_metrics_loop.csv', 'FFN_model_L1_metrics_loop.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L2_results_loop.to_csv('FFN_model_L2_results_loop.csv')\n",
    "file_saver('FFN_model_L2_results_loop.csv', 'FFN_model_L2_results_loop.csv')\n",
    "\n",
    "FFN_model_L2_metrics_loop.to_csv('FFN_model_L2_metrics_loop.csv')\n",
    "file_saver('FFN_model_L2_metrics_loop.csv', 'FFN_model_L2_metrics_loop.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L3_results_loop.to_csv('FFN_model_L3_results_loop.csv')\n",
    "file_saver('FFN_model_L3_results_loop.csv', 'FFN_model_L3_results_loop.csv')\n",
    "\n",
    "FFN_model_L3_metrics_loop.to_csv('FFN_model_L3_metrics_loop.csv')\n",
    "file_saver('FFN_model_L3_metrics_loop.csv', 'FFN_model_L3_metrics_loop.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L4_results_loop.to_csv('FFN_model_L4_results_loop.csv')\n",
    "file_saver('FFN_model_L4_results_loop.csv', 'FFN_model_L4_results_loop.csv')\n",
    "\n",
    "FFN_model_L4_metrics_loop.to_csv('FFN_model_L4_metrics_loop.csv')\n",
    "file_saver('FFN_model_L4_metrics_loop.csv', 'FFN_model_L4_metrics_loop.csv')\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_LSTM_model_merged_1L = 'checkpoints_LSTM_model_merged_1L'\n",
    "checkpoints_LSTM_model_merged_2L = 'checkpoints_LSTM_model_merged_2L'\n",
    "checkpoints_LSTM_model_merged_3L = 'checkpoints_LSTM_model_merged_3L'\n",
    "checkpoints_LSTM_model_merged_4L = 'checkpoints_LSTM_model_merged_4L'\n",
    "\n",
    "checkpoints_LSTM_model_1L = 'checkpoints_LSTM_model_1L'\n",
    "checkpoints_LSTM_model_2L = 'checkpoints_LSTM_model_2L'\n",
    "checkpoints_LSTM_model_3L = 'checkpoints_LSTM_model_3L'\n",
    "checkpoints_LSTM_model_4L = 'checkpoints_LSTM_model_4L'\n",
    "\n",
    "checkpoints_LSTM_model_macro_1L = 'checkpoints_LSTM_model_macro_1L'\n",
    "checkpoints_LSTM_model_macro_2L = 'checkpoints_LSTM_model_macro_2L'\n",
    "checkpoints_LSTM_model_macro_3L = 'checkpoints_LSTM_model_macro_3L'\n",
    "checkpoints_LSTM_model_macro_4L = 'checkpoints_LSTM_model_macro_4L'\n",
    "\n",
    "checkpoints_FFN_model_L1 = 'checkpoints_FFN_model_L1'\n",
    "checkpoints_FFN_model_L2 = 'checkpoints_FFN_model_L2'\n",
    "checkpoints_FFN_model_L3 = 'checkpoints_FFN_model_L3'\n",
    "checkpoints_FFN_model_L4 = 'checkpoints_FFN_model_L4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM_merged_1L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_1L)\n",
    "model_LSTM_merged_2L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_2L)\n",
    "model_LSTM_merged_3L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_3L)\n",
    "model_LSTM_merged_4L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_4L)\n",
    "\n",
    "model_LSTM_1L = tf.keras.models.load_model(checkpoints_LSTM_model_1L)\n",
    "model_LSTM_2L = tf.keras.models.load_model(checkpoints_LSTM_model_2L)\n",
    "model_LSTM_3L = tf.keras.models.load_model(checkpoints_LSTM_model_3L)\n",
    "model_LSTM_4L = tf.keras.models.load_model(checkpoints_LSTM_model_4L)\n",
    "\n",
    "model_FFN_1L = tf.keras.models.load_model(checkpoints_FFN_model_L1)\n",
    "model_FFN_2L = tf.keras.models.load_model(checkpoints_FFN_model_L2)\n",
    "model_FFN_3L = tf.keras.models.load_model(checkpoints_FFN_model_L3)\n",
    "model_FFN_4L = tf.keras.models.load_model(checkpoints_FFN_model_L4)\n",
    "\n",
    "\n",
    "xgb_model_factors = xgb.Booster()\n",
    "xgb_model_factors = xgb_model_factors.load_model(XGB_factors_only_file)\n",
    "\n",
    "xgb_model_merged = xgb.Booster()\n",
    "xgb_model_merged = xgb_model_factors.load_model(XGB_factors_only_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_factors = final_factor_data.columns.tolist()\n",
    "features_macro = final_macro_data.columns.tolist()\n",
    "features_merged = features_factors +features_macro\n",
    "\n",
    "background_factors = x_train_factors[:500]\n",
    "background_macro = x_train_macro[:500]\n",
    "background_merged = x_train_merged[:500]\n",
    "\n",
    "test_factors = x_test_factors[:500] \n",
    "test_macro = x_test_macro[:500]\n",
    "test_merged = x_test_merged[:500]\n",
    "\n",
    "explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "shap_values = explainer.shap_values(test_merged)\n",
    "shap.summary(shap_values)\n",
    "shap.bar_plot(shap_values)\n",
    "\n",
    "explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "shap_values = explainer.shap_values(test_merged)\n",
    "shap.summary(shap_values)\n",
    "shap.bar_plot(shap_values)\n",
    "\n",
    "explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "shap_values = explainer.shap_values(test_merged)\n",
    "shap.summary(shap_values)\n",
    "shap.bar_plot(shap_values)\n",
    "\n",
    "explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "shap_values = explainer.shap_values(test_merged)\n",
    "shap.summary(shap_values)\n",
    "shap.bar_plot(shap_values)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.r5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
