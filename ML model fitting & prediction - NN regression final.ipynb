{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c01b7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #### run once before each notebook execution and clear output\n",
    "# %pip install pandas_datareader \n",
    "# %pip install statsmodels\n",
    "# %pip install linearmodels\n",
    "# %pip install quantstats\n",
    "# %pip install wrds\n",
    "# %pip install pathlib\n",
    "# %pip install scipy\n",
    "# %pip install sklearn\n",
    "# %pip install pandas_datareader \n",
    "# %pip install tqdm\n",
    "# %pip install ZipFile\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install xgboost\n",
    "# %pip install quandl\n",
    "# %pip install keras\n",
    "# %pip install --upgrade tensorflow \n",
    "# %pip install shap\n",
    "# %conda install shap\n",
    "\n",
    "# # print('############################# completed installations #####################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a03fac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 16:02:50.078108: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-28 16:02:50.078150: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# after we install all the packages, import all of them for the use in today's lecture!\n",
    "# database access\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import s3fs\n",
    "import h5py\n",
    "import tempfile\n",
    "\n",
    "import quandl as quandl\n",
    "import wrds as wrds\n",
    "# storage and operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "# stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "from scipy import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import linearmodels as lm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#portfolio optimization:\n",
    "# libraries we might use for testing\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
    "import linearmodels as lm\n",
    "from linearmodels.panel import compare  \n",
    "import time\n",
    "import quantstats as qs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc\n",
    "\n",
    "# path_factors = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\Factors')\n",
    "# path_wrds = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\WRDS')\n",
    "# path_ml = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Machine Learning Models')\n",
    "# path_visual = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Visualisation/')\n",
    "# path_betas = Path(r'C:\\Users\\mauri\\FSoF\\Frankfurt School of Finance - Master Thesis\\Master Thesis - Data\\Factor Betas')\n",
    "\n",
    "# factor_data = path_factors / 'factor_data.h5'\n",
    "# macro_data = path_factors / 'macro_data.h5'\n",
    "# wrds_data = path_wrds / 'wrds_data.h5'\n",
    "# saved_models = path_ml / 'saved_ml_models.h5'\n",
    "\n",
    "#bucket s3 AWS\n",
    "path_bucket = 's3://mm-master-thesis-data/'\n",
    "path_bucket_ml = 's3://mm-master-thesis-data/ml-models/'\n",
    "\n",
    "#factor data \n",
    "factor_data = path_bucket + \"monthly_factor_data.csv\"\n",
    "factor_data_adj = path_bucket + 'monthly_factors_adjusted.csv'\n",
    "factor_data_base = path_bucket + 'monthly_factors_base.csv'\n",
    "factor_data_base_extended = path_bucket + 'monthly_factors_base_extended.csv'\n",
    "\n",
    "benchmark_ff = path_bucket + 'benchmark_factor_data.csv'\n",
    "benchmark_bk = path_bucket + 'benchmark_factor_data_bkelly.csv'\n",
    "\n",
    "#macro data \n",
    "macro_data = path_bucket + 'macro_data.csv'\n",
    "\n",
    "# stock data\n",
    "wrds_data_file = path_bucket +'wrds_data_complete_abs.csv' \n",
    "wrds_data_file_wins = path_bucket + 'monthly_wrds_data_complete_wins.csv'\n",
    "wrds_data_file_wins_quin = path_bucket + 'monthly_stock_data_wins_quin.csv'\n",
    "wrds_data_file_final = path_bucket + 'monthly_stock_data_final.csv'\n",
    "\n",
    "#pca factors\n",
    "pca_factor_path = path_bucket + 'monthly_pca_factors.csv'\n",
    "\n",
    "\n",
    "# prediction results\n",
    "OLS_df = path_bucket + 'ols_results.csv'\n",
    "Ridge_df = path_bucket + 'ridge_results.csv'\n",
    "Lasso_df = path_bucket + 'lasso_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f77700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21eaf8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def file_saver(data_object, data_name_as_string):\n",
    "    saver = boto3.Session().resource('s3').Bucket('mm-master-thesis-data').Object(data_object).upload_file(data_name_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfecd3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def winsorizer(df, columns, limits):\n",
    "    \n",
    "    winsorized_df = df.copy(deep=True)\n",
    "    for c in columns:\n",
    "        goods    = winsorized_df[c].notna()\n",
    "        winsorized_df.loc[goods,c] = winsorize(winsorized_df.loc[goods,c], limits=limits)\n",
    "\n",
    "    return winsorized_df\n",
    "\n",
    "def find_min_max(df, number_of_extremes, variable):\n",
    "    if isinstance(df,pd.DataFrame):\n",
    "\n",
    "        max_df_returns = df.max().tolist()\n",
    "        max_df_returns.sort(reverse=True)\n",
    "        number_max_returns = max_df_returns[:number_of_extremes]\n",
    "        number_max_returns = [ '%.2f' % elem for elem in number_max_returns ]\n",
    "        print('The', number_of_extremes, 'highest', variable, 'are: ') \n",
    "        print(number_max_returns)\n",
    "        print('')\n",
    "        min_df_returns = df.min().tolist()\n",
    "        min_df_returns.sort()\n",
    "        number_min_returns = min_df_returns[:number_of_extremes]\n",
    "        number_min_returns = [ '%.2f' % elem for elem in number_min_returns ]\n",
    "        print('The', number_of_extremes, 'lowest', variable, 'are: ') \n",
    "        print(number_min_returns)\n",
    "    elif isinstance(df,pd.Series):\n",
    "        max_df_returns = df.values.tolist()\n",
    "        max_df_returns.sort(reverse=True)\n",
    "        number_max_returns = max_df_returns[:number_of_extremes]\n",
    "        number_max_returns = [ '%.2f' % elem for elem in number_max_returns ]\n",
    "        print('The', number_of_extremes, 'highest', variable, 'are: ') \n",
    "        print(number_max_returns)\n",
    "        print('')\n",
    "        min_df_returns = df.values.tolist()\n",
    "        min_df_returns.sort()\n",
    "        number_min_returns = min_df_returns[:number_of_extremes]\n",
    "        number_min_returns = [ '%.2f' % elem for elem in number_min_returns ]\n",
    "        print('The', number_of_extremes, 'lowest', variable, 'are: ') \n",
    "        print(number_min_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c81517",
   "metadata": {},
   "source": [
    "# Macro Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5301913d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>bm</th>\n",
       "      <th>tbl</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>...</th>\n",
       "      <th>DDURRG3M086SBEA</th>\n",
       "      <th>DNDGRG3M086SBEA</th>\n",
       "      <th>DSERRG3M086SBEA</th>\n",
       "      <th>CES0600000008</th>\n",
       "      <th>CES2000000008</th>\n",
       "      <th>CES3000000008</th>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <th>INVEST</th>\n",
       "      <th>VIXCLSx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>4.407</td>\n",
       "      <td>10.517</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>85.536</td>\n",
       "      <td>35.172</td>\n",
       "      <td>24.873</td>\n",
       "      <td>5.97</td>\n",
       "      <td>7.54</td>\n",
       "      <td>5.58</td>\n",
       "      <td>12900.00</td>\n",
       "      <td>35574.00</td>\n",
       "      <td>230.302</td>\n",
       "      <td>15.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>4.453</td>\n",
       "      <td>10.613</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>85.817</td>\n",
       "      <td>35.270</td>\n",
       "      <td>25.027</td>\n",
       "      <td>5.99</td>\n",
       "      <td>7.56</td>\n",
       "      <td>5.61</td>\n",
       "      <td>13058.00</td>\n",
       "      <td>36030.00</td>\n",
       "      <td>230.172</td>\n",
       "      <td>14.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>4.500</td>\n",
       "      <td>10.710</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>85.946</td>\n",
       "      <td>35.317</td>\n",
       "      <td>25.173</td>\n",
       "      <td>6.03</td>\n",
       "      <td>7.62</td>\n",
       "      <td>5.65</td>\n",
       "      <td>13177.00</td>\n",
       "      <td>36241.00</td>\n",
       "      <td>230.965</td>\n",
       "      <td>14.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>4.557</td>\n",
       "      <td>10.770</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>86.439</td>\n",
       "      <td>35.456</td>\n",
       "      <td>25.293</td>\n",
       "      <td>6.08</td>\n",
       "      <td>7.66</td>\n",
       "      <td>5.69</td>\n",
       "      <td>14686.00</td>\n",
       "      <td>37965.00</td>\n",
       "      <td>229.856</td>\n",
       "      <td>15.454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>4.613</td>\n",
       "      <td>10.830</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>...</td>\n",
       "      <td>86.802</td>\n",
       "      <td>35.699</td>\n",
       "      <td>25.434</td>\n",
       "      <td>6.11</td>\n",
       "      <td>7.68</td>\n",
       "      <td>5.72</td>\n",
       "      <td>14975.00</td>\n",
       "      <td>38553.00</td>\n",
       "      <td>231.645</td>\n",
       "      <td>19.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>59.129</td>\n",
       "      <td>98.557</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>...</td>\n",
       "      <td>86.662</td>\n",
       "      <td>98.675</td>\n",
       "      <td>120.600</td>\n",
       "      <td>25.50</td>\n",
       "      <td>29.43</td>\n",
       "      <td>22.85</td>\n",
       "      <td>344023.25</td>\n",
       "      <td>726723.85</td>\n",
       "      <td>4385.243</td>\n",
       "      <td>22.879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>58.851</td>\n",
       "      <td>98.220</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>86.616</td>\n",
       "      <td>98.508</td>\n",
       "      <td>120.964</td>\n",
       "      <td>25.46</td>\n",
       "      <td>29.11</td>\n",
       "      <td>23.00</td>\n",
       "      <td>347627.43</td>\n",
       "      <td>730734.42</td>\n",
       "      <td>4452.042</td>\n",
       "      <td>27.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>58.660</td>\n",
       "      <td>96.857</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>86.532</td>\n",
       "      <td>98.398</td>\n",
       "      <td>121.094</td>\n",
       "      <td>25.54</td>\n",
       "      <td>29.31</td>\n",
       "      <td>22.99</td>\n",
       "      <td>348262.68</td>\n",
       "      <td>730398.69</td>\n",
       "      <td>4514.528</td>\n",
       "      <td>29.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11</th>\n",
       "      <td>58.470</td>\n",
       "      <td>95.493</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>86.339</td>\n",
       "      <td>98.547</td>\n",
       "      <td>121.109</td>\n",
       "      <td>25.67</td>\n",
       "      <td>29.49</td>\n",
       "      <td>23.11</td>\n",
       "      <td>350766.10</td>\n",
       "      <td>733096.73</td>\n",
       "      <td>4621.237</td>\n",
       "      <td>24.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12</th>\n",
       "      <td>58.279</td>\n",
       "      <td>94.130</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>86.486</td>\n",
       "      <td>98.992</td>\n",
       "      <td>121.597</td>\n",
       "      <td>25.78</td>\n",
       "      <td>29.63</td>\n",
       "      <td>23.18</td>\n",
       "      <td>350336.43</td>\n",
       "      <td>733463.42</td>\n",
       "      <td>4685.876</td>\n",
       "      <td>22.383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D12     E12     bm    tbl    lty   ntis  Rfree   infl    ltr  \\\n",
       "date                                                                       \n",
       "1977-07   4.407  10.517  0.897  0.052  0.077  0.033  0.004  0.005 -0.007   \n",
       "1977-08   4.453  10.613  0.927  0.055  0.075  0.034  0.004  0.003  0.020   \n",
       "1977-09   4.500  10.710  0.942  0.058  0.076  0.032  0.004  0.003 -0.003   \n",
       "1977-10   4.557  10.770  0.975  0.062  0.078  0.033  0.005  0.003 -0.009   \n",
       "1977-11   4.613  10.830  0.962  0.061  0.078  0.029  0.005  0.005  0.009   \n",
       "...         ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2020-08  59.129  98.557  0.236  0.001  0.006 -0.009  0.000  0.003 -0.035   \n",
       "2020-09  58.851  98.220  0.241  0.001  0.007 -0.006  0.000  0.001  0.008   \n",
       "2020-10  58.660  96.857  0.253  0.001  0.008 -0.002  0.000  0.000 -0.024   \n",
       "2020-11  58.470  95.493  0.226  0.001  0.009 -0.005  0.000 -0.001  0.009   \n",
       "2020-12  58.279  94.130  0.219  0.001  0.009 -0.000  0.000  0.001 -0.011   \n",
       "\n",
       "         corpr  ...  DDURRG3M086SBEA  DNDGRG3M086SBEA  DSERRG3M086SBEA  \\\n",
       "date            ...                                                      \n",
       "1977-07 -0.001  ...           85.536           35.172           24.873   \n",
       "1977-08  0.014  ...           85.817           35.270           25.027   \n",
       "1977-09 -0.002  ...           85.946           35.317           25.173   \n",
       "1977-10 -0.004  ...           86.439           35.456           25.293   \n",
       "1977-11  0.006  ...           86.802           35.699           25.434   \n",
       "...        ...  ...              ...              ...              ...   \n",
       "2020-08 -0.049  ...           86.662           98.675          120.600   \n",
       "2020-09  0.004  ...           86.616           98.508          120.964   \n",
       "2020-10 -0.019  ...           86.532           98.398          121.094   \n",
       "2020-11  0.051  ...           86.339           98.547          121.109   \n",
       "2020-12  0.000  ...           86.486           98.992          121.597   \n",
       "\n",
       "         CES0600000008  CES2000000008  CES3000000008  DTCOLNVHFNM   DTCTHFNM  \\\n",
       "date                                                                           \n",
       "1977-07           5.97           7.54           5.58     12900.00   35574.00   \n",
       "1977-08           5.99           7.56           5.61     13058.00   36030.00   \n",
       "1977-09           6.03           7.62           5.65     13177.00   36241.00   \n",
       "1977-10           6.08           7.66           5.69     14686.00   37965.00   \n",
       "1977-11           6.11           7.68           5.72     14975.00   38553.00   \n",
       "...                ...            ...            ...          ...        ...   \n",
       "2020-08          25.50          29.43          22.85    344023.25  726723.85   \n",
       "2020-09          25.46          29.11          23.00    347627.43  730734.42   \n",
       "2020-10          25.54          29.31          22.99    348262.68  730398.69   \n",
       "2020-11          25.67          29.49          23.11    350766.10  733096.73   \n",
       "2020-12          25.78          29.63          23.18    350336.43  733463.42   \n",
       "\n",
       "           INVEST  VIXCLSx  \n",
       "date                        \n",
       "1977-07   230.302   15.362  \n",
       "1977-08   230.172   14.493  \n",
       "1977-09   230.965   14.087  \n",
       "1977-10   229.856   15.454  \n",
       "1977-11   231.645   19.638  \n",
       "...           ...      ...  \n",
       "2020-08  4385.243   22.879  \n",
       "2020-09  4452.042   27.587  \n",
       "2020-10  4514.528   29.436  \n",
       "2020-11  4621.237   24.390  \n",
       "2020-12  4685.876   22.383  \n",
       "\n",
       "[522 rows x 136 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_factors = pd.read_csv(macro_data)\n",
    "macro_factors = macro_factors.set_index('date',drop=True)\n",
    "macro_factors = macro_factors.applymap(lambda x: f'{x:.3f}')\n",
    "macro_factors = macro_factors.drop(columns=['Unnamed: 0'])\n",
    "macro_factors = macro_factors.astype(float)\n",
    "macro_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bea9b1",
   "metadata": {},
   "source": [
    "# Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77746389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0trade</th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>adm</th>\n",
       "      <th>almq_1</th>\n",
       "      <th>amihud</th>\n",
       "      <th>atoq_1</th>\n",
       "      <th>bab</th>\n",
       "      <th>bm</th>\n",
       "      <th>bmj</th>\n",
       "      <th>...</th>\n",
       "      <th>ta</th>\n",
       "      <th>vfp</th>\n",
       "      <th>vhp</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>fp_6</th>\n",
       "      <th>iaq_12</th>\n",
       "      <th>p52w_6</th>\n",
       "      <th>r6_1</th>\n",
       "      <th>resid6</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>-0.049</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11</th>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12</th>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0trade  abr_1    aci    adm  almq_1  amihud  atoq_1    bab     bm  \\\n",
       "date                                                                         \n",
       "1977-07   0.019 -0.005 -0.021  0.013  -0.017   0.013   0.011  0.029 -0.011   \n",
       "1977-08  -0.006  0.017 -0.004 -0.004  -0.017   0.001   0.018  0.003 -0.034   \n",
       "1977-09   0.010  0.018 -0.003 -0.008  -0.005   0.012  -0.007  0.004 -0.002   \n",
       "1977-10   0.011 -0.007 -0.006  0.012   0.016   0.017  -0.000  0.002  0.014   \n",
       "1977-11  -0.031  0.032 -0.010  0.001   0.024   0.041   0.007  0.003  0.011   \n",
       "...         ...    ...    ...    ...     ...     ...     ...    ...    ...   \n",
       "2020-08  -0.049  0.018 -0.011  0.000   0.009  -0.014   0.030 -0.040 -0.010   \n",
       "2020-09  -0.006  0.006 -0.013 -0.010  -0.004  -0.005   0.011  0.013 -0.028   \n",
       "2020-10  -0.012 -0.003  0.008  0.045   0.056   0.041   0.002 -0.020  0.025   \n",
       "2020-11  -0.100 -0.014 -0.032  0.021   0.088   0.078  -0.058 -0.051  0.056   \n",
       "2020-12  -0.042  0.010 -0.008 -0.066  -0.027   0.047   0.005  0.053 -0.046   \n",
       "\n",
       "           bmj  ...     ta    vfp    vhp  dtv_12   fp_6  iaq_12  p52w_6  \\\n",
       "date            ...                                                       \n",
       "1977-07 -0.004  ... -0.006  0.009 -0.008  -0.011 -0.009  -0.007   0.008   \n",
       "1977-08 -0.033  ...  0.016 -0.006 -0.030  -0.066 -0.025   0.017  -0.018   \n",
       "1977-09 -0.014  ...  0.011 -0.010  0.019  -0.071 -0.022   0.005   0.022   \n",
       "1977-10  0.019  ...  0.007 -0.108  0.011   0.055 -0.008   0.000   0.010   \n",
       "1977-11  0.004  ...  0.004  0.029  0.004  -0.005  0.013  -0.014  -0.005   \n",
       "...        ...  ...    ...    ...    ...     ...    ...     ...     ...   \n",
       "2020-08  0.032  ... -0.001 -0.006  0.038   0.036  0.006   0.023  -0.011   \n",
       "2020-09 -0.063  ...  0.020 -0.006  0.006   0.141 -0.018   0.012   0.049   \n",
       "2020-10  0.043  ... -0.000  0.005  0.002  -0.036  0.003  -0.000  -0.027   \n",
       "2020-11  0.113  ... -0.026  0.013  0.035   0.083  0.219  -0.018  -0.197   \n",
       "2020-12 -0.018  ... -0.014 -0.024 -0.019  -0.006  0.019  -0.004  -0.017   \n",
       "\n",
       "          r6_1  resid6  tbiq_12  \n",
       "date                             \n",
       "1977-07 -0.016   0.001    0.015  \n",
       "1977-08 -0.012  -0.017   -0.003  \n",
       "1977-09  0.025   0.007    0.013  \n",
       "1977-10 -0.007   0.003    0.008  \n",
       "1977-11  0.026   0.007    0.001  \n",
       "...        ...     ...      ...  \n",
       "2020-08 -0.034  -0.025    0.022  \n",
       "2020-09  0.043  -0.014   -0.030  \n",
       "2020-10 -0.014  -0.006   -0.013  \n",
       "2020-11  0.017   0.005    0.019  \n",
       "2020-12  0.014  -0.013    0.002  \n",
       "\n",
       "[522 rows x 106 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors = pd.read_csv(factor_data)\n",
    "monthly_factors= monthly_factors.set_index('date',drop=True)\n",
    "monthly_factors = monthly_factors.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors= monthly_factors.astype('float')\n",
    "monthly_factors= monthly_factors.drop(columns=['Unnamed: 0','poa_y','rf'])\n",
    "monthly_factors= monthly_factors.rename(columns={'poa_x':'poa'})\n",
    "monthly_factors= monthly_factors.loc[(monthly_factors.index>=('1977-01'))&(monthly_factors.index<=('2020-12'))]\n",
    "monthly_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5945ba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>amihud</th>\n",
       "      <th>bab</th>\n",
       "      <th>def_1</th>\n",
       "      <th>dfin</th>\n",
       "      <th>dii</th>\n",
       "      <th>dlno</th>\n",
       "      <th>dsre</th>\n",
       "      <th>ep</th>\n",
       "      <th>...</th>\n",
       "      <th>pda</th>\n",
       "      <th>poa</th>\n",
       "      <th>qmj</th>\n",
       "      <th>r1a</th>\n",
       "      <th>rer</th>\n",
       "      <th>rev_1</th>\n",
       "      <th>roe_1</th>\n",
       "      <th>sm_1</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abr_1    aci  amihud    bab  def_1   dfin    dii   dlno   dsre  \\\n",
       "date                                                                      \n",
       "1977-07 -0.005 -0.021   0.013  0.029  0.026  0.007 -0.016  0.002 -0.004   \n",
       "1977-08  0.017 -0.004   0.001  0.003  0.004  0.020  0.001  0.001 -0.008   \n",
       "1977-09  0.018 -0.003   0.012  0.004  0.057  0.000  0.000  0.013 -0.007   \n",
       "1977-10 -0.007 -0.006   0.017  0.002  0.009  0.007  0.003 -0.005 -0.004   \n",
       "1977-11  0.032 -0.010   0.041  0.003  0.021  0.017 -0.003 -0.019 -0.007   \n",
       "\n",
       "            ep  ...    pda    poa    qmj    r1a    rer  rev_1  roe_1   sm_1  \\\n",
       "date            ...                                                           \n",
       "1977-07  0.001  ... -0.005  0.005 -0.008  0.013  0.012 -0.016  0.018 -0.012   \n",
       "1977-08 -0.035  ... -0.002  0.010  0.009  0.021 -0.007 -0.023  0.018  0.004   \n",
       "1977-09  0.014  ...  0.011  0.007  0.031  0.008  0.002 -0.014  0.023 -0.011   \n",
       "1977-10  0.015  ... -0.000  0.010  0.004  0.013  0.019  0.008 -0.013 -0.010   \n",
       "1977-11 -0.000  ... -0.010 -0.009  0.011  0.019 -0.014 -0.034  0.042  0.017   \n",
       "\n",
       "         dtv_12  tbiq_12  \n",
       "date                      \n",
       "1977-07  -0.011    0.015  \n",
       "1977-08  -0.066   -0.003  \n",
       "1977-09  -0.071    0.013  \n",
       "1977-10   0.055    0.008  \n",
       "1977-11  -0.005    0.001  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors_adj = pd.read_csv(path_bucket+'monthly_factors_adjusted.csv')\n",
    "monthly_factors_adj = monthly_factors_adj.set_index('date',drop=True)\n",
    "monthly_factors_adj = monthly_factors_adj.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors_adj = monthly_factors_adj.astype('float')\n",
    "monthly_factors_adj= monthly_factors_adj.loc[(monthly_factors_adj.index>=('1977-01'))&(monthly_factors_adj.index<=('2020-12'))]\n",
    "\n",
    "monthly_factors_adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c52e0d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abr_1</th>\n",
       "      <th>aci</th>\n",
       "      <th>amihud</th>\n",
       "      <th>bab</th>\n",
       "      <th>cla</th>\n",
       "      <th>ctoq_1</th>\n",
       "      <th>def_1</th>\n",
       "      <th>dfin</th>\n",
       "      <th>dii</th>\n",
       "      <th>dlno</th>\n",
       "      <th>...</th>\n",
       "      <th>qmj</th>\n",
       "      <th>r1a</th>\n",
       "      <th>rer</th>\n",
       "      <th>rev_1</th>\n",
       "      <th>roa_1</th>\n",
       "      <th>roe_1</th>\n",
       "      <th>sm_1</th>\n",
       "      <th>sp</th>\n",
       "      <th>dtv_12</th>\n",
       "      <th>tbiq_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abr_1    aci  amihud    bab    cla  ctoq_1  def_1   dfin    dii  \\\n",
       "date                                                                       \n",
       "1977-07 -0.005 -0.021   0.013  0.029  0.024   0.003  0.026  0.007 -0.016   \n",
       "1977-08  0.017 -0.004   0.001  0.003  0.022   0.017  0.004  0.020  0.001   \n",
       "1977-09  0.018 -0.003   0.012  0.004  0.022  -0.007  0.057  0.000  0.000   \n",
       "1977-10 -0.007 -0.006   0.017  0.002  0.003  -0.003  0.009  0.007  0.003   \n",
       "1977-11  0.032 -0.010   0.041  0.003  0.007   0.018  0.021  0.017 -0.003   \n",
       "\n",
       "          dlno  ...    qmj    r1a    rer  rev_1  roa_1  roe_1   sm_1     sp  \\\n",
       "date            ...                                                           \n",
       "1977-07  0.002  ... -0.008  0.013  0.012 -0.016  0.012  0.018 -0.012 -0.005   \n",
       "1977-08  0.001  ...  0.009  0.021 -0.007 -0.023  0.022  0.018  0.004 -0.027   \n",
       "1977-09  0.013  ...  0.031  0.008  0.002 -0.014  0.013  0.023 -0.011 -0.007   \n",
       "1977-10 -0.005  ...  0.004  0.013  0.019  0.008 -0.007 -0.013 -0.010  0.006   \n",
       "1977-11 -0.019  ...  0.011  0.019 -0.014 -0.034  0.035  0.042  0.017  0.012   \n",
       "\n",
       "         dtv_12  tbiq_12  \n",
       "date                      \n",
       "1977-07  -0.011    0.015  \n",
       "1977-08  -0.066   -0.003  \n",
       "1977-09  -0.071    0.013  \n",
       "1977-10   0.055    0.008  \n",
       "1977-11  -0.005    0.001  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_factors_base = pd.read_csv(path_bucket+'monthly_factors_base.csv')\n",
    "monthly_factors_base = monthly_factors_base.set_index('date',drop=True)\n",
    "monthly_factors_base = monthly_factors_base.applymap(lambda x: f'{x:.3f}')\n",
    "monthly_factors_base = monthly_factors_base.astype('float')\n",
    "monthly_factors_base= monthly_factors_base.loc[(monthly_factors_base.index>=('1977-01'))&(monthly_factors_base.index<=('2020-12'))]\n",
    "\n",
    "monthly_factors_base.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92ea5a",
   "metadata": {},
   "source": [
    "# Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71a998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_stock_data = pd.read_csv(wrds_data_file_final)\n",
    "monthly_stock_data = monthly_stock_data.loc[(monthly_stock_data.date>=('1977-01'))&(monthly_stock_data.date<=('2020-12'))]\n",
    "monthly_stock_data = monthly_stock_data.set_index('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac95a80",
   "metadata": {},
   "source": [
    "## Adding s&p 500 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8343ebde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "component_list = pd.read_csv(path_bucket+'sp500_permnos.csv')\n",
    "component_list.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "component_list =  component_list.rename(columns={'0':'permno'})\n",
    "len(component_list.permno.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37497a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data = monthly_stock_data.loc[monthly_stock_data.permno.isin(component_list.permno.unique())]\n",
    "len(monthly_stock_data.permno.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1eb4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly_stock_data\n",
    "# data_to_scale = np.array(monthly_stock_data['fwd_quintile'])\n",
    "# data_to_scale = data_to_scale.reshape(-1,1)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(data_to_scale)\n",
    "# monthly_stock_data['scaled_Y'] = scaled_data\n",
    "# monthly_stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0294b85",
   "metadata": {},
   "source": [
    "## For Categorization purposes the quintiles need to be shifted by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c5cffb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>prc</th>\n",
       "      <th>ret</th>\n",
       "      <th>shrout</th>\n",
       "      <th>mktcap</th>\n",
       "      <th>vweights</th>\n",
       "      <th>ret_mk</th>\n",
       "      <th>monthly_exret</th>\n",
       "      <th>fwd_monthly_exret</th>\n",
       "      <th>quintile</th>\n",
       "      <th>fwd_quintile</th>\n",
       "      <th>fwd_quintile_adj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-04</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.197605</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.297250e+05</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.002530</td>\n",
       "      <td>0.200135</td>\n",
       "      <td>-0.113263</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-05</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>-0.060000</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.099415e+05</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.053263</td>\n",
       "      <td>-0.113263</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-06</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>24.375000</td>\n",
       "      <td>0.037234</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>3.214819e+05</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>-0.314516</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-07</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>-0.335632</td>\n",
       "      <td>13189.0</td>\n",
       "      <td>2.044295e+05</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.049587</td>\n",
       "      <td>-0.314516</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08</th>\n",
       "      <td>10104.0</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>13242.0</td>\n",
       "      <td>2.184930e+05</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.074085</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>-0.060247</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>294.950012</td>\n",
       "      <td>-0.028375</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.797247e+08</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>-0.055658</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>302.779999</td>\n",
       "      <td>0.026547</td>\n",
       "      <td>948380.0</td>\n",
       "      <td>2.871505e+08</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.059811</td>\n",
       "      <td>-0.033264</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>312.549988</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.970275e+08</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.069506</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>311.769989</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.962862e+08</td>\n",
       "      <td>0.008568</td>\n",
       "      <td>-0.031868</td>\n",
       "      <td>0.033372</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>92655.0</td>\n",
       "      <td>305.140015</td>\n",
       "      <td>-0.021266</td>\n",
       "      <td>950336.0</td>\n",
       "      <td>2.899855e+08</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>-0.019232</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>-0.019856</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200393 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          permno         prc       ret    shrout        mktcap  vweights  \\\n",
       "date                                                                       \n",
       "1986-04  10104.0   25.000000  0.197605   13189.0  3.297250e+05  0.000139   \n",
       "1986-05  10104.0   23.500000 -0.060000   13189.0  3.099415e+05  0.000142   \n",
       "1986-06  10104.0   24.375000  0.037234   13189.0  3.214819e+05  0.000096   \n",
       "1986-07  10104.0   15.500000 -0.335632   13189.0  2.044295e+05  0.000096   \n",
       "1986-08  10104.0   16.500000  0.064516   13242.0  2.184930e+05  0.000090   \n",
       "...          ...         ...       ...       ...           ...       ...   \n",
       "2020-06  92655.0  294.950012 -0.028375  948380.0  2.797247e+08  0.008455   \n",
       "2020-07  92655.0  302.779999  0.026547  948380.0  2.871505e+08  0.008236   \n",
       "2020-08  92655.0  312.549988  0.032268  950336.0  2.970275e+08  0.008534   \n",
       "2020-09  92655.0  311.769989  0.001504  950336.0  2.962862e+08  0.008568   \n",
       "2020-10  92655.0  305.140015 -0.021266  950336.0  2.899855e+08  0.008495   \n",
       "\n",
       "           ret_mk  monthly_exret  fwd_monthly_exret  quintile  fwd_quintile  \\\n",
       "date                                                                          \n",
       "1986-04 -0.002530       0.200135          -0.113263         4             4   \n",
       "1986-05  0.053263      -0.113263           0.016762         4             4   \n",
       "1986-06  0.020472       0.016762          -0.314516         4             3   \n",
       "1986-07 -0.049587      -0.314516          -0.009569         3             3   \n",
       "1986-08  0.074085      -0.009569          -0.060247         3             3   \n",
       "...           ...            ...                ...       ...           ...   \n",
       "2020-06  0.027283      -0.055658          -0.033264         5             5   \n",
       "2020-07  0.059811      -0.033264          -0.037238         5             5   \n",
       "2020-08  0.069506      -0.037238           0.033372         5             5   \n",
       "2020-09 -0.031868       0.033372          -0.002034         5             5   \n",
       "2020-10 -0.019232      -0.002034          -0.019856         5             5   \n",
       "\n",
       "         fwd_quintile_adj  \n",
       "date                       \n",
       "1986-04                 3  \n",
       "1986-05                 3  \n",
       "1986-06                 2  \n",
       "1986-07                 2  \n",
       "1986-08                 2  \n",
       "...                   ...  \n",
       "2020-06                 4  \n",
       "2020-07                 4  \n",
       "2020-08                 4  \n",
       "2020-09                 4  \n",
       "2020-10                 4  \n",
       "\n",
       "[200393 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monthly_stock_data['fwd_quintile_adj'] = monthly_stock_data['fwd_quintile']-1\n",
    "monthly_stock_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee12df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prediction months:  520\n",
      "total number of stocks:  501\n"
     ]
    }
   ],
   "source": [
    "print('total prediction months: ', len(monthly_stock_data.index.unique()))\n",
    "print('total number of stocks: ', len(monthly_stock_data.permno.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "010595bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "1977-07      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "1977-08      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "1977-09      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "1977-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "1977-11      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "1977-07      4.0      2.0      3.0  ...      3.0      4.0      4.0      3.0   \n",
       "1977-08      4.0      2.0      3.0  ...      3.0      4.0      4.0      3.0   \n",
       "1977-09      4.0      2.0      3.0  ...      3.0      4.0      4.0      3.0   \n",
       "1977-10      4.0      2.0      3.0  ...      3.0      4.0      4.0      3.0   \n",
       "1977-11      4.0      2.0      3.0  ...      3.0      4.0      4.0      3.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "1977-07      4.0      3.0      4.0      4.0      3.0      3.0  \n",
       "1977-08      4.0      3.0      4.0      4.0      3.0      3.0  \n",
       "1977-09      4.0      3.0      4.0      4.0      3.0      3.0  \n",
       "1977-10      4.0      3.0      4.0      4.0      3.0      3.0  \n",
       "1977-11      4.0      3.0      4.0      4.0      3.0      3.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "\n",
       "[520 rows x 501 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_stocks = pd.pivot_table(data=monthly_stock_data, values ='fwd_quintile_adj', index=monthly_stock_data.index, columns='permno')\n",
    "pivot_table_stocks_avg = pivot_table_stocks.apply(lambda x: x.fillna(x.mean(),axis=0))\n",
    "pivot_table_stocks_avg = pivot_table_stocks_avg.round()\n",
    "pivot_table_stocks_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cedf0c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "2008-11      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2008-12      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-01      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-02      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-03      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "2008-11      4.0      1.0      4.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-12      4.0      1.0      4.0  ...      1.0      3.0      4.0      2.0   \n",
       "2009-01      4.0      1.0      4.0  ...      1.0      2.0      3.0      3.0   \n",
       "2009-02      4.0      1.0      4.0  ...      1.0      2.0      4.0      3.0   \n",
       "2009-03      4.0      1.0      4.0  ...      1.0      3.0      4.0      3.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "2008-11      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-12      1.0      3.0      4.0      4.0      3.0      4.0  \n",
       "2009-01      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-02      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-03      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "\n",
       "[108 rows x 501 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_stocks_total = pivot_table_stocks.dropna().sort_values(by='date')\n",
    "pivot_table_stocks_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51712690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2009-03      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2009-02      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-01      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2008-12      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2008-11      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2009-03      4.0      1.0      4.0  ...      1.0      3.0      4.0      3.0   \n",
       "2009-02      4.0      1.0      4.0  ...      1.0      2.0      4.0      3.0   \n",
       "2009-01      4.0      1.0      4.0  ...      1.0      2.0      3.0      3.0   \n",
       "2008-12      4.0      1.0      4.0  ...      1.0      3.0      4.0      2.0   \n",
       "2008-11      4.0      1.0      4.0  ...      2.0      3.0      3.0      3.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2009-03      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-02      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-01      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-12      1.0      3.0      4.0      4.0      3.0      4.0  \n",
       "2008-11      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "\n",
       "[108 rows x 501 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_stocks_total.sort_values(by='date', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701f412",
   "metadata": {},
   "source": [
    "## Data issues with multi classification\n",
    "\n",
    "Taking a look at the graph below, it can be noticed that the data set is highly imbalanced. Therefore each label class receives a weight to count the imbalance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a7511f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f4dc0b143a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(monthly_stock_data['fwd_quintile_adj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85c785fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2 3 4], y=date\n",
      "1986-04    3\n",
      "1986-05    3\n",
      "1986-06    2\n",
      "1986-07    2\n",
      "1986-08    2\n",
      "          ..\n",
      "2020-06    4\n",
      "2020-07    4\n",
      "2020-08    4\n",
      "2020-09    4\n",
      "2020-10    4\n",
      "Name: fwd_quintile_adj, Length: 200393, dtype: int64 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 17.68693733451015,\n",
       " 1: 4.343151278716949,\n",
       " 2: 1.85781300699949,\n",
       " 3: 0.7196215032139908,\n",
       " 4: 0.359024294109216}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights= class_weight.compute_class_weight('balanced', np.unique(monthly_stock_data['fwd_quintile_adj'].values), monthly_stock_data['fwd_quintile_adj'])\n",
    "class_weights_dict  ={0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3],4:class_weights[4]}\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0713ebf",
   "metadata": {},
   "source": [
    "# Prediction Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "99d59488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stock_data_prop = pivot_table_stocks_total.copy(deep=True).sort_values(by='date', ascending=False)\n",
    "final_factor_data_prop = monthly_factors_adj.copy(deep=True).sort_values(by='date', ascending=False)\n",
    "final_macro_data_prop = macro_factors.copy(deep=True).sort_values(by='date', ascending=False)\n",
    "test_data_factors_prop =  monthly_factors_adj.loc[(monthly_factors_adj.index>=('1977-01'))&(monthly_factors_adj.index<('2008-11'))].sort_values(by='date', ascending=False)\n",
    "test_data_macro_prop = macro_factors.loc[(macro_factors.index>=('1977-01'))&(macro_factors.index<('2008-11'))].sort_values(by='date', ascending=False)\n",
    "\n",
    "\n",
    "final_stock_data = pivot_table_stocks.copy(deep=True)\n",
    "final_factor_data = monthly_factors_adj.copy(deep=True)\n",
    "final_macro_data = macro_factors.copy(deep=True)\n",
    "\n",
    "batch_size_prop = 1\n",
    "batch_size = 4\n",
    "n_factors = len(final_factor_data.columns)\n",
    "patience_value = 400\n",
    "\n",
    "start_date = ('1977-01')\n",
    "end_date = ('2020-10')\n",
    "Y_name = 'fwd_quintile_adj'\n",
    "\n",
    "\n",
    "label_names = np.sort(monthly_stock_data[Y_name].unique())\n",
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef9d21",
   "metadata": {},
   "source": [
    "# Neural Network Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f443151e",
   "metadata": {},
   "source": [
    "Saving all model processes in a file to continue training the model with previous weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8b8fda",
   "metadata": {},
   "source": [
    "### local savings version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b4adcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has:  33 Smart-beta factors,  136 Macro factors,  and 169 Total factors, \n"
     ]
    }
   ],
   "source": [
    "path_alternative = 'checkpoints_alternative/'\n",
    "\n",
    "path_multi_target_merged_prop = path_alternative + 'path_multi_target_merged_prop.h5'\n",
    "path_multi_target_factors_prop = path_alternative + 'path_multi_target_factors_prop.h5'\n",
    "\n",
    "\n",
    "checkpoints_LSTM_model_merged_1L_alternative = path_alternative+'checkpoints_LSTM_model_merged_1L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_2L_alternative = path_alternative+'checkpoints_LSTM_model_merged_2L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_3L_alternative = path_alternative+'checkpoints_LSTM_model_merged_3L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_4L_alternative = path_alternative+'checkpoints_LSTM_model_merged_4L_alternative.h5'\n",
    "\n",
    "checkpoints_LSTM_model_1L_alternative = path_alternative+'checkpoints_LSTM_model_1L_alternative.h5'\n",
    "checkpoints_LSTM_model_2L_alternative = path_alternative+'checkpoints_LSTM_model_2L_alternative.h5'\n",
    "checkpoints_LSTM_model_3L_alternative = path_alternative+'checkpoints_LSTM_model_3L_alternative.h5'\n",
    "checkpoints_LSTM_model_4L_alternative = path_alternative+'checkpoints_LSTM_model_4L_alternative.h5'\n",
    "\n",
    "checkpoints_FFN_model_L1_alternative = path_alternative+'checkpoints_FFN_model_L1_alternative.h5'\n",
    "checkpoints_FFN_model_L2_alternative = path_alternative+'checkpoints_FFN_model_L2_alternative.h5'\n",
    "checkpoints_FFN_model_L3_alternative = path_alternative+'checkpoints_FFN_model_L3_alternative.h5'\n",
    "checkpoints_FFN_model_L4_alternative = path_alternative+'checkpoints_FFN_model_L4_alternative.h5'\n",
    "\n",
    "checkpoints_model_merged_1L_alternative = path_alternative+'checkpoints_model_merged_1L_alternative.h5'\n",
    "checkpoints_model_merged_2L_alternative = path_alternative+'checkpoints_model_merged_2L_alternative.h5'\n",
    "checkpoints_model_merged_3L_alternative = path_alternative+'checkpoints_model_merged_3L_alternative.h5'\n",
    "checkpoints_model_merged_4L_alternative = path_alternative+'checkpoints_model_merged_4L_alternative.h5'\n",
    "\n",
    "checkpoints_model_1L_alternative = path_alternative+'checkpoints_model_1L_alternative.h5'\n",
    "checkpoints_model_2L_alternative = path_alternative+'checkpoints_model_2L_alternative.h5'\n",
    "checkpoints_model_3L_alternative = path_alternative+'checkpoints_model_3L_alternative.h5'\n",
    "checkpoints_model_4L_alternative = path_alternative+'checkpoints_model_4L_alternative.h5'\n",
    "\n",
    "\n",
    "number_of_factors = final_factor_data.shape[1]\n",
    "number_of_macro_factors = final_macro_data.shape[1] \n",
    "number_of_merged_factors = number_of_factors+number_of_macro_factors \n",
    "\n",
    "print('The dataset has: ', \\\n",
    "     number_of_factors, 'Smart-beta factors, ', \\\n",
    "     number_of_macro_factors, 'Macro factors, ', 'and',\n",
    "     number_of_merged_factors, 'Total factors, ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d37c93",
   "metadata": {},
   "source": [
    "# Data Creation: Tensor Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca26171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_backwards_data(stock_data,macro_data, factor_data, test_data_factors, test_data_macro, batch_size_prop):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "#\n",
    "    train_date_index = stock_data.index.unique().tolist() \n",
    "    #train data\n",
    "    train_data_factors = factor_data.loc[factor_data.index.isin(train_date_index)]\n",
    "    train_data_macro = macro_data.loc[macro_data.index.isin(train_date_index)]\n",
    "    train_data_macro_norm = train_data_macro.copy(deep=True)\n",
    "    \n",
    "    for c in train_data_macro_norm.columns: \n",
    "        train_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(train_data_macro_norm[c]))\n",
    "\n",
    "    train_data_merged = pd.concat([train_data_factors, train_data_macro_norm],axis=1)\n",
    "    \n",
    "    \n",
    "     #train data\n",
    "\n",
    "    \n",
    "    test_data_macro_norm = test_data_macro.copy(deep=True)\n",
    "    \n",
    "    for c in test_data_macro_norm.columns: \n",
    "        test_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(test_data_macro_norm[c]))\n",
    "\n",
    "    test_data_merged = pd.concat([test_data_factors, test_data_macro_norm],axis=1)\n",
    "    \n",
    "\n",
    "    x_train_factors = []\n",
    "    x_train_macro = []\n",
    "    x_train_merged =[]\n",
    "    y_train =[]\n",
    "\n",
    "   \n",
    "    for i in tqdm(range(batch_size_prop, len(stock_data))):\n",
    "        x_train_factors.append(train_data_factors.values[i-batch_size_prop:i,:])\n",
    "        x_train_macro.append(train_data_macro_norm.values[i-batch_size_prop:i,:])\n",
    "        x_train_merged.append(train_data_merged.values[i-batch_size_prop:i,:])\n",
    "        y_train.append(stock_data.values[i, :])   #\n",
    "\n",
    "    x_train_factors, x_train_macro, x_train_merged, y_train= np.array(x_train_factors),np.array(x_train_macro),np.array(x_train_merged), np.array(y_train)\n",
    "    \n",
    "    \n",
    "    x_test_factors = []\n",
    "    x_test_macro =[]\n",
    "    x_test_merged =[]\n",
    "    \n",
    "    for i in tqdm(range(batch_size_prop, len(test_data_macro_norm))):\n",
    "        x_test_factors.append(test_data_factors.values[i-batch_size_prop:i,:])\n",
    "        x_test_macro.append(test_data_macro_norm.values[i-batch_size_prop:i,:])\n",
    "        x_test_merged.append(test_data_merged.values[i-batch_size_prop:i,:])  \n",
    "        \n",
    "        \n",
    "    x_test_factors, x_test_macro, x_test_merged = np.array(x_test_factors), np.array(x_test_macro), np.array(x_test_merged)\n",
    "    \n",
    "    return x_train_factors, x_train_macro,x_train_merged,y_train , x_test_factors, x_test_macro, x_test_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da8be229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:00<00:00, 49241.88it/s]\n",
      "100%|██████████| 375/375 [00:00<00:00, 80912.80it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_factors_prop, x_train_macro_prop,x_train_merged_prop,y_train_prop, x_test_factors_prop, x_test_macro_prop, x_test_merged_prop = batch_generator_backwards_data(final_stock_data_prop,final_macro_data_prop, final_factor_data_prop,test_data_factors_prop, test_data_macro_prop, batch_size_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bee5a",
   "metadata": {},
   "source": [
    "## Running backward prediction to get quintiles of stocks that do not have the length of 522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64251d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 16:03:05.157230: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-28 16:03:05.157265: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-28 16:03:05.157289: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (sagemaker-data-scie-ml-m5d-2xlarge-4b6d3b0a4abc36c03b0ba25d3988): /proc/driver/nvidia/version does not exist\n",
      "2022-08-28 16:03:05.157512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-3/10, momentum=0.9, nesterov=True)\n",
    "sgd_empty =SGD() \n",
    "adam = Adam(0.001)\n",
    "rmsprop = RMSprop(0.0015)\n",
    "\n",
    "config_prop = {'LOSS':[tf.keras.losses.MeanSquaredError()],             \n",
    "          'METRICS':[tf.keras.metrics.MeanSquaredError()],    \n",
    "         'OPTIMIZER':adam}  #specify learnig rate after having found optimal value                     #tf.keras.losses.MeanSquaredError() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d6b1913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model_merged_fillna_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_12 (Lay  (None, 1, 169)           338       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_35 (LSTM)              (None, 1, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_13 (Lay  (None, 1, 128)           256       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_36 (LSTM)              (None, 1, 64)             49408     \n",
      "                                                                 \n",
      " layer_normalization_14 (Lay  (None, 1, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_37 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " layer_normalization_15 (Lay  (None, 32)               64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 501)               8517      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,231\n",
      "Trainable params: 224,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_fillna_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_16 (Lay  (None, 1, 33)            66        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_38 (LSTM)              (None, 1, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_17 (Lay  (None, 1, 64)            128       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_39 (LSTM)              (None, 1, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_18 (Lay  (None, 1, 32)            64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " lstm_40 (LSTM)              (None, 32)                8320      \n",
      "                                                                 \n",
      " layer_normalization_19 (Lay  (None, 32)               64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 501)               8517      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,191\n",
      "Trainable params: 55,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "LSTM_model_merged_3L_fillna = Sequential([layers.Input(shape=(batch_size_prop,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(32, return_sequences =False),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)), \n",
    "                                   layers.Dense(501,activation='leaky_relu')],name='LSTM_model_merged_fillna_3L')\n",
    "\n",
    "LSTM_model_merged_3L_fillna.compile(loss=config_prop['LOSS'],optimizer=config_prop['OPTIMIZER'],metrics=config_prop['METRICS'])\n",
    "LSTM_model_merged_3L_fillna.save(path_multi_target_merged_prop)\n",
    "LSTM_model_merged_3L_fillna.summary()\n",
    "\n",
    "LSTM_model_3L_fillna = Sequential([layers.Input(shape=(batch_size_prop,number_of_factors)),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32, return_sequences =False),       \n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(16, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='LSTM_model_factors_fillna_3L')\n",
    "\n",
    "LSTM_model_3L_fillna.compile(loss=config_prop['LOSS'],optimizer=config_prop['OPTIMIZER'],metrics=config_prop['METRICS'])\n",
    "LSTM_model_3L_fillna.save(path_multi_target_factors_prop)\n",
    "LSTM_model_3L_fillna.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f031dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards_prop(model_type,checkpoint_path, x_train_factors,x_train_merged,y_train, x_test_factors, x_test_merged, batch_size_prop, num_epochs, class_weights_dict):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mean_squared_error',                   #'loss'  #val_mean_squared_error\n",
    "        patience=2000,\n",
    "        mode='min')\n",
    "    \n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor= 'val_mean_squared_error',\n",
    "        verbose=False,\n",
    "        save_best_only=True,\n",
    "        save_freq = 'epoch',\n",
    "        mode='min')\n",
    "\n",
    "    trained_model = tf.keras.models.load_model(checkpoint_path)\n",
    "    \n",
    "    if model_type=='merged':\n",
    "\n",
    "\n",
    "        trained_model.fit(x=x_train_merged, y=y_train,batch_size=batch_size_prop, epochs=num_epochs,class_weight=class_weights_dict,\n",
    "                          validation_split=0.09, callbacks=[cp_callback, early_stopping], verbose=1)  # \n",
    "        \n",
    "        model_score = trained_model.evaluate(x_train_merged, y_train, batch_size_prop)\n",
    "        predictions = trained_model.predict(x_test_merged, batch_size_prop)\n",
    "        \n",
    "    if model_type=='factors':\n",
    "\n",
    "        trained_model.fit(x=x_train_factors,y=y_train,batch_size=batch_size_prop, epochs=num_epochs,class_weight=class_weights_dict,\n",
    "                          validation_split=0.09, callbacks=[cp_callback, early_stopping], verbose=1)\n",
    "        \n",
    "        model_score = trained_model.evaluate(x_train_factors, y_train,batch_size_prop)\n",
    "        \n",
    "        predictions = trained_model.predict(x_test_factors, batch_size_prop)\n",
    "    return predictions, model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b31417",
   "metadata": {},
   "source": [
    "## Predicting backward quintiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16ef6ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1167 - mean_squared_error: 0.0383 - val_loss: 0.2883 - val_mean_squared_error: 0.2820\n",
      "Epoch 4722/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1189 - mean_squared_error: 0.0390 - val_loss: 0.2974 - val_mean_squared_error: 0.2907\n",
      "Epoch 4723/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1216 - mean_squared_error: 0.0398 - val_loss: 0.2874 - val_mean_squared_error: 0.2805\n",
      "Epoch 4724/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1193 - mean_squared_error: 0.0392 - val_loss: 0.2881 - val_mean_squared_error: 0.2811\n",
      "Epoch 4725/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1201 - mean_squared_error: 0.0395 - val_loss: 0.2918 - val_mean_squared_error: 0.2850\n",
      "Epoch 4726/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1175 - mean_squared_error: 0.0386 - val_loss: 0.2896 - val_mean_squared_error: 0.2830\n",
      "Epoch 4727/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1171 - mean_squared_error: 0.0384 - val_loss: 0.2887 - val_mean_squared_error: 0.2823\n",
      "Epoch 4728/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1192 - mean_squared_error: 0.0390 - val_loss: 0.2874 - val_mean_squared_error: 0.2801\n",
      "Epoch 4729/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1227 - mean_squared_error: 0.0400 - val_loss: 0.2895 - val_mean_squared_error: 0.2822\n",
      "Epoch 4730/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1174 - mean_squared_error: 0.0385 - val_loss: 0.2847 - val_mean_squared_error: 0.2783\n",
      "Epoch 4731/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - mean_squared_error: 0.0389 - val_loss: 0.2840 - val_mean_squared_error: 0.2773\n",
      "Epoch 4732/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1190 - mean_squared_error: 0.0392 - val_loss: 0.2915 - val_mean_squared_error: 0.2847\n",
      "Epoch 4733/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0385 - val_loss: 0.2874 - val_mean_squared_error: 0.2811\n",
      "Epoch 4734/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0382 - val_loss: 0.2864 - val_mean_squared_error: 0.2802\n",
      "Epoch 4735/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0381 - val_loss: 0.2855 - val_mean_squared_error: 0.2792\n",
      "Epoch 4736/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1200 - mean_squared_error: 0.0394 - val_loss: 0.2876 - val_mean_squared_error: 0.2797\n",
      "Epoch 4737/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1290 - mean_squared_error: 0.0416 - val_loss: 0.2885 - val_mean_squared_error: 0.2804\n",
      "Epoch 4738/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1283 - mean_squared_error: 0.0415 - val_loss: 0.2899 - val_mean_squared_error: 0.2814\n",
      "Epoch 4739/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1202 - mean_squared_error: 0.0393 - val_loss: 0.2934 - val_mean_squared_error: 0.2866\n",
      "Epoch 4740/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0382 - val_loss: 0.2909 - val_mean_squared_error: 0.2845\n",
      "Epoch 4741/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0381 - val_loss: 0.2881 - val_mean_squared_error: 0.2820\n",
      "Epoch 4742/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0381 - val_loss: 0.2882 - val_mean_squared_error: 0.2820\n",
      "Epoch 4743/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0382 - val_loss: 0.2850 - val_mean_squared_error: 0.2788\n",
      "Epoch 4744/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1162 - mean_squared_error: 0.0382 - val_loss: 0.2838 - val_mean_squared_error: 0.2777\n",
      "Epoch 4745/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0382 - val_loss: 0.2847 - val_mean_squared_error: 0.2784\n",
      "Epoch 4746/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0381 - val_loss: 0.2876 - val_mean_squared_error: 0.2813\n",
      "Epoch 4747/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1172 - mean_squared_error: 0.0385 - val_loss: 0.2935 - val_mean_squared_error: 0.2870\n",
      "Epoch 4748/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1183 - mean_squared_error: 0.0388 - val_loss: 0.2855 - val_mean_squared_error: 0.2789\n",
      "Epoch 4749/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1180 - mean_squared_error: 0.0387 - val_loss: 0.2859 - val_mean_squared_error: 0.2793\n",
      "Epoch 4750/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1196 - mean_squared_error: 0.0391 - val_loss: 0.2860 - val_mean_squared_error: 0.2787\n",
      "Epoch 4751/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1208 - mean_squared_error: 0.0395 - val_loss: 0.2852 - val_mean_squared_error: 0.2777\n",
      "Epoch 4752/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1208 - mean_squared_error: 0.0395 - val_loss: 0.2911 - val_mean_squared_error: 0.2838\n",
      "Epoch 4753/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1186 - mean_squared_error: 0.0389 - val_loss: 0.2884 - val_mean_squared_error: 0.2821\n",
      "Epoch 4754/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1167 - mean_squared_error: 0.0384 - val_loss: 0.2959 - val_mean_squared_error: 0.2894\n",
      "Epoch 4755/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2860 - val_mean_squared_error: 0.2791\n",
      "Epoch 4756/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1183 - mean_squared_error: 0.0387 - val_loss: 0.2873 - val_mean_squared_error: 0.2801\n",
      "Epoch 4757/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1257 - mean_squared_error: 0.0410 - val_loss: 0.3079 - val_mean_squared_error: 0.3003\n",
      "Epoch 4758/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1226 - mean_squared_error: 0.0403 - val_loss: 0.2923 - val_mean_squared_error: 0.2849\n",
      "Epoch 4759/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1214 - mean_squared_error: 0.0400 - val_loss: 0.2906 - val_mean_squared_error: 0.2834\n",
      "Epoch 4760/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1181 - mean_squared_error: 0.0387 - val_loss: 0.2906 - val_mean_squared_error: 0.2841\n",
      "Epoch 4761/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0380 - val_loss: 0.2853 - val_mean_squared_error: 0.2789\n",
      "Epoch 4762/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1162 - mean_squared_error: 0.0382 - val_loss: 0.2866 - val_mean_squared_error: 0.2804\n",
      "Epoch 4763/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1189 - mean_squared_error: 0.0390 - val_loss: 0.2904 - val_mean_squared_error: 0.2820\n",
      "Epoch 4764/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1330 - mean_squared_error: 0.0429 - val_loss: 0.2942 - val_mean_squared_error: 0.2849\n",
      "Epoch 4765/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1240 - mean_squared_error: 0.0404 - val_loss: 0.2901 - val_mean_squared_error: 0.2829\n",
      "Epoch 4766/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1190 - mean_squared_error: 0.0390 - val_loss: 0.2898 - val_mean_squared_error: 0.2825\n",
      "Epoch 4767/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1173 - mean_squared_error: 0.0384 - val_loss: 0.2909 - val_mean_squared_error: 0.2845\n",
      "Epoch 4768/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0382 - val_loss: 0.2893 - val_mean_squared_error: 0.2832\n",
      "Epoch 4769/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0381 - val_loss: 0.2892 - val_mean_squared_error: 0.2829\n",
      "Epoch 4770/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0381 - val_loss: 0.2882 - val_mean_squared_error: 0.2820\n",
      "Epoch 4771/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0383 - val_loss: 0.2893 - val_mean_squared_error: 0.2829\n",
      "Epoch 4772/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0384 - val_loss: 0.2863 - val_mean_squared_error: 0.2795\n",
      "Epoch 4773/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1238 - mean_squared_error: 0.0400 - val_loss: 0.2869 - val_mean_squared_error: 0.2788\n",
      "Epoch 4774/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1211 - mean_squared_error: 0.0394 - val_loss: 0.2866 - val_mean_squared_error: 0.2800\n",
      "Epoch 4775/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1171 - mean_squared_error: 0.0384 - val_loss: 0.2879 - val_mean_squared_error: 0.2818\n",
      "Epoch 4776/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2848 - val_mean_squared_error: 0.2783\n",
      "Epoch 4777/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1187 - mean_squared_error: 0.0391 - val_loss: 0.2888 - val_mean_squared_error: 0.2823\n",
      "Epoch 4778/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1171 - mean_squared_error: 0.0384 - val_loss: 0.2882 - val_mean_squared_error: 0.2818\n",
      "Epoch 4779/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0383 - val_loss: 0.2856 - val_mean_squared_error: 0.2795\n",
      "Epoch 4780/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0383 - val_loss: 0.2868 - val_mean_squared_error: 0.2807\n",
      "Epoch 4781/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0382 - val_loss: 0.2982 - val_mean_squared_error: 0.2919\n",
      "Epoch 4782/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1180 - mean_squared_error: 0.0388 - val_loss: 0.2889 - val_mean_squared_error: 0.2814\n",
      "Epoch 4783/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1345 - mean_squared_error: 0.0449 - val_loss: 0.2888 - val_mean_squared_error: 0.2801\n",
      "Epoch 4784/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1218 - mean_squared_error: 0.0397 - val_loss: 0.2945 - val_mean_squared_error: 0.2877\n",
      "Epoch 4785/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1172 - mean_squared_error: 0.0384 - val_loss: 0.2894 - val_mean_squared_error: 0.2828\n",
      "Epoch 4786/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1170 - mean_squared_error: 0.0383 - val_loss: 0.2900 - val_mean_squared_error: 0.2835\n",
      "Epoch 4787/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1170 - mean_squared_error: 0.0384 - val_loss: 0.2879 - val_mean_squared_error: 0.2815\n",
      "Epoch 4788/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0383 - val_loss: 0.2887 - val_mean_squared_error: 0.2825\n",
      "Epoch 4789/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2848 - val_mean_squared_error: 0.2779\n",
      "Epoch 4790/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1167 - mean_squared_error: 0.0383 - val_loss: 0.2898 - val_mean_squared_error: 0.2834\n",
      "Epoch 4791/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1186 - mean_squared_error: 0.0389 - val_loss: 0.2897 - val_mean_squared_error: 0.2831\n",
      "Epoch 4792/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1201 - mean_squared_error: 0.0395 - val_loss: 0.2979 - val_mean_squared_error: 0.2899\n",
      "Epoch 4793/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1231 - mean_squared_error: 0.0401 - val_loss: 0.2913 - val_mean_squared_error: 0.2844\n",
      "Epoch 4794/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1183 - mean_squared_error: 0.0389 - val_loss: 0.2877 - val_mean_squared_error: 0.2808\n",
      "Epoch 4795/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1183 - mean_squared_error: 0.0388 - val_loss: 0.2982 - val_mean_squared_error: 0.2915\n",
      "Epoch 4796/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1179 - mean_squared_error: 0.0387 - val_loss: 0.2903 - val_mean_squared_error: 0.2836\n",
      "Epoch 4797/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1174 - mean_squared_error: 0.0385 - val_loss: 0.2886 - val_mean_squared_error: 0.2821\n",
      "Epoch 4798/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1175 - mean_squared_error: 0.0385 - val_loss: 0.2911 - val_mean_squared_error: 0.2843\n",
      "Epoch 4799/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1180 - mean_squared_error: 0.0386 - val_loss: 0.2881 - val_mean_squared_error: 0.2818\n",
      "Epoch 4800/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1162 - mean_squared_error: 0.0382 - val_loss: 0.2916 - val_mean_squared_error: 0.2852\n",
      "Epoch 4801/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1179 - mean_squared_error: 0.0386 - val_loss: 0.2882 - val_mean_squared_error: 0.2805\n",
      "Epoch 4802/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1254 - mean_squared_error: 0.0408 - val_loss: 0.2895 - val_mean_squared_error: 0.2816\n",
      "Epoch 4803/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1205 - mean_squared_error: 0.0391 - val_loss: 0.2893 - val_mean_squared_error: 0.2820\n",
      "Epoch 4804/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1185 - mean_squared_error: 0.0387 - val_loss: 0.2909 - val_mean_squared_error: 0.2841\n",
      "Epoch 4805/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1189 - mean_squared_error: 0.0389 - val_loss: 0.2900 - val_mean_squared_error: 0.2827\n",
      "Epoch 4806/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1215 - mean_squared_error: 0.0399 - val_loss: 0.2961 - val_mean_squared_error: 0.2892\n",
      "Epoch 4807/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2910 - val_mean_squared_error: 0.2846\n",
      "Epoch 4808/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0382 - val_loss: 0.2920 - val_mean_squared_error: 0.2856\n",
      "Epoch 4809/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2906 - val_mean_squared_error: 0.2835\n",
      "Epoch 4810/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1230 - mean_squared_error: 0.0405 - val_loss: 0.3020 - val_mean_squared_error: 0.2945\n",
      "Epoch 4811/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1225 - mean_squared_error: 0.0402 - val_loss: 0.2889 - val_mean_squared_error: 0.2823\n",
      "Epoch 4812/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0383 - val_loss: 0.2925 - val_mean_squared_error: 0.2865\n",
      "Epoch 4813/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1156 - mean_squared_error: 0.0381 - val_loss: 0.2876 - val_mean_squared_error: 0.2815\n",
      "Epoch 4814/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1155 - mean_squared_error: 0.0379 - val_loss: 0.2855 - val_mean_squared_error: 0.2793\n",
      "Epoch 4815/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1179 - mean_squared_error: 0.0387 - val_loss: 0.2966 - val_mean_squared_error: 0.2898\n",
      "Epoch 4816/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1209 - mean_squared_error: 0.0395 - val_loss: 0.2896 - val_mean_squared_error: 0.2820\n",
      "Epoch 4817/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1197 - mean_squared_error: 0.0391 - val_loss: 0.2913 - val_mean_squared_error: 0.2845\n",
      "Epoch 4818/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1166 - mean_squared_error: 0.0383 - val_loss: 0.2893 - val_mean_squared_error: 0.2831\n",
      "Epoch 4819/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1162 - mean_squared_error: 0.0382 - val_loss: 0.2920 - val_mean_squared_error: 0.2859\n",
      "Epoch 4820/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1171 - mean_squared_error: 0.0385 - val_loss: 0.2994 - val_mean_squared_error: 0.2924\n",
      "Epoch 4821/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1208 - mean_squared_error: 0.0395 - val_loss: 0.2981 - val_mean_squared_error: 0.2911\n",
      "Epoch 4822/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1205 - mean_squared_error: 0.0395 - val_loss: 0.2880 - val_mean_squared_error: 0.2810\n",
      "Epoch 4823/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1198 - mean_squared_error: 0.0392 - val_loss: 0.2910 - val_mean_squared_error: 0.2838\n",
      "Epoch 4824/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1192 - mean_squared_error: 0.0388 - val_loss: 0.2867 - val_mean_squared_error: 0.2796\n",
      "Epoch 4825/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1215 - mean_squared_error: 0.0398 - val_loss: 0.2883 - val_mean_squared_error: 0.2814\n",
      "Epoch 4826/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1175 - mean_squared_error: 0.0385 - val_loss: 0.2919 - val_mean_squared_error: 0.2854\n",
      "Epoch 4827/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1175 - mean_squared_error: 0.0386 - val_loss: 0.2901 - val_mean_squared_error: 0.2837\n",
      "Epoch 4828/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2930 - val_mean_squared_error: 0.2867\n",
      "Epoch 4829/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1196 - mean_squared_error: 0.0392 - val_loss: 0.2917 - val_mean_squared_error: 0.2844\n",
      "Epoch 4830/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - mean_squared_error: 0.0387 - val_loss: 0.3094 - val_mean_squared_error: 0.3026\n",
      "Epoch 4831/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1194 - mean_squared_error: 0.0393 - val_loss: 0.2935 - val_mean_squared_error: 0.2860\n",
      "Epoch 4832/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1193 - mean_squared_error: 0.0391 - val_loss: 0.2899 - val_mean_squared_error: 0.2834\n",
      "Epoch 4833/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0382 - val_loss: 0.2900 - val_mean_squared_error: 0.2840\n",
      "Epoch 4834/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0382 - val_loss: 0.2884 - val_mean_squared_error: 0.2823\n",
      "Epoch 4835/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0383 - val_loss: 0.2912 - val_mean_squared_error: 0.2849\n",
      "Epoch 4836/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0387 - val_loss: 0.2894 - val_mean_squared_error: 0.2829\n",
      "Epoch 4837/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1217 - mean_squared_error: 0.0399 - val_loss: 0.2923 - val_mean_squared_error: 0.2848\n",
      "Epoch 4838/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1365 - mean_squared_error: 0.0446 - val_loss: 0.2905 - val_mean_squared_error: 0.2823\n",
      "Epoch 4839/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1194 - mean_squared_error: 0.0389 - val_loss: 0.2885 - val_mean_squared_error: 0.2816\n",
      "Epoch 4840/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - mean_squared_error: 0.0387 - val_loss: 0.2863 - val_mean_squared_error: 0.2796\n",
      "Epoch 4841/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1171 - mean_squared_error: 0.0383 - val_loss: 0.2894 - val_mean_squared_error: 0.2829\n",
      "Epoch 4842/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0382 - val_loss: 0.2903 - val_mean_squared_error: 0.2840\n",
      "Epoch 4843/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1154 - mean_squared_error: 0.0379 - val_loss: 0.2938 - val_mean_squared_error: 0.2875\n",
      "Epoch 4844/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0381 - val_loss: 0.2936 - val_mean_squared_error: 0.2873\n",
      "Epoch 4845/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1155 - mean_squared_error: 0.0380 - val_loss: 0.2947 - val_mean_squared_error: 0.2887\n",
      "Epoch 4846/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1156 - mean_squared_error: 0.0381 - val_loss: 0.2925 - val_mean_squared_error: 0.2863\n",
      "Epoch 4847/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1173 - mean_squared_error: 0.0385 - val_loss: 0.2992 - val_mean_squared_error: 0.2927\n",
      "Epoch 4848/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0386 - val_loss: 0.2966 - val_mean_squared_error: 0.2897\n",
      "Epoch 4849/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1190 - mean_squared_error: 0.0390 - val_loss: 0.2885 - val_mean_squared_error: 0.2817\n",
      "Epoch 4850/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1179 - mean_squared_error: 0.0387 - val_loss: 0.2860 - val_mean_squared_error: 0.2794\n",
      "Epoch 4851/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - mean_squared_error: 0.0388 - val_loss: 0.2916 - val_mean_squared_error: 0.2851\n",
      "Epoch 4852/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0387 - val_loss: 0.2905 - val_mean_squared_error: 0.2841\n",
      "Epoch 4853/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1215 - mean_squared_error: 0.0395 - val_loss: 0.2961 - val_mean_squared_error: 0.2881\n",
      "Epoch 4854/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1232 - mean_squared_error: 0.0403 - val_loss: 0.2956 - val_mean_squared_error: 0.2883\n",
      "Epoch 4855/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1205 - mean_squared_error: 0.0395 - val_loss: 0.3108 - val_mean_squared_error: 0.3038\n",
      "Epoch 4856/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1223 - mean_squared_error: 0.0401 - val_loss: 0.2922 - val_mean_squared_error: 0.2841\n",
      "Epoch 4857/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1188 - mean_squared_error: 0.0387 - val_loss: 0.2869 - val_mean_squared_error: 0.2806\n",
      "Epoch 4858/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0381 - val_loss: 0.2912 - val_mean_squared_error: 0.2850\n",
      "Epoch 4859/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1178 - mean_squared_error: 0.0387 - val_loss: 0.2940 - val_mean_squared_error: 0.2868\n",
      "Epoch 4860/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1213 - mean_squared_error: 0.0393 - val_loss: 0.2916 - val_mean_squared_error: 0.2848\n",
      "Epoch 4861/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1181 - mean_squared_error: 0.0386 - val_loss: 0.2906 - val_mean_squared_error: 0.2839\n",
      "Epoch 4862/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0382 - val_loss: 0.2865 - val_mean_squared_error: 0.2803\n",
      "Epoch 4863/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0381 - val_loss: 0.2865 - val_mean_squared_error: 0.2800\n",
      "Epoch 4864/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1170 - mean_squared_error: 0.0384 - val_loss: 0.2834 - val_mean_squared_error: 0.2765\n",
      "Epoch 4865/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1208 - mean_squared_error: 0.0397 - val_loss: 0.2934 - val_mean_squared_error: 0.2862\n",
      "Epoch 4866/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1188 - mean_squared_error: 0.0391 - val_loss: 0.2922 - val_mean_squared_error: 0.2858\n",
      "Epoch 4867/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1169 - mean_squared_error: 0.0383 - val_loss: 0.3064 - val_mean_squared_error: 0.2998\n",
      "Epoch 4868/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1232 - mean_squared_error: 0.0408 - val_loss: 0.2905 - val_mean_squared_error: 0.2830\n",
      "Epoch 4869/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1223 - mean_squared_error: 0.0399 - val_loss: 0.2909 - val_mean_squared_error: 0.2839\n",
      "Epoch 4870/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1217 - mean_squared_error: 0.0398 - val_loss: 0.3284 - val_mean_squared_error: 0.3200\n",
      "Epoch 4871/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1212 - mean_squared_error: 0.0396 - val_loss: 0.2858 - val_mean_squared_error: 0.2787\n",
      "Epoch 4872/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1167 - mean_squared_error: 0.0383 - val_loss: 0.2889 - val_mean_squared_error: 0.2826\n",
      "Epoch 4873/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0381 - val_loss: 0.2863 - val_mean_squared_error: 0.2801\n",
      "Epoch 4874/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1151 - mean_squared_error: 0.0379 - val_loss: 0.2877 - val_mean_squared_error: 0.2817\n",
      "Epoch 4875/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1151 - mean_squared_error: 0.0378 - val_loss: 0.2891 - val_mean_squared_error: 0.2832\n",
      "Epoch 4876/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1155 - mean_squared_error: 0.0381 - val_loss: 0.2883 - val_mean_squared_error: 0.2824\n",
      "Epoch 4877/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1162 - mean_squared_error: 0.0382 - val_loss: 0.2892 - val_mean_squared_error: 0.2829\n",
      "Epoch 4878/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1263 - mean_squared_error: 0.0413 - val_loss: 0.3045 - val_mean_squared_error: 0.2950\n",
      "Epoch 4879/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1343 - mean_squared_error: 0.0442 - val_loss: 0.2882 - val_mean_squared_error: 0.2783\n",
      "Epoch 4880/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1261 - mean_squared_error: 0.0411 - val_loss: 0.2883 - val_mean_squared_error: 0.2801\n",
      "Epoch 4881/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1210 - mean_squared_error: 0.0395 - val_loss: 0.2912 - val_mean_squared_error: 0.2841\n",
      "Epoch 4882/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1191 - mean_squared_error: 0.0390 - val_loss: 0.2902 - val_mean_squared_error: 0.2836\n",
      "Epoch 4883/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1164 - mean_squared_error: 0.0381 - val_loss: 0.2929 - val_mean_squared_error: 0.2865\n",
      "Epoch 4884/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0380 - val_loss: 0.2874 - val_mean_squared_error: 0.2812\n",
      "Epoch 4885/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1157 - mean_squared_error: 0.0380 - val_loss: 0.2897 - val_mean_squared_error: 0.2833\n",
      "Epoch 4886/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1156 - mean_squared_error: 0.0380 - val_loss: 0.2870 - val_mean_squared_error: 0.2809\n",
      "Epoch 4887/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0381 - val_loss: 0.2866 - val_mean_squared_error: 0.2805\n",
      "Epoch 4888/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1157 - mean_squared_error: 0.0380 - val_loss: 0.2855 - val_mean_squared_error: 0.2793\n",
      "Epoch 4889/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1154 - mean_squared_error: 0.0380 - val_loss: 0.2870 - val_mean_squared_error: 0.2810\n",
      "Epoch 4890/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1158 - mean_squared_error: 0.0381 - val_loss: 0.2880 - val_mean_squared_error: 0.2819\n",
      "Epoch 4891/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1187 - mean_squared_error: 0.0389 - val_loss: 0.2927 - val_mean_squared_error: 0.2857\n",
      "Epoch 4892/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1204 - mean_squared_error: 0.0395 - val_loss: 0.3018 - val_mean_squared_error: 0.2944\n",
      "Epoch 4893/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1254 - mean_squared_error: 0.0406 - val_loss: 0.2892 - val_mean_squared_error: 0.2810\n",
      "Epoch 4894/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1231 - mean_squared_error: 0.0400 - val_loss: 0.2912 - val_mean_squared_error: 0.2827\n",
      "Epoch 4895/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1255 - mean_squared_error: 0.0406 - val_loss: 0.3024 - val_mean_squared_error: 0.2949\n",
      "Epoch 4896/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1182 - mean_squared_error: 0.0386 - val_loss: 0.2917 - val_mean_squared_error: 0.2848\n",
      "Epoch 4897/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1165 - mean_squared_error: 0.0381 - val_loss: 0.2883 - val_mean_squared_error: 0.2820\n",
      "Epoch 4898/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0385 - val_loss: 0.2861 - val_mean_squared_error: 0.2792\n",
      "Epoch 4899/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1168 - mean_squared_error: 0.0383 - val_loss: 0.2871 - val_mean_squared_error: 0.2806\n",
      "Epoch 4900/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1156 - mean_squared_error: 0.0379 - val_loss: 0.2889 - val_mean_squared_error: 0.2828\n",
      "Epoch 4901/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0381 - val_loss: 0.2856 - val_mean_squared_error: 0.2794\n",
      "Epoch 4902/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1208 - mean_squared_error: 0.0395 - val_loss: 0.2899 - val_mean_squared_error: 0.2823\n",
      "Epoch 4903/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1269 - mean_squared_error: 0.0410 - val_loss: 0.2957 - val_mean_squared_error: 0.2869\n",
      "Epoch 4904/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1222 - mean_squared_error: 0.0399 - val_loss: 0.2896 - val_mean_squared_error: 0.2828\n",
      "Epoch 4905/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1177 - mean_squared_error: 0.0386 - val_loss: 0.2892 - val_mean_squared_error: 0.2828\n",
      "Epoch 4906/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1163 - mean_squared_error: 0.0382 - val_loss: 0.2887 - val_mean_squared_error: 0.2822\n",
      "Epoch 4907/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1161 - mean_squared_error: 0.0382 - val_loss: 0.2893 - val_mean_squared_error: 0.2830\n",
      "Epoch 4908/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1159 - mean_squared_error: 0.0380 - val_loss: 0.2861 - val_mean_squared_error: 0.2800\n",
      "Epoch 4909/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1157 - mean_squared_error: 0.0381 - val_loss: 0.2872 - val_mean_squared_error: 0.2811\n",
      "Epoch 4910/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1175 - mean_squared_error: 0.0386 - val_loss: 0.2941 - val_mean_squared_error: 0.2869\n",
      "Epoch 4911/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1222 - mean_squared_error: 0.0400 - val_loss: 0.2936 - val_mean_squared_error: 0.2865\n",
      "Epoch 4912/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1243 - mean_squared_error: 0.0405 - val_loss: 0.2889 - val_mean_squared_error: 0.2800\n",
      "Epoch 4913/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1287 - mean_squared_error: 0.0428 - val_loss: 0.2987 - val_mean_squared_error: 0.2910\n",
      "Epoch 4914/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1196 - mean_squared_error: 0.0390 - val_loss: 0.2878 - val_mean_squared_error: 0.2807\n",
      "Epoch 4915/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1176 - mean_squared_error: 0.0386 - val_loss: 0.2883 - val_mean_squared_error: 0.2816\n",
      "Epoch 4916/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1160 - mean_squared_error: 0.0381 - val_loss: 0.2865 - val_mean_squared_error: 0.2802\n",
      "Epoch 4917/10000\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1156 - mean_squared_error: 0.0380 - val_loss: 0.2911 - val_mean_squared_error: 0.2849\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 0.0667 - mean_squared_error: 0.0605\n",
      "375/375 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_factors, score_prop_factors = backwards_prop('factors', path_multi_target_factors_prop ,x_train_factors_prop,x_train_merged_prop,y_train_prop, x_test_factors_prop, x_test_merged_prop, batch_size_prop, 10000, class_weights_dict)\n",
    "\n",
    "predictions_merged,score_prop_merged  = backwards_prop('merged', path_multi_target_merged_prop,x_train_factors_prop,x_train_merged_prop,y_train_prop, x_test_factors_prop, x_test_merged_prop, batch_size_prop, 10000, class_weights_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeb43b0",
   "metadata": {},
   "source": [
    "### The LSTM merged model achieved a val MSE of 0.26 to predict backwards quintiles, the factor model 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91a7a14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_prop.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77a2dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save prop scores \n"
     ]
    }
   ],
   "source": [
    "predictions_factors = pd.DataFrame(predictions_factors)\n",
    "predictions_merged = pd.DataFrame(predictions_merged)\n",
    "\n",
    "predictions_factors.to_csv('predictions_factors_back_prop_NN.csv')\n",
    "file_saver('predictions_factors_back_prop_NN.csv','predictions_factors_back_prop_NN.csv')\n",
    "\n",
    "predictions_merged.to_csv('predictions_merged_back_prop_NN.csv')\n",
    "file_saver('predictions_merged_back_prop_NN.csv','predictions_merged_back_prop_NN.csv')\n",
    "\n",
    "print('save prop scores ')\n",
    "np.savez_compressed('prop_model_scores.npz', score_prop_factors=score_prop_factors, score_prop_merged=score_prop_factors)\n",
    "                    \n",
    "file_saver('prop_model_scores.npz','prop_model_scores.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cbaa4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12568454444408417, 0.11696605384349823]\n",
      "[0.0667126253247261, 0.060485225170850754]\n"
     ]
    }
   ],
   "source": [
    "print(score_prop_factors)\n",
    "print(score_prop_merged)\n",
    "\n",
    "# [0.12568454444408417, 0.11696605384349823]\n",
    "# [0.0667126253247261, 0.060485225170850754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5abf225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_merged.round().values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "781fcede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "1986-03      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "1986-03      4.0      NaN      4.0  ...      NaN      NaN      NaN      NaN   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "1986-03      NaN      NaN      NaN      NaN      NaN      2.0  \n",
       "\n",
       "[1 rows x 501 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_stocks.loc[pivot_table_stocks.index==('1986-03')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "80b3144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2008-09', '2008-08', '2008-07', '2008-06', '2008-05', '2008-04',\n",
       "       '2008-03', '2008-02', '2008-01', '2007-12',\n",
       "       ...\n",
       "       '1978-04', '1978-03', '1978-02', '1978-01', '1977-12', '1977-11',\n",
       "       '1977-10', '1977-09', '1977-08', '1977-07'],\n",
       "      dtype='object', name='date', length=375)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_factors_prop.index[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "72e49584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-05</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "1977-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-10      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-11      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2008-05      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-06      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-07      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-08      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-09      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "1977-07      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-08      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-09      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-10      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-11      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2008-05      4.0      1.0      3.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-06      4.0      1.0      3.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-07      4.0      1.0      4.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-08      4.0      1.0      4.0  ...      2.0      3.0      3.0      2.0   \n",
       "2008-09      4.0      1.0      4.0  ...      2.0      3.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "1977-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-10      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-11      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2008-05      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-06      1.0      3.0      4.0      4.0      2.0      3.0  \n",
       "2008-07      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-08      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-09      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "\n",
       "[375 rows x 501 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncomplete_df = pivot_table_stocks.copy()\n",
    "uncomplete_df = uncomplete_df.loc[(uncomplete_df.index>=('1977-01'))&(uncomplete_df.index<=('2008-09'))]\n",
    "uncomplete_df = uncomplete_df.sort_values(by='date', ascending=False)\n",
    "uncomplete_df\n",
    "\n",
    "predictions_factors = pd.DataFrame(predictions_factors)\n",
    "predictions_factors.index = test_data_factors_prop.index[1:]\n",
    "predictions_factors.columns = pivot_table_stocks.columns\n",
    "predictions_factors = predictions_factors.round().sort_values(by='date',ascending=False)\n",
    "predictions_factors\n",
    "\n",
    "predictions_merged = pd.DataFrame(predictions_merged)\n",
    "predictions_merged.index = test_data_factors_prop.index[1:]\n",
    "predictions_merged.columns = pivot_table_stocks.columns\n",
    "predictions_merged = predictions_merged.round().sort_values(by='date',ascending=False)\n",
    "predictions_merged\n",
    "\n",
    "def value_propagation(df, df_prop):\n",
    "    altered_df = df.copy()\n",
    "    for i in df.columns:\n",
    "        if any(df[i].isna()): \n",
    "            \n",
    "            isna_index = df.loc[pd.isna(df[i]), :].index.values\n",
    "            altered_df.loc[isna_index,i] = df_prop.loc[isna_index, i].values\n",
    "\n",
    "    return altered_df\n",
    "\n",
    "complete_stock_quintiles_factors = value_propagation(uncomplete_df, predictions_factors)\n",
    "complete_stock_quintiles_factors = complete_stock_quintiles_factors.sort_values(by='date')\n",
    "complete_stock_quintiles_factors\n",
    "\n",
    "complete_stock_quintiles_merged = value_propagation(uncomplete_df, predictions_merged)\n",
    "complete_stock_quintiles_merged = complete_stock_quintiles_merged.sort_values(by='date')\n",
    "complete_stock_quintiles_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "59266fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-01</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "2008-11      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2008-12      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-01      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-02      3.0      3.0      4.0      4.0      4.0      4.0      4.0   \n",
       "2009-03      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "2008-11      4.0      1.0      4.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-12      4.0      1.0      4.0  ...      1.0      3.0      4.0      2.0   \n",
       "2009-01      4.0      1.0      4.0  ...      1.0      2.0      3.0      3.0   \n",
       "2009-02      4.0      1.0      4.0  ...      1.0      2.0      4.0      3.0   \n",
       "2009-03      4.0      1.0      4.0  ...      1.0      3.0      4.0      3.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "2008-11      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-12      1.0      3.0      4.0      4.0      3.0      4.0  \n",
       "2009-01      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-02      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2009-03      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "\n",
       "[108 rows x 501 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_stocks.dropna().sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69b0edec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-05</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "2008-09      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-08      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-07      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-06      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2008-05      3.0      3.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "1977-11      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "1977-10      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "1977-09      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "1977-08      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "1977-07      NaN      NaN      NaN      4.0      3.0      NaN      NaN   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "2008-09      4.0      1.0      4.0  ...      2.0      3.0      3.0      2.0   \n",
       "2008-08      4.0      1.0      4.0  ...      2.0      3.0      3.0      2.0   \n",
       "2008-07      4.0      1.0      4.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-06      4.0      1.0      3.0  ...      2.0      3.0      3.0      3.0   \n",
       "2008-05      4.0      1.0      3.0  ...      2.0      3.0      3.0      3.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "1977-11      4.0      NaN      3.0  ...      NaN      NaN      NaN      NaN   \n",
       "1977-10      4.0      NaN      3.0  ...      NaN      NaN      NaN      NaN   \n",
       "1977-09      4.0      NaN      3.0  ...      NaN      NaN      NaN      NaN   \n",
       "1977-08      4.0      NaN      3.0  ...      NaN      NaN      NaN      NaN   \n",
       "1977-07      4.0      NaN      3.0  ...      NaN      NaN      NaN      NaN   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "2008-09      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-08      2.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-07      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "2008-06      1.0      3.0      4.0      4.0      2.0      3.0  \n",
       "2008-05      1.0      3.0      4.0      4.0      3.0      3.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "1977-11      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1977-10      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1977-09      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1977-08      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "1977-07      NaN      NaN      NaN      NaN      NaN      NaN  \n",
       "\n",
       "[375 rows x 501 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncomplete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4472778f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "1977-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-10      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-11      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "1977-07      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-08      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-09      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-10      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-11      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "1977-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-10      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-11      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "\n",
       "[520 rows x 501 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge = pivot_table_stocks.loc[pivot_table_stocks.index>=('2008-10')]\n",
    "complete_stock_quintiles = pd.concat([complete_stock_quintiles_merged, to_merge],axis=0)\n",
    "complete_stock_quintiles = complete_stock_quintiles.apply(lambda x: x.fillna(x.mean(),axis=0))\n",
    "complete_stock_quintiles = complete_stock_quintiles.dropna()\n",
    "complete_stock_quintiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46024826",
   "metadata": {},
   "source": [
    "### To avoid model false fitting we replace all values above 5 with 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "052512cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(complete_stock_quintiles.values.max())\n",
    "print(complete_stock_quintiles.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "191cf5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_stock_quintiles = complete_stock_quintiles.replace(5,4)\n",
    "complete_stock_quintiles.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1623ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_stock_quintiles.to_csv('complete_stock_quintiles_NN_pred.csv')\n",
    "file_saver('complete_stock_quintiles_NN_pred.csv','complete_stock_quintiles_NN_pred.csv')\n",
    "\n",
    "complete_stock_quintiles_factors.to_csv('complete_stock_quintiles_factors_NN.csv')\n",
    "file_saver('complete_stock_quintiles_factors_NN.csv','complete_stock_quintiles_factors_NN.csv')\n",
    "\n",
    "complete_stock_quintiles_merged.to_csv('complete_stock_quintiles_merged_NN.csv')\n",
    "file_saver('complete_stock_quintiles_merged_NN.csv','complete_stock_quintiles_merged_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7bbeb880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_stock_quintiles_merged = pd.read_csv(path_bucket+'complete_stock_quintiles_NN_pred.csv', index_col=0)\n",
    "complete_stock_quintiles_merged = complete_stock_quintiles_merged.astype(float)\n",
    "complete_stock_quintiles_merged.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e282579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator_complete_data(stock_data,macro_data, factor_data, batch_size, train_set_ratio):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    train_stock_data = stock_data.iloc[0:int(len(stock_data)*train_set_ratio),:]\n",
    "    test_stock_data = stock_data.iloc[int(len(stock_data)*(train_set_ratio)):,:]\n",
    "        \n",
    "    train_date_index = train_stock_data.index.unique().tolist() \n",
    "    test_date_index = test_stock_data.index.unique().tolist()\n",
    "\n",
    "        \n",
    "    #train data\n",
    "    train_data_factors = factor_data.loc[factor_data.index.isin(train_date_index)]\n",
    "    train_data_macro = macro_data.loc[macro_data.index.isin(train_date_index)]\n",
    "    train_data_macro_norm = train_data_macro.copy(deep=True)\n",
    "    \n",
    "    for c in train_data_macro_norm.columns: \n",
    "        train_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(train_data_macro_norm[c]))\n",
    "\n",
    "    train_data_merged = pd.concat([train_data_factors, train_data_macro_norm],axis=1)\n",
    "    \n",
    "    test_data_factors = factor_data.loc[factor_data.index.isin(test_date_index)]\n",
    "    test_data_macro = macro_data.loc[macro_data.index.isin(test_date_index)]\n",
    "    test_data_macro_norm = test_data_macro.copy(deep=True)\n",
    "    \n",
    "    for c in test_data_macro_norm.columns: \n",
    "        test_data_macro_norm[c] = MinMaxScaler([-1,1]).fit_transform(pd.DataFrame(test_data_macro_norm[c]))\n",
    "\n",
    "    test_data_merged = pd.concat([test_data_factors, test_data_macro_norm],axis=1)\n",
    "    \n",
    "\n",
    "    x_train_factors = []\n",
    "    x_train_macro = []\n",
    "    x_train_merged =[]\n",
    "    y_train =[]\n",
    "\n",
    "    for i in tqdm(range(batch_size, len(train_data_factors))):\n",
    "        x_train_factors.append(train_data_factors.values[i-batch_size:i,:])\n",
    "        x_train_macro.append(train_data_macro_norm.values[i-batch_size:i,:])\n",
    "        x_train_merged.append(train_data_merged.values[i-batch_size:i,:])\n",
    "        y_train.append(train_stock_data.values[i,:])   #\n",
    "\n",
    "    x_train_factors, x_train_macro, x_train_merged, y_train= np.array(x_train_factors),np.array(x_train_macro),np.array(x_train_merged), np.array(y_train)\n",
    "\n",
    "\n",
    "    x_test_factors = []\n",
    "    x_test_macro = []\n",
    "    x_test_merged =[]\n",
    "    x_test_merged = []\n",
    "    y_test = []\n",
    "\n",
    "    for i in tqdm(range(batch_size, len(test_data_factors))):\n",
    "        x_test_factors.append(test_data_factors.values[i-batch_size:i,:])\n",
    "        x_test_macro.append(test_data_macro_norm.values[i-batch_size:i,:])\n",
    "        x_test_merged.append(test_data_merged.values[i-batch_size:i,:])\n",
    "        y_test.append(test_stock_data.values[i,:])\n",
    "\n",
    "    x_test_factors, x_test_macro,x_test_merged, y_test = np.array(x_test_factors), np.array(x_test_macro),np.array(x_test_merged), np.array(y_test)\n",
    "\n",
    "\n",
    "    return x_train_factors, x_train_macro,x_train_merged,y_train,x_test_factors,x_test_macro,x_test_merged, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e040d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412/412 [00:00<00:00, 61008.06it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 51787.92it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_factors, x_train_macro,x_train_merged,y_train,x_test_factors,x_test_macro,x_test_merged, y_test = batch_generator_complete_data(complete_stock_quintiles,final_macro_data, final_factor_data, batch_size, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b264d2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 501)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_stock_quintiles.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a5f3923a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4, 169)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape\n",
    "x_test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c985310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(412, 4, 33)\n",
      "(412, 501)\n",
      "(100, 4, 33)\n",
      "(100, 501)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_factors.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test_factors.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1d76bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save train data\n",
      "save test data\n"
     ]
    }
   ],
   "source": [
    "print('save train data')\n",
    "np.savez_compressed('train_arrays_multi_available.npz',x_train_factors=x_train_factors,x_train_macro=x_train_macro,x_train_merged=x_train_merged,y_train=y_train)\n",
    "\n",
    "print('save test data')\n",
    "np.savez_compressed('test_arrays_multi_available.npz',x_test_factors=x_test_factors,x_test_macro=x_test_macro,x_test_merged=x_test_merged, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2dffd6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1977-07'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_stock_data.index.unique())\n",
    "final_stock_data.index.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3bec8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('train_arrays_multi_available.npz','train_arrays_multi_available.npz')\n",
    "file_saver('test_arrays_multi_available.npz','test_arrays_multi_available.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264627d",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f45c17",
   "metadata": {},
   "source": [
    "## Data for complete data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "610d1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.load('train_arrays_multi_available.npz', allow_pickle=True)\n",
    "x_train_factors = train_array['x_train_factors']\n",
    "x_train_macro = train_array['x_train_macro']\n",
    "x_train_merged = train_array['x_train_merged']\n",
    "y_train = train_array['y_train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "baf22dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 4, 169)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_factors.shape\n",
    "x_train_macro.shape\n",
    "x_train_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba09b77",
   "metadata": {},
   "source": [
    "## Non loop model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7de0862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit_simple(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, batch_size, num_epochs, goal,class_weights_dict):\n",
    "    \"\"\"\n",
    "    model = NN model, \n",
    "    data = stock data, factor data, macro data,\n",
    "    batch_size = timesteps per batch\n",
    "    alpha adam = learning rate optimizer\n",
    "    data set ratios = train_set_ratio, val_set_ratio (eg. 0.5)\n",
    "    \"\"\"\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', #val_mean_squared_error                  #'loss'  #val_mean_squared_error\n",
    "        patience=patience,\n",
    "        mode='min')\n",
    "    \n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor= 'val_loss',\n",
    "        verbose=False,\n",
    "        save_best_only=True,\n",
    "        save_freq = 'epoch',\n",
    "        mode='min')\n",
    "    \n",
    "    scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-04*10**(epoch/10))\n",
    "    \n",
    "    trained_model = tf.keras.models.load_model(checkpoint_path)\n",
    "\n",
    "    \n",
    "    if goal=='lr':\n",
    "        if model_type=='combined':\n",
    "             \n",
    "                \n",
    "            history = trained_model.fit(x=[x_train_macro,x_train_factors],y=y_train,batch_size=batch_size, epochs=20,\n",
    "                              validation_split=0.09, callbacks=[scheduler]) #\n",
    "\n",
    "        if model_type=='merged':\n",
    "\n",
    "\n",
    "            history = trained_model.fit(x=x_train_merged, y=y_train,batch_size=batch_size, epochs=20,\n",
    "                              validation_split=0.09, callbacks=[scheduler])  # \n",
    "        if model_type=='factors':\n",
    "\n",
    "            history =  trained_model.fit(x=x_train_factors,y=y_train,batch_size=batch_size, epochs=20,\n",
    "                              validation_split=0.09, callbacks=[scheduler])             \n",
    "\n",
    "        return trained_model \n",
    "    if goal=='fit':\n",
    "        if model_type=='combined':\n",
    "\n",
    "            history = trained_model.fit(x=[x_train_macro,x_train_factors],y=y_train,batch_size=batch_size, epochs=num_epochs,\n",
    "                              validation_split=0.09, callbacks=[early_stopping,cp_callback]) #\n",
    "            \n",
    "            score = trained_model.evaluate([x_train_macro, x_train_factors], y_train, batch_size)\n",
    "            \n",
    "        if model_type=='merged':\n",
    "\n",
    "\n",
    "            history = trained_model.fit(x=x_train_merged, y=y_train,batch_size=batch_size, epochs=num_epochs,\n",
    "                              validation_split=0.09, callbacks=[early_stopping,cp_callback])  # \n",
    "            \n",
    "            score = trained_model.evaluate(x_train_merged, y_train, batch_size)\n",
    "        if model_type=='factors':\n",
    "\n",
    "            history = trained_model.fit(x=x_train_factors,y=y_train,batch_size=batch_size, epochs=num_epochs,\n",
    "                              validation_split=0.09, callbacks=[early_stopping,cp_callback])             \n",
    "\n",
    "            score = trained_model.evaluate(x_train_factors, y_train, batch_size)\n",
    "        \n",
    "        trained_model.save(checkpoint_path[:-3]+'_final'+ '.h5')\n",
    "        \n",
    "    return history, score                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "01556677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_stocks(stock_data,batch_size,train_set_ratio):\n",
    "    \n",
    "    pivot_stock_data = stock_data.iloc[int(len(stock_data)*(train_set_ratio)):,:]\n",
    "    pivot_stock_data = pivot_stock_data.iloc[batch_size:,:] \n",
    "    pivot_stock_data = pivot_stock_data+1\n",
    "    test_stock_data_df = pivot_stock_data.reset_index().melt(id_vars=['date'])\n",
    "    test_stock_data_df = test_stock_data_df.sort_values(by=['date','permno'])\n",
    "    return pivot_stock_data, test_stock_data_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e647bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def nn_predict_simple(model_type,test_data_stocks, test_factors,test_macro,test_merged, model_path, batch_size, label_names):\n",
    "    permnos = test_data_stocks.columns.unique()\n",
    "    permno_df =test_data_stocks.copy()\n",
    "    \n",
    "    trained_model = tf.keras.models.load_model(model_path) \n",
    "    \n",
    "    if model_type =='combined':\n",
    "        predictions = trained_model.predict([test_macro, test_factors],batch_size)\n",
    "        predictions_pivot = pd.DataFrame(index=test_data_stocks.index, columns=test_data_stocks.columns)\n",
    "        predictions_pivot.iloc[:,:] =  predictions\n",
    " \n",
    "    elif model_type =='factors':\n",
    "        predictions = trained_model.predict(test_factors,batch_size)\n",
    "        predictions_pivot = pd.DataFrame(index=test_data_stocks.index, columns=test_data_stocks.columns)\n",
    "        predictions_pivot.iloc[:,:] =  predictions\n",
    "    \n",
    "    elif model_type =='merged':\n",
    "        predictions = trained_model.predict(test_merged,batch_size)\n",
    "        predictions_pivot = pd.DataFrame(index=test_data_stocks.index, columns=test_data_stocks.columns)\n",
    "        predictions_pivot.iloc[:,:] =  predictions\n",
    "    \n",
    "    predictions_pivot = predictions_pivot.astype(float)\n",
    "    predictions_pivot = predictions_pivot.round()\n",
    "    predictions_pivot = predictions_pivot+1\n",
    "    predictions_df = predictions_pivot.copy()\n",
    "    predictions_df = predictions_df.astype(float)\n",
    "    predictions_df =  predictions_df.reset_index().melt(id_vars=['date'])\n",
    "    predictions_df = predictions_df.sort_values(by=['date', 'permno'])\n",
    "\n",
    "    return predictions_pivot, predictions_df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b657f9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_oos(true_values, pred_values):\n",
    "    r2 = 1 - ((sum((true_values-pred_values)**2))/(sum(true_values**2)))\n",
    "    return r2 \n",
    "\n",
    "\n",
    "def create_metrics(test_data_df, prediction_df, batch_size, n_factors, r2_oos,dataframe_name):\n",
    "    prediction_df = prediction_df.copy()\n",
    "    pred_name = prediction_df.columns.tolist()\n",
    "    pred_name = pred_name[2]\n",
    "    \n",
    "    comparison = pd.merge(test_data_df, prediction_df, how='left', on=['date', 'permno'])\n",
    "    comparison = comparison.rename(columns={'value_x':'true quint', 'value_y':'pred quint'})\n",
    "    r2= r2_score(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    r2_oos = r2_oos(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    \n",
    "    r2_adj = 1-(1-r2)*(len(comparison.iloc[:,3])-1)/((len(comparison.iloc[:,3])-n_factors-1))\n",
    "    r2_adj_oos = 1-(1-r2_oos)*(len(comparison.iloc[:,3])-1)/((len(comparison.iloc[:,3])-n_factors-1))\n",
    "    MSE = mean_squared_error(comparison.iloc[:,2],comparison.iloc[:,3])\n",
    "    rss = np.sum((comparison.iloc[:,3]-comparison.iloc[:,2])**2)\n",
    "\n",
    "    df_metrics =  pd.DataFrame(data=[r2,r2_oos,r2_adj,r2_adj_oos, MSE, rss])\n",
    "    df_metrics=df_metrics.rename(index={0:'r2',1:'r2_oos', 2:'r2_adj', 3:'r2_adj_oos',4:'MSE',5:'rss'},columns={0:dataframe_name})\n",
    "    return comparison, df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "81b45bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(model, x, y, y_pred, sample_weights=1):\n",
    "    compute_loss = model.compute_loss(x,y,y_pred, sample_weights)\n",
    "    \n",
    "    compute_metrics = model.compute_metrics(x,y,y_pred, sample_weights)\n",
    "    \n",
    "    evaluated_model = model.evaluate(x,y,batch_size=32)\n",
    "    \n",
    "    return compute_loss, compute_metrics, evaluated_model\n",
    "\n",
    "def sharpe(r):\n",
    "    return (np.nanmean(r)/np.nanstd(r))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffce23",
   "metadata": {},
   "source": [
    "# Neural network models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3c716de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-3/10, momentum=0.9, nesterov=True)\n",
    "sgd_empty =SGD() \n",
    "adam = Adam(0.001)\n",
    "rmsprop = RMSprop(0.0015)\n",
    "\n",
    "config = {'LOSS':[tf.keras.losses.MeanSquaredError()],             \n",
    "          'METRICS':[tf.keras.metrics.MeanSquaredError()],    \n",
    "         'OPTIMIZER':adam} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e557dc6",
   "metadata": {},
   "source": [
    "## Merged Feedforward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c4aae77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 4, 169)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "42c20276",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_merged_datasets_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_376 (Dense)           (None, 4, 128)            21760     \n",
      "                                                                 \n",
      " dropout_167 (Dropout)       (None, 4, 128)            0         \n",
      "                                                                 \n",
      " flatten_64 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_377 (Dense)           (None, 501)               257013    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 278,773\n",
      "Trainable params: 278,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_378 (Dense)           (None, 4, 128)            21760     \n",
      "                                                                 \n",
      " dropout_168 (Dropout)       (None, 4, 128)            0         \n",
      "                                                                 \n",
      " dense_379 (Dense)           (None, 4, 64)             8256      \n",
      "                                                                 \n",
      " dropout_169 (Dropout)       (None, 4, 64)             0         \n",
      "                                                                 \n",
      " flatten_65 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_380 (Dense)           (None, 501)               128757    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 158,773\n",
      "Trainable params: 158,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_381 (Dense)           (None, 4, 128)            21760     \n",
      "                                                                 \n",
      " dropout_170 (Dropout)       (None, 4, 128)            0         \n",
      "                                                                 \n",
      " dense_382 (Dense)           (None, 4, 64)             8256      \n",
      "                                                                 \n",
      " dropout_171 (Dropout)       (None, 4, 64)             0         \n",
      "                                                                 \n",
      " dense_383 (Dense)           (None, 4, 32)             2080      \n",
      "                                                                 \n",
      " dropout_172 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " flatten_66 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_384 (Dense)           (None, 501)               64629     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,725\n",
      "Trainable params: 96,725\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_merged_datasets_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_385 (Dense)           (None, 4, 128)            21760     \n",
      "                                                                 \n",
      " dropout_173 (Dropout)       (None, 4, 128)            0         \n",
      "                                                                 \n",
      " dense_386 (Dense)           (None, 4, 64)             8256      \n",
      "                                                                 \n",
      " dropout_174 (Dropout)       (None, 4, 64)             0         \n",
      "                                                                 \n",
      " dense_387 (Dense)           (None, 4, 32)             2080      \n",
      "                                                                 \n",
      " dropout_175 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " dense_388 (Dense)           (None, 4, 16)             528       \n",
      "                                                                 \n",
      " dropout_176 (Dropout)       (None, 4, 16)             0         \n",
      "                                                                 \n",
      " flatten_67 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_389 (Dense)           (None, 501)               32565     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,189\n",
      "Trainable params: 65,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_merged_1L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                              layers.Dense(128, activation='leaky_relu' , kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                              layers.Dropout(0.4),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(501,activation='leaky_relu')],name = 'model_merged_datasets_1L') \n",
    "\n",
    "model_merged_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "# model_merged_1L.save(checkpoints_model_merged_1L_alternative)\n",
    "model_merged_1L.summary()\n",
    "\n",
    "model_merged_2L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.Dense(128 ,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                                   layers.Dropout(0.2),\n",
    "                                   layers.Dense(64,activation='leaky_relu'),\n",
    "                                   layers.Dropout(0.2),\n",
    "                                   layers.Flatten(),\n",
    "                                   layers.Dense(501,activation='leaky_relu')],name='model_merged_datasets_2L')\n",
    "\n",
    "model_merged_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "# model_merged_2L.save(checkpoints_model_merged_2L_alternative)\n",
    "model_merged_2L.summary()\n",
    "\n",
    "model_merged_3L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                             layers.Dense(128,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Dense(64,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Dense(32,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                             layers.Dropout(0.2),\n",
    "                             layers.Flatten(),\n",
    "                             layers.Dense(501, activation='leaky_relu')],name='model_merged_datasets_3L')\n",
    "\n",
    "model_merged_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "# model_merged_3L.save(checkpoints_model_merged_3L_alternative)\n",
    "model_merged_3L.summary()\n",
    "\n",
    "model_merged_4L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                              layers.Dense(128,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(64,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(32,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Dense(16,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                              layers.Dropout(0.2),\n",
    "                              layers.Flatten(),\n",
    "                              layers.Dense(501,activation='leaky_relu' )],name='model_merged_datasets_4L')\n",
    "\n",
    "model_merged_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "# model_merged_4L.save(checkpoints_model_merged_4L_alternative)\n",
    "model_merged_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d749d",
   "metadata": {},
   "source": [
    "## Factor Only Feedforward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "94a6886e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_factors_only_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_390 (Dense)           (None, 4, 32)             1088      \n",
      "                                                                 \n",
      " dropout_177 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " flatten_68 (Flatten)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_391 (Dense)           (None, 501)               64629     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,717\n",
      "Trainable params: 65,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_392 (Dense)           (None, 4, 32)             1088      \n",
      "                                                                 \n",
      " dropout_178 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " dense_393 (Dense)           (None, 4, 16)             528       \n",
      "                                                                 \n",
      " dropout_179 (Dropout)       (None, 4, 16)             0         \n",
      "                                                                 \n",
      " flatten_69 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_394 (Dense)           (None, 501)               32565     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,181\n",
      "Trainable params: 34,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_395 (Dense)           (None, 4, 32)             1088      \n",
      "                                                                 \n",
      " dropout_180 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " dense_396 (Dense)           (None, 4, 16)             528       \n",
      "                                                                 \n",
      " dropout_181 (Dropout)       (None, 4, 16)             0         \n",
      "                                                                 \n",
      " dense_397 (Dense)           (None, 4, 8)              136       \n",
      "                                                                 \n",
      " dropout_182 (Dropout)       (None, 4, 8)              0         \n",
      "                                                                 \n",
      " flatten_70 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_398 (Dense)           (None, 501)               16533     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,285\n",
      "Trainable params: 18,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_factors_only_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_399 (Dense)           (None, 4, 32)             1088      \n",
      "                                                                 \n",
      " dropout_183 (Dropout)       (None, 4, 32)             0         \n",
      "                                                                 \n",
      " dense_400 (Dense)           (None, 4, 16)             528       \n",
      "                                                                 \n",
      " dropout_184 (Dropout)       (None, 4, 16)             0         \n",
      "                                                                 \n",
      " dense_401 (Dense)           (None, 4, 8)              136       \n",
      "                                                                 \n",
      " dropout_185 (Dropout)       (None, 4, 8)              0         \n",
      "                                                                 \n",
      " dense_402 (Dense)           (None, 4, 8)              72        \n",
      "                                                                 \n",
      " dropout_186 (Dropout)       (None, 4, 8)              0         \n",
      "                                                                 \n",
      " dropout_187 (Dropout)       (None, 4, 8)              0         \n",
      "                                                                 \n",
      " flatten_71 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_403 (Dense)           (None, 501)               16533     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,357\n",
      "Trainable params: 18,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "\n",
    "model_input_macro = layers.Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = layers.Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#################################################### LSTM  models with only factor data ##################################################\n",
    "model_1L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.3),\n",
    "                         layers.Flatten(),\n",
    "                         layers.Dense(501,activation='leaky_relu')],name='model_factors_only_1L')\n",
    "\n",
    "model_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "model_1L.save(checkpoints_model_1L_alternative)\n",
    "model_1L.summary()\n",
    "\n",
    "model_2L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(16,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Flatten(),\n",
    "                         layers.Dense(501,activation='leaky_relu')],name='model_factors_only_2L')\n",
    "\n",
    "model_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "model_2L.save(checkpoints_model_2L_alternative)\n",
    "model_2L.summary()\n",
    "\n",
    "model_3L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                         layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Dense(8, activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                         layers.Dropout(0.2),\n",
    "                         layers.Flatten(),\n",
    "                         layers.Dense(501,activation='leaky_relu')],name='model_factors_only_3L')\n",
    "\n",
    "model_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "model_3L.save(checkpoints_model_3L_alternative)\n",
    "model_3L.summary()\n",
    "\n",
    "model_4L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(8,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dense(8,activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Dropout(0.2),\n",
    "                            layers.Flatten(),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='model_factors_only_4L')\n",
    "\n",
    "model_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "model_4L.save(checkpoints_model_4L_alternative)\n",
    "model_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e64e1",
   "metadata": {},
   "source": [
    "## Simple LSTM neural network models:\n",
    "* models using the merged dataset including factors and macro data \n",
    "* models using only factor data as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c96074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'LOSS':[tf.keras.losses.MeanSquaredError()],             \n",
    "          'METRICS':[tf.keras.metrics.MeanSquaredError()],    \n",
    "         'OPTIMIZER':adam}  #specify learnig rate after having found optimal value                     #tf.keras.losses.MeanSquaredError() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acf4be",
   "metadata": {},
   "source": [
    "### LSTM models with merged data set (macro & factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "60617221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model_merged_datasets_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_227 (La  (None, 4, 169)           338       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_240 (LSTM)             (None, 128)               152576    \n",
      "                                                                 \n",
      " dense_404 (Dense)           (None, 64)                8256      \n",
      "                                                                 \n",
      " layer_normalization_228 (La  (None, 64)               128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_405 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_406 (Dense)           (None, 501)               16533     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179,911\n",
      "Trainable params: 179,911\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_229 (La  (None, 4, 169)           338       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_241 (LSTM)             (None, 4, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_230 (La  (None, 4, 128)           256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_242 (LSTM)             (None, 64)                49408     \n",
      "                                                                 \n",
      " layer_normalization_231 (La  (None, 64)               128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_407 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_408 (Dense)           (None, 501)               16533     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221,319\n",
      "Trainable params: 221,319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_232 (La  (None, 4, 169)           338       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_243 (LSTM)             (None, 4, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_233 (La  (None, 4, 128)           256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_244 (LSTM)             (None, 4, 64)             49408     \n",
      "                                                                 \n",
      " layer_normalization_234 (La  (None, 4, 64)            128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_245 (LSTM)             (None, 32)                12416     \n",
      "                                                                 \n",
      " layer_normalization_235 (La  (None, 32)               64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_409 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_410 (Dense)           (None, 501)               8517      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 224,231\n",
      "Trainable params: 224,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_merged_datasets_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_246 (LSTM)             (None, 4, 128)            152576    \n",
      "                                                                 \n",
      " layer_normalization_236 (La  (None, 4, 128)           256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_247 (LSTM)             (None, 4, 64)             49408     \n",
      "                                                                 \n",
      " layer_normalization_237 (La  (None, 4, 64)            128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_248 (LSTM)             (None, 4, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_238 (La  (None, 4, 32)            64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_249 (LSTM)             (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_239 (La  (None, 16)               32        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_411 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_412 (Dense)           (None, 501)               4509      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,661\n",
      "Trainable params: 222,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "LSTM_model_merged_1L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128,  return_sequences =False),\n",
    "                                   layers.Dense(64, activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                                   layers.Dense(501,activation='leaky_relu')],name = 'LSTM_model_merged_datasets_1L') \n",
    "\n",
    "LSTM_model_merged_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_1L.save(checkpoints_LSTM_model_merged_1L_alternative)\n",
    "LSTM_model_merged_1L.summary()\n",
    "\n",
    "LSTM_model_merged_2L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64, return_sequences =False),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)), \n",
    "                                   layers.Dense(501,activation='leaky_relu')],name='LSTM_model_merged_datasets_2L')\n",
    "\n",
    "LSTM_model_merged_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_2L.save(checkpoints_LSTM_model_merged_2L_alternative)\n",
    "LSTM_model_merged_2L.summary()\n",
    "\n",
    "LSTM_model_merged_3L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(128,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(32,  return_sequences = False),\n",
    "                                   layers.LayerNormalization(axis=1),\n",
    "                                   layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                                   layers.Dense(501, activation='leaky_relu')],name='LSTM_model_merged_datasets_3L')\n",
    "\n",
    "LSTM_model_merged_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_3L.save(checkpoints_LSTM_model_merged_3L_alternative)\n",
    "LSTM_model_merged_3L.summary()\n",
    "\n",
    "LSTM_model_merged_4L = Sequential([layers.Input(shape=(batch_size,number_of_merged_factors)),\n",
    "                                   layers.LSTM(128, return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(64,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(32,  return_sequences =True),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.LSTM(16,  return_sequences = False),\n",
    "                                   layers.LayerNormalization(),\n",
    "                                   layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                                   layers.Dense(501,activation='leaky_relu' )],name='LSTM_model_merged_datasets_4L')\n",
    "\n",
    "LSTM_model_merged_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_merged_4L.save(checkpoints_LSTM_model_merged_4L_alternative)\n",
    "LSTM_model_merged_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98154e6f",
   "metadata": {},
   "source": [
    "### LSTM models with only factor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a5bc83af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_model_factors_only_1L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_240 (La  (None, 4, 33)            66        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_250 (LSTM)             (None, 64)                25088     \n",
      "                                                                 \n",
      " layer_normalization_241 (La  (None, 64)               128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_413 (Dense)           (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_414 (Dense)           (None, 501)               16533     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,895\n",
      "Trainable params: 43,895\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_2L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_normalization_242 (La  (None, 4, 33)            66        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_251 (LSTM)             (None, 4, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_243 (La  (None, 4, 64)            128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_252 (LSTM)             (None, 32)                12416     \n",
      "                                                                 \n",
      " layer_normalization_244 (La  (None, 32)               64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_415 (Dense)           (None, 16)                528       \n",
      "                                                                 \n",
      " dense_416 (Dense)           (None, 501)               8517      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,807\n",
      "Trainable params: 46,807\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_3L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_253 (LSTM)             (None, 4, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_245 (La  (None, 4, 64)            128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_254 (LSTM)             (None, 4, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_246 (La  (None, 4, 32)            64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_255 (LSTM)             (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_247 (La  (None, 16)               32        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_417 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_418 (Dense)           (None, 501)               4509      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,509\n",
      "Trainable params: 45,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"LSTM_model_factors_only_4L\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_256 (LSTM)             (None, 4, 64)             25088     \n",
      "                                                                 \n",
      " layer_normalization_248 (La  (None, 4, 64)            128       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_257 (LSTM)             (None, 4, 32)             12416     \n",
      "                                                                 \n",
      " layer_normalization_249 (La  (None, 4, 32)            64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_258 (LSTM)             (None, 4, 32)             8320      \n",
      "                                                                 \n",
      " layer_normalization_250 (La  (None, 4, 32)            64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " lstm_259 (LSTM)             (None, 16)                3136      \n",
      "                                                                 \n",
      " layer_normalization_251 (La  (None, 16)               32        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_419 (Dense)           (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_420 (Dense)           (None, 501)               4509      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,893\n",
      "Trainable params: 53,893\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, Sequential   \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "\n",
    "model_input_macro = layers.Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = layers.Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#################################################### LSTM  models with only factor data ##################################################\n",
    "LSTM_model_1L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(64, return_sequences =False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='LSTM_model_factors_only_1L')\n",
    "\n",
    "LSTM_model_1L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_1L.save(checkpoints_LSTM_model_1L_alternative)\n",
    "LSTM_model_1L.summary()\n",
    "\n",
    "LSTM_model_2L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(16, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='LSTM_model_factors_only_2L')\n",
    "\n",
    "LSTM_model_2L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_2L.save(checkpoints_LSTM_model_2L_alternative)\n",
    "LSTM_model_2L.summary()\n",
    "\n",
    "LSTM_model_3L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(16,  return_sequences = False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(8, activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='LSTM_model_factors_only_3L')\n",
    "\n",
    "LSTM_model_3L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_3L.save(checkpoints_LSTM_model_3L_alternative)\n",
    "LSTM_model_3L.summary()\n",
    "\n",
    "LSTM_model_4L = Sequential([layers.Input(shape=(batch_size,number_of_factors)),\n",
    "                            layers.LSTM(64,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(32,  return_sequences =True),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.LSTM(16,   return_sequences = False),\n",
    "                            layers.LayerNormalization(),\n",
    "                            layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005)),\n",
    "                            layers.Dense(501,activation='leaky_relu')],name='LSTM_model_factors_only_4L')\n",
    "\n",
    "LSTM_model_4L.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "LSTM_model_4L.save(checkpoints_LSTM_model_4L_alternative)\n",
    "LSTM_model_4L.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95687cc9",
   "metadata": {},
   "source": [
    "### Combined models (LSTM transformed macro + LSTM FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1dc67",
   "metadata": {},
   "source": [
    "### One layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64634a",
   "metadata": {},
   "source": [
    "Model using another macro model to pass input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dee2ce57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"one_layer_FFN_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_197 (InputLayer)         [(None, 4, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_198 (InputLayer)         [(None, 4, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " one-layer-macro-model (Functio  ((None, 4, 4),      145296      ['input_197[0][0]']              \n",
      " nal)                            (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_17 (Concatenate)   (None, 4, 37)        0           ['input_198[0][0]',              \n",
      "                                                                  'one-layer-macro-model[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_262 (LSTM)                (None, 32)           8960        ['concatenate_17[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_253 (Layer  (None, 32)          64          ['lstm_262[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_422 (Dense)              (None, 16)           528         ['layer_normalization_253[0][0]']\n",
      "                                                                                                  \n",
      " dense_423 (Dense)              (None, 501)          8517        ['dense_422[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 163,365\n",
      "Trainable params: 163,365\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_one_layer_FFN = {'input_units_macro': 128, 'output_units':4, 'outputs':0, 'input_units_FFN_1L':32}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_one_layer_FFN['input_units_macro'], return_sequences =True, return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "regularize_1 = layers.Dense(64,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_1)\n",
    "LSTM_model_macro_L1_output,LSTM_model_macro_L1_states_h, LSTM_model_macro_L1_states_c = layers.LSTM(specs_one_layer_FFN['output_units'],return_sequences=True, return_state=True)(regularize_1)\n",
    "\n",
    "LSTM_model_macro_1L = Model(model_input_macro, (LSTM_model_macro_L1_output,LSTM_model_macro_L1_states_h), name='one-layer-macro-model')\n",
    "macro_transformed_1L = LSTM_model_macro_1L(model_input_macro)[specs_one_layer_FFN['outputs']] \n",
    "\n",
    "# # macro_transformed_1L = tf.expand_dims(macro_transformed_1L, 1)\n",
    "merged_layers_transformed_1L = layers.Concatenate()([model_input_factors,macro_transformed_1L])\n",
    "\n",
    "#one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_one_layer_FFN['input_units_FFN_1L'],  return_sequences =False, return_state=False)(merged_layers_transformed_1L)\n",
    "normalize_2 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "regularize_2 = layers.Dense(16,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0001))(normalize_2)\n",
    "returns_L1 = layers.Dense(501,activation='leaky_relu')(regularize_2)\n",
    "\n",
    "FFN_model_L1 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L1, name='one_layer_FFN_model')\n",
    "# FFN_model_L1([macro_factors,monthly_factors])\n",
    "\n",
    "\n",
    "FFN_model_L1.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L1.save(checkpoints_FFN_model_L1_alternative)\n",
    "FFN_model_L1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd29a7",
   "metadata": {},
   "source": [
    "### Two layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d8ed56f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"two-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_199 (InputLayer)         [(None, 4, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_200 (InputLayer)         [(None, 4, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " two-layer-macro-model (Functio  ((None, 4, 4),      188144      ['input_199[0][0]']              \n",
      " nal)                            (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_18 (Concatenate)   (None, 4, 37)        0           ['input_200[0][0]',              \n",
      "                                                                  'two-layer-macro-model[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_266 (LSTM)                (None, 4, 32)        8960        ['concatenate_18[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_256 (Layer  (None, 4, 32)       64          ['lstm_266[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " lstm_267 (LSTM)                (None, 16)           3136        ['layer_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_257 (Layer  (None, 16)          32          ['lstm_267[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_425 (Dense)              (None, 32)           544         ['layer_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " dense_426 (Dense)              (None, 501)          16533       ['dense_425[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 217,413\n",
      "Trainable params: 217,413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_two_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64, 'output_units':4, 'outputs':0,\n",
    "                       'input_units_FFN_1L':32, 'input_units_FFN_2L':16}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_two_layer_FFN['input_units_macro_1L'],  return_sequences =True,  return_state=False)(model_input_macro)\n",
    "normalize_1= layers.LayerNormalization()(x_macro_L1)\n",
    "x_macro_L2 = layers.LSTM(specs_two_layer_FFN['input_units_macro_2L'], return_sequences =True,  return_state=False)(normalize_1)\n",
    "normalize_2= layers.LayerNormalization()(x_macro_L2)\n",
    "regularize_1 = layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_2)\n",
    "LSTM_model_macro_L2_output,LSTM_model_macro_L2_states_h, LSTM_model_macro_L2_states_c = layers.LSTM(specs_two_layer_FFN['output_units'],return_sequences=True, return_state=True)(regularize_1)\n",
    "\n",
    "LSTM_model_macro_2L = Model(model_input_macro, (LSTM_model_macro_L2_output,LSTM_model_macro_L2_states_h), name='two-layer-macro-model')\n",
    "macro_transformed_2L = LSTM_model_macro_2L(model_input_macro)[specs_two_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_2L = tf.expand_dims(macro_transformed_2L, 1)\n",
    "merged_layers_transformed_2L = layers.Concatenate()([model_input_factors,macro_transformed_2L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_two_layer_FFN['input_units_FFN_1L'],  return_sequences =True, return_state=False)(merged_layers_transformed_2L)\n",
    "normalize_3= layers.LayerNormalization()(LSTM_comb_1L)\n",
    "LSTM_comb_2L = layers.LSTM(specs_two_layer_FFN['input_units_FFN_2L'],  return_sequences =False,  return_state=False)(normalize_3)\n",
    "normalize_4 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "regularize_2 = layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0001))(normalize_4)\n",
    "returns_L2 = layers.Dense(501, activation='leaky_relu')(regularize_2)\n",
    "\n",
    "FFN_model_L2 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L2, name='two-layer-FFN-model')\n",
    "\n",
    "FFN_model_L2.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L2.save(checkpoints_FFN_model_L2_alternative)\n",
    "FFN_model_L2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c4691",
   "metadata": {},
   "source": [
    "### Three layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5ba409cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"three-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_205 (InputLayer)         [(None, 4, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_206 (InputLayer)         [(None, 4, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " three-layer-macro-model (Funct  ((None, 4, 4),      191472      ['input_205[0][0]']              \n",
      " ional)                          (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_21 (Concatenate)   (None, 4, 37)        0           ['input_206[0][0]',              \n",
      "                                                                  'three-layer-macro-model[0][0]']\n",
      "                                                                                                  \n",
      " lstm_288 (LSTM)                (None, 4, 32)        8960        ['concatenate_21[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_275 (Layer  (None, 4, 32)       64          ['lstm_288[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_439 (Dense)              (None, 4, 32)        1056        ['layer_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " lstm_289 (LSTM)                (None, 4, 16)        3136        ['dense_439[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_276 (Layer  (None, 4, 16)       32          ['lstm_289[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " lstm_290 (LSTM)                (None, 8)            800         ['layer_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_277 (Layer  (None, 8)           16          ['lstm_290[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_440 (Dense)              (None, 4)            36          ['layer_normalization_277[0][0]']\n",
      "                                                                                                  \n",
      " dense_441 (Dense)              (None, 501)          2505        ['dense_440[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 208,077\n",
      "Trainable params: 208,077\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_three_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64,'input_units_macro_3L': 32, \n",
    "                         'output_units':4, 'outputs':0, 'input_units_FFN_1L':32, 'input_units_FFN_2L':16,\n",
    "                         'input_units_FFN_3L':8}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_three_layer_FFN['input_units_macro_1L'],  return_sequences =True,  return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "regularize_1 = layers.Dense(64,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_1)\n",
    "x_macro_L2 = layers.LSTM(specs_three_layer_FFN['input_units_macro_2L'],  return_sequences =True,  return_state=False)(regularize_1)\n",
    "normalize_2 = layers.LayerNormalization()(x_macro_L2)\n",
    "x_macro_L3 = layers.LSTM(specs_three_layer_FFN['input_units_macro_3L'],  return_sequences =True,  return_state=False)(normalize_2)\n",
    "normalize_3 = layers.LayerNormalization()(x_macro_L3)\n",
    "regularize_2 = layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.005))(normalize_3)\n",
    "LSTM_model_macro_L3_output,LSTM_model_macro_L3_states_h, LSTM_model_macro_L3_states_c = layers.LSTM(specs_three_layer_FFN['output_units'],return_sequences=True, return_state=True)(regularize_2)\n",
    "\n",
    "LSTM_model_macro_3L = Model(model_input_macro, (LSTM_model_macro_L3_output,LSTM_model_macro_L3_states_h), name='three-layer-macro-model')\n",
    "macro_transformed_3L = LSTM_model_macro_3L(model_input_macro)[specs_three_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_3L = tf.expand_dims(macro_transformed_3L, 1)\n",
    "merged_layers_transformed_3L = layers.Concatenate()([model_input_factors,macro_transformed_3L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_1L'], return_sequences =True, return_state=False)(merged_layers_transformed_3L)\n",
    "normalize_4 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "regularize_3 = layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_4)\n",
    "LSTM_comb_2L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_2L'], return_sequences =True, return_state=False)(regularize_3)\n",
    "normalize_5 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "LSTM_comb_3L = layers.LSTM(specs_three_layer_FFN['input_units_FFN_3L'], return_sequences =False,  return_state=False)(normalize_5)\n",
    "normalize_6 = layers.LayerNormalization()(LSTM_comb_3L)\n",
    "regularize_4 = layers.Dense(4,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0001))(normalize_6)\n",
    "returns_L3 = layers.Dense(501,activation='leaky_relu')(regularize_4)\n",
    "\n",
    "FFN_model_L3 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L3, name='three-layer-FFN-model')\n",
    "\n",
    "FFN_model_L3.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L3.save(checkpoints_FFN_model_L3_alternative)\n",
    "FFN_model_L3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd396bbf",
   "metadata": {},
   "source": [
    "### Four layer combined model\n",
    "\n",
    "Adjust outputs:\n",
    "* to 0 to get LSTM output that includes hidden states and transformes input\n",
    "* to 1 to get LSTM hidden states h (only compatible with batch size = 1, activate expand dims layer function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "18e40608",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"four-layer-FFN-model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_207 (InputLayer)         [(None, 4, 136)]     0           []                               \n",
      "                                                                                                  \n",
      " input_208 (InputLayer)         [(None, 4, 33)]      0           []                               \n",
      "                                                                                                  \n",
      " four-layer-macro-model (Functi  ((None, 4, 4),      201464      ['input_207[0][0]']              \n",
      " onal)                           (None, 4))                                                       \n",
      "                                                                                                  \n",
      " concatenate_22 (Concatenate)   (None, 4, 37)        0           ['input_208[0][0]',              \n",
      "                                                                  'four-layer-macro-model[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_296 (LSTM)                (None, 4, 32)        8960        ['concatenate_22[0][0]']         \n",
      "                                                                                                  \n",
      " layer_normalization_282 (Layer  (None, 4, 32)       64          ['lstm_296[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_444 (Dense)              (None, 4, 32)        1056        ['layer_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " lstm_297 (LSTM)                (None, 4, 32)        8320        ['dense_444[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_283 (Layer  (None, 4, 32)       64          ['lstm_297[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " lstm_298 (LSTM)                (None, 4, 16)        3136        ['layer_normalization_283[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_284 (Layer  (None, 4, 16)       32          ['lstm_298[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " lstm_299 (LSTM)                (None, 8)            800         ['layer_normalization_284[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_285 (Layer  (None, 8)           16          ['lstm_299[0][0]']               \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_445 (Dense)              (None, 8)            72          ['layer_normalization_285[0][0]']\n",
      "                                                                                                  \n",
      " dense_446 (Dense)              (None, 501)          4509        ['dense_445[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 228,493\n",
      "Trainable params: 228,493\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "specs_four_layer_FFN = {'input_units_macro_1L': 128,'input_units_macro_2L': 64,'input_units_macro_3L': 32, 'input_units_macro_4L': 16,\n",
    "                        'output_units':4, 'outputs':0, 'input_units_FFN_1L':32, 'input_units_FFN_2L':32,'input_units_FFN_3L':16,\n",
    "                        'input_units_FFN_4L':8}\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "model_input_macro = Input(shape=(batch_size,number_of_macro_factors))      #Input = number of timesteps (batch size), number of features\n",
    "model_input_factors = Input(shape=(batch_size,number_of_factors))\n",
    "\n",
    "#transform macro variables\n",
    "x_macro_L1 = layers.LSTM(specs_four_layer_FFN['input_units_macro_1L'],  return_sequences =True, return_state=False)(model_input_macro)\n",
    "normalize_1 = layers.LayerNormalization()(x_macro_L1)\n",
    "regularize_1 = layers.Dense(64,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_1)\n",
    "x_macro_L2 = layers.LSTM(specs_four_layer_FFN['input_units_macro_2L'],  return_sequences =True, return_state=False)(normalize_1)\n",
    "normalize_2 = layers.LayerNormalization()(x_macro_L2)\n",
    "x_macro_L3 = layers.LSTM(specs_four_layer_FFN['input_units_macro_3L'],  return_sequences =True, return_state=False)(normalize_2)\n",
    "normalize_3 = layers.LayerNormalization()(x_macro_L3)\n",
    "x_macro_L4 = layers.LSTM(specs_four_layer_FFN['input_units_macro_4L'],  return_sequences =True, return_state=False)(normalize_3)\n",
    "normalize_4 = layers.LayerNormalization()(x_macro_L4)\n",
    "regularize_2 = layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_4)\n",
    "LSTM_model_macro_L4_output,LSTM_model_macro_L4_states_h, LSTM_model_macro_L4_states_c = layers.LSTM(specs_four_layer_FFN['output_units'],return_sequences=True, return_state=True)(regularize_2)\n",
    "\n",
    "LSTM_model_macro_4L = Model(model_input_macro, (LSTM_model_macro_L4_output,LSTM_model_macro_L4_states_h), name='four-layer-macro-model')\n",
    "macro_transformed_4L = LSTM_model_macro_4L(model_input_macro)[specs_four_layer_FFN['outputs']] \n",
    "\n",
    "# macro_transformed_4L = tf.expand_dims(macro_transformed_4L, 1)\n",
    "merged_layers_transformed_4L = layers.Concatenate()([model_input_factors,macro_transformed_4L])\n",
    "\n",
    "# #one layer LSTM network to predict returns\n",
    "LSTM_comb_1L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_1L'], return_sequences =True, return_state=False)(merged_layers_transformed_4L)\n",
    "normalize_5 = layers.LayerNormalization()(LSTM_comb_1L)\n",
    "regularize_3 = layers.Dense(32,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0005))(normalize_5)\n",
    "LSTM_comb_2L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_2L'],  return_sequences =True, return_state=False)(regularize_3)\n",
    "normalize_6 = layers.LayerNormalization()(LSTM_comb_2L)\n",
    "LSTM_comb_3L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_3L'],  return_sequences =True, return_state=False)(normalize_6)\n",
    "normalize_7 = layers.LayerNormalization()(LSTM_comb_3L)\n",
    "LSTM_comb_4L = layers.LSTM(specs_four_layer_FFN['input_units_FFN_4L'],  return_sequences =False,return_state=False)(normalize_7)\n",
    "normalize_8 = layers.LayerNormalization()(LSTM_comb_4L)\n",
    "regularize_4 = layers.Dense(8,activation='leaky_relu', kernel_regularizer=tf.keras.regularizers.L2(0.0001))(normalize_8)\n",
    "returns_L4 = layers.Dense(501, activation='leaky_relu')(regularize_4)\n",
    "\n",
    "FFN_model_L4 = Model(inputs=(model_input_macro,model_input_factors),outputs=returns_L4, name='four-layer-FFN-model')\n",
    "\n",
    "FFN_model_L4.compile(loss=config['LOSS'],optimizer=config['OPTIMIZER'],metrics=config['METRICS'])\n",
    "FFN_model_L4.save(checkpoints_FFN_model_L4_alternative)\n",
    "FFN_model_L4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d99f5",
   "metadata": {},
   "source": [
    "# Neural Network Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f9a12",
   "metadata": {},
   "source": [
    "## Find optimal adam learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f859aad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"def compile_and_fit_simple(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, batch_size, num_epochs, goal):\"\"\"\n",
    "\n",
    "\n",
    "# model_history_LSTM_model_merged_1L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_1L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  2, batch_size, \n",
    "#                     100, 'lr', class_weights_dict)\n",
    "# model_history_LSTM_model_merged_2L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_2L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, 2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)\n",
    "# model_history_LSTM_model_merged_3L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_3L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, 2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)\n",
    "# model_history_LSTM_model_merged_4L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_4L,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  2, batch_size, \n",
    "#                     100,'lr',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06eef346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs = 1e-4*(10**(tf.range(20)/20))\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.semilogx(lrs, model_history_LSTM_model_merged_1L.history['loss'])\n",
    "# plt.xlabel('learningrate')\n",
    "# plt.ylable('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464778a",
   "metadata": {},
   "source": [
    "## Fitting Feedforward models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dfa155",
   "metadata": {},
   "source": [
    "### Merged Feedforward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "574fc2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 4, 136)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_factors.shape\n",
    "x_train_macro.shape\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fe85aab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1724 - mean_squared_error: 0.1413 - val_loss: 0.4336 - val_mean_squared_error: 0.4044\n",
      "Epoch 14/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1680 - mean_squared_error: 0.1382 - val_loss: 0.4358 - val_mean_squared_error: 0.4069\n",
      "Epoch 15/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1655 - mean_squared_error: 0.1362 - val_loss: 0.4346 - val_mean_squared_error: 0.4023\n",
      "Epoch 16/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1771 - mean_squared_error: 0.1437 - val_loss: 0.4468 - val_mean_squared_error: 0.4154\n",
      "Epoch 17/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1721 - mean_squared_error: 0.1392 - val_loss: 0.3900 - val_mean_squared_error: 0.3595\n",
      "Epoch 18/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1719 - mean_squared_error: 0.1397 - val_loss: 0.4296 - val_mean_squared_error: 0.3985\n",
      "Epoch 19/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1710 - mean_squared_error: 0.1406 - val_loss: 0.4072 - val_mean_squared_error: 0.3777\n",
      "Epoch 20/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1652 - mean_squared_error: 0.1360 - val_loss: 0.3975 - val_mean_squared_error: 0.3693\n",
      "Epoch 21/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1660 - mean_squared_error: 0.1373 - val_loss: 0.3905 - val_mean_squared_error: 0.3611\n",
      "Epoch 22/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1343 - val_loss: 0.3950 - val_mean_squared_error: 0.3656\n",
      "Epoch 23/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1653 - mean_squared_error: 0.1363 - val_loss: 0.4612 - val_mean_squared_error: 0.4320\n",
      "Epoch 24/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1715 - mean_squared_error: 0.1410 - val_loss: 0.3992 - val_mean_squared_error: 0.3684\n",
      "Epoch 25/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1693 - mean_squared_error: 0.1382 - val_loss: 0.4063 - val_mean_squared_error: 0.3745\n",
      "Epoch 26/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1668 - mean_squared_error: 0.1356 - val_loss: 0.4184 - val_mean_squared_error: 0.3880\n",
      "Epoch 27/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1667 - mean_squared_error: 0.1364 - val_loss: 0.4257 - val_mean_squared_error: 0.3955\n",
      "Epoch 28/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1658 - mean_squared_error: 0.1361 - val_loss: 0.4225 - val_mean_squared_error: 0.3927\n",
      "Epoch 29/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1685 - mean_squared_error: 0.1388 - val_loss: 0.3989 - val_mean_squared_error: 0.3634\n",
      "Epoch 30/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1656 - mean_squared_error: 0.1347 - val_loss: 0.4048 - val_mean_squared_error: 0.3753\n",
      "Epoch 31/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1680 - mean_squared_error: 0.1370 - val_loss: 0.4132 - val_mean_squared_error: 0.3775\n",
      "Epoch 32/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1741 - mean_squared_error: 0.1409 - val_loss: 0.3946 - val_mean_squared_error: 0.3625\n",
      "Epoch 33/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1706 - mean_squared_error: 0.1387 - val_loss: 0.4267 - val_mean_squared_error: 0.3949\n",
      "Epoch 34/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1655 - mean_squared_error: 0.1353 - val_loss: 0.4447 - val_mean_squared_error: 0.4150\n",
      "Epoch 35/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1351 - val_loss: 0.4507 - val_mean_squared_error: 0.4207\n",
      "Epoch 36/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1674 - mean_squared_error: 0.1366 - val_loss: 0.4151 - val_mean_squared_error: 0.3854\n",
      "Epoch 37/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1670 - mean_squared_error: 0.1370 - val_loss: 0.4345 - val_mean_squared_error: 0.4054\n",
      "Epoch 38/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1714 - mean_squared_error: 0.1395 - val_loss: 0.3835 - val_mean_squared_error: 0.3517\n",
      "Epoch 39/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1728 - mean_squared_error: 0.1390 - val_loss: 0.4054 - val_mean_squared_error: 0.3689\n",
      "Epoch 40/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1692 - mean_squared_error: 0.1372 - val_loss: 0.4281 - val_mean_squared_error: 0.3967\n",
      "Epoch 41/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1798 - mean_squared_error: 0.1441 - val_loss: 0.4216 - val_mean_squared_error: 0.3861\n",
      "Epoch 42/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1784 - mean_squared_error: 0.1419 - val_loss: 0.4150 - val_mean_squared_error: 0.3817\n",
      "Epoch 43/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1726 - mean_squared_error: 0.1402 - val_loss: 0.4086 - val_mean_squared_error: 0.3766\n",
      "Epoch 44/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1694 - mean_squared_error: 0.1380 - val_loss: 0.4073 - val_mean_squared_error: 0.3782\n",
      "Epoch 45/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1701 - mean_squared_error: 0.1390 - val_loss: 0.4230 - val_mean_squared_error: 0.3919\n",
      "Epoch 46/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1684 - mean_squared_error: 0.1375 - val_loss: 0.4414 - val_mean_squared_error: 0.4090\n",
      "Epoch 47/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1707 - mean_squared_error: 0.1384 - val_loss: 0.3815 - val_mean_squared_error: 0.3491\n",
      "Epoch 48/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1714 - mean_squared_error: 0.1387 - val_loss: 0.4189 - val_mean_squared_error: 0.3868\n",
      "Epoch 49/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1664 - mean_squared_error: 0.1344 - val_loss: 0.4445 - val_mean_squared_error: 0.4136\n",
      "Epoch 50/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1334 - val_loss: 0.4367 - val_mean_squared_error: 0.4083\n",
      "Epoch 51/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1643 - mean_squared_error: 0.1350 - val_loss: 0.4409 - val_mean_squared_error: 0.4113\n",
      "Epoch 52/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1352 - val_loss: 0.4083 - val_mean_squared_error: 0.3765\n",
      "Epoch 53/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1711 - mean_squared_error: 0.1388 - val_loss: 0.4119 - val_mean_squared_error: 0.3819\n",
      "Epoch 54/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1669 - mean_squared_error: 0.1369 - val_loss: 0.4258 - val_mean_squared_error: 0.3951\n",
      "Epoch 55/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1367 - val_loss: 0.4413 - val_mean_squared_error: 0.4113\n",
      "Epoch 56/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1709 - mean_squared_error: 0.1385 - val_loss: 0.3833 - val_mean_squared_error: 0.3545\n",
      "Epoch 57/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1343 - val_loss: 0.4010 - val_mean_squared_error: 0.3724\n",
      "Epoch 58/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1372 - val_loss: 0.4862 - val_mean_squared_error: 0.4538\n",
      "Epoch 59/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1710 - mean_squared_error: 0.1394 - val_loss: 0.4598 - val_mean_squared_error: 0.4243\n",
      "Epoch 60/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1728 - mean_squared_error: 0.1380 - val_loss: 0.4658 - val_mean_squared_error: 0.4341\n",
      "Epoch 61/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1680 - mean_squared_error: 0.1370 - val_loss: 0.4459 - val_mean_squared_error: 0.4145\n",
      "Epoch 62/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1671 - mean_squared_error: 0.1365 - val_loss: 0.4390 - val_mean_squared_error: 0.4095\n",
      "Epoch 63/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1568 - mean_squared_error: 0.1278 - val_loss: 0.4009 - val_mean_squared_error: 0.3712\n",
      "Epoch 64/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1333 - val_loss: 0.4515 - val_mean_squared_error: 0.4178\n",
      "Epoch 65/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1663 - mean_squared_error: 0.1338 - val_loss: 0.4331 - val_mean_squared_error: 0.4025\n",
      "Epoch 66/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1611 - mean_squared_error: 0.1314 - val_loss: 0.4249 - val_mean_squared_error: 0.3942\n",
      "Epoch 67/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1728 - mean_squared_error: 0.1401 - val_loss: 0.4450 - val_mean_squared_error: 0.4101\n",
      "Epoch 68/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1684 - mean_squared_error: 0.1356 - val_loss: 0.5029 - val_mean_squared_error: 0.4670\n",
      "Epoch 69/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1726 - mean_squared_error: 0.1380 - val_loss: 0.4127 - val_mean_squared_error: 0.3811\n",
      "Epoch 70/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1764 - mean_squared_error: 0.1429 - val_loss: 0.4145 - val_mean_squared_error: 0.3798\n",
      "Epoch 71/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1667 - mean_squared_error: 0.1334 - val_loss: 0.4774 - val_mean_squared_error: 0.4467\n",
      "Epoch 72/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1695 - mean_squared_error: 0.1381 - val_loss: 0.4255 - val_mean_squared_error: 0.3926\n",
      "Epoch 73/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1661 - mean_squared_error: 0.1342 - val_loss: 0.4554 - val_mean_squared_error: 0.4255\n",
      "Epoch 74/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1340 - val_loss: 0.4324 - val_mean_squared_error: 0.4020\n",
      "Epoch 75/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1657 - mean_squared_error: 0.1338 - val_loss: 0.4186 - val_mean_squared_error: 0.3879\n",
      "Epoch 76/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1628 - mean_squared_error: 0.1332 - val_loss: 0.4494 - val_mean_squared_error: 0.4195\n",
      "Epoch 77/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1656 - mean_squared_error: 0.1356 - val_loss: 0.4241 - val_mean_squared_error: 0.3935\n",
      "Epoch 78/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1695 - mean_squared_error: 0.1365 - val_loss: 0.4837 - val_mean_squared_error: 0.4501\n",
      "Epoch 79/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1661 - mean_squared_error: 0.1342 - val_loss: 0.4933 - val_mean_squared_error: 0.4557\n",
      "Epoch 80/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1754 - mean_squared_error: 0.1386 - val_loss: 0.4256 - val_mean_squared_error: 0.3927\n",
      "Epoch 81/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1698 - mean_squared_error: 0.1371 - val_loss: 0.3900 - val_mean_squared_error: 0.3574\n",
      "Epoch 82/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1680 - mean_squared_error: 0.1353 - val_loss: 0.4050 - val_mean_squared_error: 0.3728\n",
      "Epoch 83/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1301 - val_loss: 0.4051 - val_mean_squared_error: 0.3757\n",
      "Epoch 84/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1643 - mean_squared_error: 0.1335 - val_loss: 0.4487 - val_mean_squared_error: 0.4142\n",
      "Epoch 85/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1716 - mean_squared_error: 0.1381 - val_loss: 0.4767 - val_mean_squared_error: 0.4375\n",
      "Epoch 86/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1695 - mean_squared_error: 0.1348 - val_loss: 0.4558 - val_mean_squared_error: 0.4243\n",
      "Epoch 87/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1650 - mean_squared_error: 0.1337 - val_loss: 0.3971 - val_mean_squared_error: 0.3662\n",
      "Epoch 88/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1643 - mean_squared_error: 0.1336 - val_loss: 0.4331 - val_mean_squared_error: 0.4034\n",
      "Epoch 89/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1627 - mean_squared_error: 0.1335 - val_loss: 0.4269 - val_mean_squared_error: 0.3964\n",
      "Epoch 90/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1668 - mean_squared_error: 0.1353 - val_loss: 0.4073 - val_mean_squared_error: 0.3769\n",
      "Epoch 91/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1683 - mean_squared_error: 0.1370 - val_loss: 0.4465 - val_mean_squared_error: 0.4129\n",
      "Epoch 92/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1686 - mean_squared_error: 0.1357 - val_loss: 0.4634 - val_mean_squared_error: 0.4303\n",
      "Epoch 93/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1650 - mean_squared_error: 0.1341 - val_loss: 0.4027 - val_mean_squared_error: 0.3728\n",
      "Epoch 94/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1672 - mean_squared_error: 0.1357 - val_loss: 0.4353 - val_mean_squared_error: 0.4066\n",
      "Epoch 95/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1604 - mean_squared_error: 0.1304 - val_loss: 0.4370 - val_mean_squared_error: 0.4060\n",
      "Epoch 96/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1654 - mean_squared_error: 0.1337 - val_loss: 0.3997 - val_mean_squared_error: 0.3672\n",
      "Epoch 97/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1701 - mean_squared_error: 0.1361 - val_loss: 0.4920 - val_mean_squared_error: 0.4556\n",
      "Epoch 98/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1767 - mean_squared_error: 0.1410 - val_loss: 0.4383 - val_mean_squared_error: 0.3958\n",
      "Epoch 99/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1720 - mean_squared_error: 0.1356 - val_loss: 0.3953 - val_mean_squared_error: 0.3640\n",
      "Epoch 100/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1668 - mean_squared_error: 0.1353 - val_loss: 0.4572 - val_mean_squared_error: 0.4264\n",
      "Epoch 101/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1591 - mean_squared_error: 0.1297 - val_loss: 0.3929 - val_mean_squared_error: 0.3637\n",
      "Epoch 102/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1323 - val_loss: 0.4454 - val_mean_squared_error: 0.4103\n",
      "Epoch 103/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1609 - mean_squared_error: 0.1298 - val_loss: 0.4157 - val_mean_squared_error: 0.3868\n",
      "Epoch 104/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1319 - val_loss: 0.4101 - val_mean_squared_error: 0.3801\n",
      "Epoch 105/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1660 - mean_squared_error: 0.1347 - val_loss: 0.4313 - val_mean_squared_error: 0.3954\n",
      "Epoch 106/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1727 - mean_squared_error: 0.1382 - val_loss: 0.4983 - val_mean_squared_error: 0.4618\n",
      "Epoch 107/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1628 - mean_squared_error: 0.1297 - val_loss: 0.4618 - val_mean_squared_error: 0.4313\n",
      "Epoch 108/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1671 - mean_squared_error: 0.1354 - val_loss: 0.4398 - val_mean_squared_error: 0.4084\n",
      "Epoch 109/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1622 - mean_squared_error: 0.1308 - val_loss: 0.4056 - val_mean_squared_error: 0.3753\n",
      "Epoch 110/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1305 - val_loss: 0.4578 - val_mean_squared_error: 0.4278\n",
      "Epoch 111/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1359 - val_loss: 0.4244 - val_mean_squared_error: 0.3930\n",
      "Epoch 112/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1633 - mean_squared_error: 0.1317 - val_loss: 0.4407 - val_mean_squared_error: 0.4087\n",
      "Epoch 113/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1604 - mean_squared_error: 0.1303 - val_loss: 0.4109 - val_mean_squared_error: 0.3813\n",
      "Epoch 114/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1344 - val_loss: 0.4347 - val_mean_squared_error: 0.4032\n",
      "Epoch 115/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1330 - val_loss: 0.4160 - val_mean_squared_error: 0.3860\n",
      "Epoch 116/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1322 - val_loss: 0.4259 - val_mean_squared_error: 0.3939\n",
      "Epoch 117/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1761 - mean_squared_error: 0.1387 - val_loss: 0.4220 - val_mean_squared_error: 0.3892\n",
      "Epoch 118/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1651 - mean_squared_error: 0.1332 - val_loss: 0.4259 - val_mean_squared_error: 0.3962\n",
      "Epoch 119/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1664 - mean_squared_error: 0.1346 - val_loss: 0.4508 - val_mean_squared_error: 0.4192\n",
      "Epoch 120/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1659 - mean_squared_error: 0.1346 - val_loss: 0.4815 - val_mean_squared_error: 0.4482\n",
      "Epoch 121/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1680 - mean_squared_error: 0.1355 - val_loss: 0.4339 - val_mean_squared_error: 0.4031\n",
      "Epoch 122/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1636 - mean_squared_error: 0.1328 - val_loss: 0.4184 - val_mean_squared_error: 0.3884\n",
      "Epoch 123/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1303 - val_loss: 0.4164 - val_mean_squared_error: 0.3840\n",
      "Epoch 124/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1657 - mean_squared_error: 0.1334 - val_loss: 0.4811 - val_mean_squared_error: 0.4457\n",
      "Epoch 125/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1658 - mean_squared_error: 0.1326 - val_loss: 0.4547 - val_mean_squared_error: 0.4244\n",
      "Epoch 126/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1641 - mean_squared_error: 0.1330 - val_loss: 0.4376 - val_mean_squared_error: 0.4059\n",
      "Epoch 127/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1627 - mean_squared_error: 0.1321 - val_loss: 0.4224 - val_mean_squared_error: 0.3933\n",
      "Epoch 128/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1665 - mean_squared_error: 0.1339 - val_loss: 0.3875 - val_mean_squared_error: 0.3547\n",
      "Epoch 129/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1622 - mean_squared_error: 0.1305 - val_loss: 0.4283 - val_mean_squared_error: 0.3959\n",
      "Epoch 130/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1721 - mean_squared_error: 0.1365 - val_loss: 0.4444 - val_mean_squared_error: 0.4103\n",
      "Epoch 131/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1691 - mean_squared_error: 0.1352 - val_loss: 0.4338 - val_mean_squared_error: 0.4009\n",
      "Epoch 132/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1633 - mean_squared_error: 0.1321 - val_loss: 0.4059 - val_mean_squared_error: 0.3730\n",
      "Epoch 133/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1726 - mean_squared_error: 0.1386 - val_loss: 0.4003 - val_mean_squared_error: 0.3640\n",
      "Epoch 134/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1665 - mean_squared_error: 0.1340 - val_loss: 0.3952 - val_mean_squared_error: 0.3646\n",
      "Epoch 135/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1644 - mean_squared_error: 0.1326 - val_loss: 0.4389 - val_mean_squared_error: 0.4077\n",
      "Epoch 136/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1671 - mean_squared_error: 0.1345 - val_loss: 0.5149 - val_mean_squared_error: 0.4816\n",
      "Epoch 137/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1308 - val_loss: 0.4069 - val_mean_squared_error: 0.3750\n",
      "Epoch 138/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1709 - mean_squared_error: 0.1367 - val_loss: 0.4568 - val_mean_squared_error: 0.4220\n",
      "Epoch 139/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1686 - mean_squared_error: 0.1340 - val_loss: 0.4334 - val_mean_squared_error: 0.4006\n",
      "Epoch 140/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1693 - mean_squared_error: 0.1360 - val_loss: 0.4012 - val_mean_squared_error: 0.3715\n",
      "Epoch 141/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1644 - mean_squared_error: 0.1328 - val_loss: 0.3961 - val_mean_squared_error: 0.3629\n",
      "Epoch 142/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1676 - mean_squared_error: 0.1343 - val_loss: 0.4068 - val_mean_squared_error: 0.3750\n",
      "Epoch 143/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1619 - mean_squared_error: 0.1302 - val_loss: 0.4402 - val_mean_squared_error: 0.4082\n",
      "Epoch 144/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1310 - val_loss: 0.4298 - val_mean_squared_error: 0.4000\n",
      "Epoch 145/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1684 - mean_squared_error: 0.1365 - val_loss: 0.4224 - val_mean_squared_error: 0.3885\n",
      "Epoch 146/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1292 - val_loss: 0.4212 - val_mean_squared_error: 0.3901\n",
      "Epoch 147/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1740 - mean_squared_error: 0.1379 - val_loss: 0.3985 - val_mean_squared_error: 0.3634\n",
      "Epoch 148/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1309 - val_loss: 0.4296 - val_mean_squared_error: 0.3997\n",
      "Epoch 149/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1668 - mean_squared_error: 0.1353 - val_loss: 0.4216 - val_mean_squared_error: 0.3862\n",
      "Epoch 150/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1679 - mean_squared_error: 0.1346 - val_loss: 0.4647 - val_mean_squared_error: 0.4313\n",
      "Epoch 151/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1311 - val_loss: 0.4275 - val_mean_squared_error: 0.3968\n",
      "Epoch 152/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1645 - mean_squared_error: 0.1326 - val_loss: 0.3956 - val_mean_squared_error: 0.3597\n",
      "Epoch 153/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1616 - mean_squared_error: 0.1295 - val_loss: 0.4694 - val_mean_squared_error: 0.4358\n",
      "Epoch 154/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1671 - mean_squared_error: 0.1327 - val_loss: 0.3978 - val_mean_squared_error: 0.3661\n",
      "Epoch 155/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1640 - mean_squared_error: 0.1325 - val_loss: 0.4194 - val_mean_squared_error: 0.3873\n",
      "Epoch 156/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1754 - mean_squared_error: 0.1403 - val_loss: 0.4236 - val_mean_squared_error: 0.3899\n",
      "Epoch 157/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1613 - mean_squared_error: 0.1301 - val_loss: 0.4223 - val_mean_squared_error: 0.3929\n",
      "Epoch 158/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1634 - mean_squared_error: 0.1323 - val_loss: 0.4046 - val_mean_squared_error: 0.3743\n",
      "Epoch 159/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1634 - mean_squared_error: 0.1321 - val_loss: 0.3980 - val_mean_squared_error: 0.3672\n",
      "Epoch 160/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1585 - mean_squared_error: 0.1285 - val_loss: 0.4167 - val_mean_squared_error: 0.3870\n",
      "Epoch 161/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1606 - mean_squared_error: 0.1294 - val_loss: 0.4710 - val_mean_squared_error: 0.4396\n",
      "Epoch 162/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1619 - mean_squared_error: 0.1311 - val_loss: 0.4232 - val_mean_squared_error: 0.3936\n",
      "Epoch 163/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1617 - mean_squared_error: 0.1307 - val_loss: 0.4610 - val_mean_squared_error: 0.4296\n",
      "Epoch 164/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1284 - val_loss: 0.4096 - val_mean_squared_error: 0.3783\n",
      "Epoch 165/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1707 - mean_squared_error: 0.1365 - val_loss: 0.4113 - val_mean_squared_error: 0.3731\n",
      "Epoch 166/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1689 - mean_squared_error: 0.1335 - val_loss: 0.4481 - val_mean_squared_error: 0.4165\n",
      "Epoch 167/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1605 - mean_squared_error: 0.1294 - val_loss: 0.4393 - val_mean_squared_error: 0.4092\n",
      "Epoch 168/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1707 - mean_squared_error: 0.1372 - val_loss: 0.4244 - val_mean_squared_error: 0.3910\n",
      "Epoch 169/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1593 - mean_squared_error: 0.1287 - val_loss: 0.4544 - val_mean_squared_error: 0.4230\n",
      "Epoch 170/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1286 - val_loss: 0.4075 - val_mean_squared_error: 0.3779\n",
      "Epoch 171/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1609 - mean_squared_error: 0.1297 - val_loss: 0.4508 - val_mean_squared_error: 0.4177\n",
      "Epoch 172/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1685 - mean_squared_error: 0.1348 - val_loss: 0.4769 - val_mean_squared_error: 0.4429\n",
      "Epoch 173/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1661 - mean_squared_error: 0.1302 - val_loss: 0.4240 - val_mean_squared_error: 0.3919\n",
      "Epoch 174/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1560 - mean_squared_error: 0.1257 - val_loss: 0.3849 - val_mean_squared_error: 0.3553\n",
      "Epoch 175/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1326 - val_loss: 0.3992 - val_mean_squared_error: 0.3681\n",
      "Epoch 176/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1663 - mean_squared_error: 0.1329 - val_loss: 0.4044 - val_mean_squared_error: 0.3720\n",
      "Epoch 177/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1605 - mean_squared_error: 0.1295 - val_loss: 0.4743 - val_mean_squared_error: 0.4419\n",
      "Epoch 178/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1628 - mean_squared_error: 0.1305 - val_loss: 0.4205 - val_mean_squared_error: 0.3887\n",
      "Epoch 179/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1639 - mean_squared_error: 0.1309 - val_loss: 0.4737 - val_mean_squared_error: 0.4393\n",
      "Epoch 180/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1692 - mean_squared_error: 0.1345 - val_loss: 0.4404 - val_mean_squared_error: 0.4058\n",
      "Epoch 181/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1622 - mean_squared_error: 0.1298 - val_loss: 0.4166 - val_mean_squared_error: 0.3843\n",
      "Epoch 182/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1578 - mean_squared_error: 0.1272 - val_loss: 0.4600 - val_mean_squared_error: 0.4277\n",
      "Epoch 183/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1673 - mean_squared_error: 0.1333 - val_loss: 0.4518 - val_mean_squared_error: 0.4173\n",
      "Epoch 184/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1305 - val_loss: 0.4758 - val_mean_squared_error: 0.4445\n",
      "Epoch 185/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1322 - val_loss: 0.4084 - val_mean_squared_error: 0.3758\n",
      "Epoch 186/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1692 - mean_squared_error: 0.1350 - val_loss: 0.4359 - val_mean_squared_error: 0.4013\n",
      "Epoch 187/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1623 - mean_squared_error: 0.1305 - val_loss: 0.4530 - val_mean_squared_error: 0.4225\n",
      "Epoch 188/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1309 - val_loss: 0.4393 - val_mean_squared_error: 0.4073\n",
      "Epoch 189/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1734 - mean_squared_error: 0.1360 - val_loss: 0.4195 - val_mean_squared_error: 0.3858\n",
      "Epoch 190/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1628 - mean_squared_error: 0.1299 - val_loss: 0.4361 - val_mean_squared_error: 0.4044\n",
      "Epoch 191/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1318 - val_loss: 0.4133 - val_mean_squared_error: 0.3808\n",
      "Epoch 192/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1678 - mean_squared_error: 0.1337 - val_loss: 0.4250 - val_mean_squared_error: 0.3897\n",
      "Epoch 193/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1718 - mean_squared_error: 0.1367 - val_loss: 0.4347 - val_mean_squared_error: 0.3990\n",
      "Epoch 194/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1658 - mean_squared_error: 0.1312 - val_loss: 0.4377 - val_mean_squared_error: 0.4035\n",
      "Epoch 195/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1302 - val_loss: 0.4308 - val_mean_squared_error: 0.3998\n",
      "Epoch 196/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1621 - mean_squared_error: 0.1310 - val_loss: 0.4250 - val_mean_squared_error: 0.3937\n",
      "Epoch 197/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1691 - mean_squared_error: 0.1355 - val_loss: 0.4448 - val_mean_squared_error: 0.4105\n",
      "Epoch 198/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1657 - mean_squared_error: 0.1320 - val_loss: 0.4389 - val_mean_squared_error: 0.4083\n",
      "Epoch 199/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1294 - val_loss: 0.5173 - val_mean_squared_error: 0.4864\n",
      "Epoch 200/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1595 - mean_squared_error: 0.1288 - val_loss: 0.4272 - val_mean_squared_error: 0.3975\n",
      "Epoch 201/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1581 - mean_squared_error: 0.1273 - val_loss: 0.4313 - val_mean_squared_error: 0.4009\n",
      "Epoch 202/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1636 - mean_squared_error: 0.1316 - val_loss: 0.4151 - val_mean_squared_error: 0.3822\n",
      "Epoch 203/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1606 - mean_squared_error: 0.1284 - val_loss: 0.4254 - val_mean_squared_error: 0.3947\n",
      "Epoch 204/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1316 - val_loss: 0.4292 - val_mean_squared_error: 0.3966\n",
      "Epoch 205/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1596 - mean_squared_error: 0.1274 - val_loss: 0.4309 - val_mean_squared_error: 0.4002\n",
      "Epoch 206/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1571 - mean_squared_error: 0.1258 - val_loss: 0.4092 - val_mean_squared_error: 0.3784\n",
      "Epoch 207/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1644 - mean_squared_error: 0.1324 - val_loss: 0.4370 - val_mean_squared_error: 0.4025\n",
      "Epoch 208/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1633 - mean_squared_error: 0.1305 - val_loss: 0.4349 - val_mean_squared_error: 0.4037\n",
      "Epoch 209/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1313 - val_loss: 0.4274 - val_mean_squared_error: 0.3975\n",
      "Epoch 210/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1576 - mean_squared_error: 0.1276 - val_loss: 0.4800 - val_mean_squared_error: 0.4491\n",
      "Epoch 211/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1584 - mean_squared_error: 0.1271 - val_loss: 0.4250 - val_mean_squared_error: 0.3933\n",
      "Epoch 212/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1313 - val_loss: 0.4090 - val_mean_squared_error: 0.3721\n",
      "Epoch 213/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1643 - mean_squared_error: 0.1305 - val_loss: 0.4347 - val_mean_squared_error: 0.4022\n",
      "Epoch 214/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1713 - mean_squared_error: 0.1371 - val_loss: 0.4564 - val_mean_squared_error: 0.4196\n",
      "Epoch 215/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1670 - mean_squared_error: 0.1313 - val_loss: 0.4586 - val_mean_squared_error: 0.4250\n",
      "Epoch 216/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1300 - val_loss: 0.4529 - val_mean_squared_error: 0.4224\n",
      "Epoch 217/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1334 - val_loss: 0.4458 - val_mean_squared_error: 0.4141\n",
      "Epoch 218/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1646 - mean_squared_error: 0.1325 - val_loss: 0.4742 - val_mean_squared_error: 0.4386\n",
      "Epoch 219/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1311 - val_loss: 0.3827 - val_mean_squared_error: 0.3525\n",
      "Epoch 220/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1672 - mean_squared_error: 0.1349 - val_loss: 0.4791 - val_mean_squared_error: 0.4430\n",
      "Epoch 221/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1336 - val_loss: 0.4197 - val_mean_squared_error: 0.3884\n",
      "Epoch 222/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1619 - mean_squared_error: 0.1291 - val_loss: 0.4281 - val_mean_squared_error: 0.3943\n",
      "Epoch 223/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1658 - mean_squared_error: 0.1323 - val_loss: 0.4045 - val_mean_squared_error: 0.3706\n",
      "Epoch 224/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1314 - val_loss: 0.4002 - val_mean_squared_error: 0.3691\n",
      "Epoch 225/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1634 - mean_squared_error: 0.1313 - val_loss: 0.4574 - val_mean_squared_error: 0.4247\n",
      "Epoch 226/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1579 - mean_squared_error: 0.1270 - val_loss: 0.4483 - val_mean_squared_error: 0.4159\n",
      "Epoch 227/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1688 - mean_squared_error: 0.1341 - val_loss: 0.4508 - val_mean_squared_error: 0.4161\n",
      "Epoch 228/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1292 - val_loss: 0.4283 - val_mean_squared_error: 0.3936\n",
      "Epoch 229/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1645 - mean_squared_error: 0.1302 - val_loss: 0.4220 - val_mean_squared_error: 0.3891\n",
      "Epoch 230/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1268 - val_loss: 0.4283 - val_mean_squared_error: 0.3989\n",
      "Epoch 231/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1595 - mean_squared_error: 0.1291 - val_loss: 0.4134 - val_mean_squared_error: 0.3837\n",
      "Epoch 232/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1577 - mean_squared_error: 0.1261 - val_loss: 0.4226 - val_mean_squared_error: 0.3907\n",
      "Epoch 233/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1323 - val_loss: 0.4346 - val_mean_squared_error: 0.4014\n",
      "Epoch 234/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1615 - mean_squared_error: 0.1300 - val_loss: 0.4238 - val_mean_squared_error: 0.3926\n",
      "Epoch 235/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1280 - val_loss: 0.4232 - val_mean_squared_error: 0.3905\n",
      "Epoch 236/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1596 - mean_squared_error: 0.1289 - val_loss: 0.4663 - val_mean_squared_error: 0.4328\n",
      "Epoch 237/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1299 - val_loss: 0.4457 - val_mean_squared_error: 0.4140\n",
      "Epoch 238/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1285 - val_loss: 0.4859 - val_mean_squared_error: 0.4544\n",
      "Epoch 239/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1342 - val_loss: 0.4267 - val_mean_squared_error: 0.3898\n",
      "Epoch 240/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1293 - val_loss: 0.4064 - val_mean_squared_error: 0.3759\n",
      "Epoch 241/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1592 - mean_squared_error: 0.1284 - val_loss: 0.4785 - val_mean_squared_error: 0.4467\n",
      "Epoch 242/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1320 - val_loss: 0.4163 - val_mean_squared_error: 0.3845\n",
      "Epoch 243/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1305 - val_loss: 0.4394 - val_mean_squared_error: 0.4086\n",
      "Epoch 244/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1567 - mean_squared_error: 0.1264 - val_loss: 0.4529 - val_mean_squared_error: 0.4221\n",
      "Epoch 245/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1320 - val_loss: 0.4607 - val_mean_squared_error: 0.4286\n",
      "Epoch 246/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1629 - mean_squared_error: 0.1303 - val_loss: 0.4071 - val_mean_squared_error: 0.3766\n",
      "Epoch 247/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1632 - mean_squared_error: 0.1317 - val_loss: 0.4721 - val_mean_squared_error: 0.4345\n",
      "Epoch 248/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1657 - mean_squared_error: 0.1315 - val_loss: 0.4271 - val_mean_squared_error: 0.3953\n",
      "Epoch 249/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1579 - mean_squared_error: 0.1270 - val_loss: 0.4231 - val_mean_squared_error: 0.3932\n",
      "Epoch 250/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1617 - mean_squared_error: 0.1292 - val_loss: 0.4454 - val_mean_squared_error: 0.4114\n",
      "Epoch 251/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1674 - mean_squared_error: 0.1315 - val_loss: 0.4388 - val_mean_squared_error: 0.4046\n",
      "Epoch 252/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1744 - mean_squared_error: 0.1377 - val_loss: 0.4519 - val_mean_squared_error: 0.4131\n",
      "Epoch 253/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1696 - mean_squared_error: 0.1345 - val_loss: 0.3886 - val_mean_squared_error: 0.3521\n",
      "Epoch 254/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1616 - mean_squared_error: 0.1283 - val_loss: 0.4297 - val_mean_squared_error: 0.3984\n",
      "Epoch 255/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1595 - mean_squared_error: 0.1288 - val_loss: 0.4585 - val_mean_squared_error: 0.4266\n",
      "Epoch 256/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1305 - val_loss: 0.4383 - val_mean_squared_error: 0.4046\n",
      "Epoch 257/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1603 - mean_squared_error: 0.1274 - val_loss: 0.4853 - val_mean_squared_error: 0.4513\n",
      "Epoch 258/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1712 - mean_squared_error: 0.1345 - val_loss: 0.4479 - val_mean_squared_error: 0.4140\n",
      "Epoch 259/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1620 - mean_squared_error: 0.1292 - val_loss: 0.4400 - val_mean_squared_error: 0.4090\n",
      "Epoch 260/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1624 - mean_squared_error: 0.1301 - val_loss: 0.4144 - val_mean_squared_error: 0.3816\n",
      "Epoch 261/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1613 - mean_squared_error: 0.1289 - val_loss: 0.4709 - val_mean_squared_error: 0.4386\n",
      "Epoch 262/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1301 - val_loss: 0.4673 - val_mean_squared_error: 0.4358\n",
      "Epoch 263/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1688 - mean_squared_error: 0.1349 - val_loss: 0.4371 - val_mean_squared_error: 0.4012\n",
      "Epoch 264/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1271 - val_loss: 0.4456 - val_mean_squared_error: 0.4142\n",
      "Epoch 265/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1623 - mean_squared_error: 0.1304 - val_loss: 0.4402 - val_mean_squared_error: 0.4088\n",
      "Epoch 266/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1616 - mean_squared_error: 0.1304 - val_loss: 0.4380 - val_mean_squared_error: 0.4036\n",
      "Epoch 267/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1691 - mean_squared_error: 0.1346 - val_loss: 0.4364 - val_mean_squared_error: 0.4022\n",
      "Epoch 268/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1605 - mean_squared_error: 0.1284 - val_loss: 0.4698 - val_mean_squared_error: 0.4365\n",
      "Epoch 269/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1584 - mean_squared_error: 0.1266 - val_loss: 0.4700 - val_mean_squared_error: 0.4377\n",
      "Epoch 270/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1286 - val_loss: 0.4860 - val_mean_squared_error: 0.4551\n",
      "Epoch 271/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1660 - mean_squared_error: 0.1330 - val_loss: 0.4708 - val_mean_squared_error: 0.4368\n",
      "Epoch 272/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1612 - mean_squared_error: 0.1278 - val_loss: 0.4678 - val_mean_squared_error: 0.4350\n",
      "Epoch 273/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1656 - mean_squared_error: 0.1319 - val_loss: 0.4932 - val_mean_squared_error: 0.4595\n",
      "Epoch 274/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1623 - mean_squared_error: 0.1300 - val_loss: 0.4805 - val_mean_squared_error: 0.4483\n",
      "Epoch 275/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1609 - mean_squared_error: 0.1294 - val_loss: 0.4223 - val_mean_squared_error: 0.3897\n",
      "Epoch 276/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1724 - mean_squared_error: 0.1370 - val_loss: 0.4518 - val_mean_squared_error: 0.4170\n",
      "Epoch 277/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1606 - mean_squared_error: 0.1272 - val_loss: 0.4631 - val_mean_squared_error: 0.4305\n",
      "Epoch 278/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1661 - mean_squared_error: 0.1311 - val_loss: 0.5005 - val_mean_squared_error: 0.4655\n",
      "Epoch 279/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1686 - mean_squared_error: 0.1335 - val_loss: 0.4996 - val_mean_squared_error: 0.4614\n",
      "Epoch 280/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1606 - mean_squared_error: 0.1273 - val_loss: 0.4290 - val_mean_squared_error: 0.3987\n",
      "Epoch 281/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1611 - mean_squared_error: 0.1299 - val_loss: 0.4887 - val_mean_squared_error: 0.4555\n",
      "Epoch 282/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1713 - mean_squared_error: 0.1338 - val_loss: 0.4072 - val_mean_squared_error: 0.3754\n",
      "Epoch 283/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1319 - val_loss: 0.4380 - val_mean_squared_error: 0.4055\n",
      "Epoch 284/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1314 - val_loss: 0.4438 - val_mean_squared_error: 0.4126\n",
      "Epoch 285/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1589 - mean_squared_error: 0.1272 - val_loss: 0.4583 - val_mean_squared_error: 0.4262\n",
      "Epoch 286/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1602 - mean_squared_error: 0.1288 - val_loss: 0.4262 - val_mean_squared_error: 0.3937\n",
      "Epoch 287/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1275 - val_loss: 0.4536 - val_mean_squared_error: 0.4197\n",
      "Epoch 288/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1593 - mean_squared_error: 0.1265 - val_loss: 0.4871 - val_mean_squared_error: 0.4552\n",
      "Epoch 289/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1618 - mean_squared_error: 0.1293 - val_loss: 0.4439 - val_mean_squared_error: 0.4135\n",
      "Epoch 290/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1603 - mean_squared_error: 0.1283 - val_loss: 0.4492 - val_mean_squared_error: 0.4181\n",
      "Epoch 291/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1311 - val_loss: 0.4164 - val_mean_squared_error: 0.3848\n",
      "Epoch 292/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1577 - mean_squared_error: 0.1262 - val_loss: 0.4618 - val_mean_squared_error: 0.4302\n",
      "Epoch 293/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1618 - mean_squared_error: 0.1295 - val_loss: 0.4390 - val_mean_squared_error: 0.4044\n",
      "Epoch 294/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1584 - mean_squared_error: 0.1252 - val_loss: 0.4790 - val_mean_squared_error: 0.4453\n",
      "Epoch 295/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1651 - mean_squared_error: 0.1313 - val_loss: 0.4612 - val_mean_squared_error: 0.4252\n",
      "Epoch 296/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1664 - mean_squared_error: 0.1305 - val_loss: 0.4651 - val_mean_squared_error: 0.4314\n",
      "Epoch 297/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1648 - mean_squared_error: 0.1313 - val_loss: 0.4390 - val_mean_squared_error: 0.4032\n",
      "Epoch 298/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1708 - mean_squared_error: 0.1340 - val_loss: 0.4388 - val_mean_squared_error: 0.4074\n",
      "Epoch 299/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1569 - mean_squared_error: 0.1260 - val_loss: 0.4596 - val_mean_squared_error: 0.4301\n",
      "Epoch 300/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1582 - mean_squared_error: 0.1279 - val_loss: 0.4592 - val_mean_squared_error: 0.4263\n",
      "Epoch 301/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1639 - mean_squared_error: 0.1303 - val_loss: 0.4399 - val_mean_squared_error: 0.4081\n",
      "Epoch 302/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1304 - val_loss: 0.4598 - val_mean_squared_error: 0.4286\n",
      "Epoch 303/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1575 - mean_squared_error: 0.1265 - val_loss: 0.4621 - val_mean_squared_error: 0.4322\n",
      "Epoch 304/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1601 - mean_squared_error: 0.1287 - val_loss: 0.5707 - val_mean_squared_error: 0.5366\n",
      "Epoch 305/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1638 - mean_squared_error: 0.1285 - val_loss: 0.4780 - val_mean_squared_error: 0.4427\n",
      "Epoch 306/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1605 - mean_squared_error: 0.1267 - val_loss: 0.4502 - val_mean_squared_error: 0.4177\n",
      "Epoch 307/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1627 - mean_squared_error: 0.1295 - val_loss: 0.4040 - val_mean_squared_error: 0.3723\n",
      "Epoch 308/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1622 - mean_squared_error: 0.1294 - val_loss: 0.4489 - val_mean_squared_error: 0.4173\n",
      "Epoch 309/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1617 - mean_squared_error: 0.1289 - val_loss: 0.5000 - val_mean_squared_error: 0.4651\n",
      "Epoch 310/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1280 - val_loss: 0.4274 - val_mean_squared_error: 0.3957\n",
      "Epoch 311/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1278 - val_loss: 0.4123 - val_mean_squared_error: 0.3804\n",
      "Epoch 312/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1281 - val_loss: 0.4004 - val_mean_squared_error: 0.3683\n",
      "Epoch 313/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1623 - mean_squared_error: 0.1292 - val_loss: 0.4066 - val_mean_squared_error: 0.3741\n",
      "Epoch 314/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1650 - mean_squared_error: 0.1319 - val_loss: 0.4285 - val_mean_squared_error: 0.3954\n",
      "Epoch 315/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1605 - mean_squared_error: 0.1275 - val_loss: 0.4642 - val_mean_squared_error: 0.4305\n",
      "Epoch 316/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1582 - mean_squared_error: 0.1255 - val_loss: 0.4607 - val_mean_squared_error: 0.4290\n",
      "Epoch 317/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1585 - mean_squared_error: 0.1266 - val_loss: 0.4686 - val_mean_squared_error: 0.4365\n",
      "Epoch 318/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1635 - mean_squared_error: 0.1300 - val_loss: 0.4338 - val_mean_squared_error: 0.4022\n",
      "Epoch 319/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1270 - val_loss: 0.4441 - val_mean_squared_error: 0.4090\n",
      "Epoch 320/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1637 - mean_squared_error: 0.1290 - val_loss: 0.4674 - val_mean_squared_error: 0.4354\n",
      "Epoch 321/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1581 - mean_squared_error: 0.1258 - val_loss: 0.4066 - val_mean_squared_error: 0.3760\n",
      "Epoch 322/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1700 - mean_squared_error: 0.1355 - val_loss: 0.4252 - val_mean_squared_error: 0.3887\n",
      "Epoch 323/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1608 - mean_squared_error: 0.1276 - val_loss: 0.4295 - val_mean_squared_error: 0.3938\n",
      "Epoch 324/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1667 - mean_squared_error: 0.1329 - val_loss: 0.4731 - val_mean_squared_error: 0.4421\n",
      "Epoch 325/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1597 - mean_squared_error: 0.1286 - val_loss: 0.4571 - val_mean_squared_error: 0.4257\n",
      "Epoch 326/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1610 - mean_squared_error: 0.1289 - val_loss: 0.4501 - val_mean_squared_error: 0.4184\n",
      "Epoch 327/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1589 - mean_squared_error: 0.1269 - val_loss: 0.4233 - val_mean_squared_error: 0.3889\n",
      "Epoch 328/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1602 - mean_squared_error: 0.1283 - val_loss: 0.3859 - val_mean_squared_error: 0.3530\n",
      "Epoch 329/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1573 - mean_squared_error: 0.1252 - val_loss: 0.4216 - val_mean_squared_error: 0.3894\n",
      "Epoch 330/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1687 - mean_squared_error: 0.1327 - val_loss: 0.4584 - val_mean_squared_error: 0.4223\n",
      "Epoch 331/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1593 - mean_squared_error: 0.1258 - val_loss: 0.4616 - val_mean_squared_error: 0.4298\n",
      "Epoch 332/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1299 - val_loss: 0.4577 - val_mean_squared_error: 0.4236\n",
      "Epoch 333/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1637 - mean_squared_error: 0.1301 - val_loss: 0.4413 - val_mean_squared_error: 0.4089\n",
      "Epoch 334/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1613 - mean_squared_error: 0.1284 - val_loss: 0.4628 - val_mean_squared_error: 0.4295\n",
      "Epoch 335/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1299 - val_loss: 0.4339 - val_mean_squared_error: 0.4023\n",
      "Epoch 336/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1625 - mean_squared_error: 0.1295 - val_loss: 0.4304 - val_mean_squared_error: 0.3981\n",
      "Epoch 337/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1613 - mean_squared_error: 0.1284 - val_loss: 0.4401 - val_mean_squared_error: 0.4065\n",
      "Epoch 338/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1690 - mean_squared_error: 0.1350 - val_loss: 0.4647 - val_mean_squared_error: 0.4292\n",
      "Epoch 339/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1296 - val_loss: 0.4766 - val_mean_squared_error: 0.4413\n",
      "Epoch 340/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1261 - val_loss: 0.4604 - val_mean_squared_error: 0.4281\n",
      "Epoch 341/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1573 - mean_squared_error: 0.1257 - val_loss: 0.4463 - val_mean_squared_error: 0.4154\n",
      "Epoch 342/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1567 - mean_squared_error: 0.1255 - val_loss: 0.4299 - val_mean_squared_error: 0.3972\n",
      "Epoch 343/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1642 - mean_squared_error: 0.1308 - val_loss: 0.4500 - val_mean_squared_error: 0.4147\n",
      "Epoch 344/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1702 - mean_squared_error: 0.1343 - val_loss: 0.4557 - val_mean_squared_error: 0.4240\n",
      "Epoch 345/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1616 - mean_squared_error: 0.1290 - val_loss: 0.4209 - val_mean_squared_error: 0.3887\n",
      "Epoch 346/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1609 - mean_squared_error: 0.1281 - val_loss: 0.4023 - val_mean_squared_error: 0.3690\n",
      "Epoch 347/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1603 - mean_squared_error: 0.1269 - val_loss: 0.4129 - val_mean_squared_error: 0.3815\n",
      "Epoch 348/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1670 - mean_squared_error: 0.1312 - val_loss: 0.4952 - val_mean_squared_error: 0.4588\n",
      "Epoch 349/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1645 - mean_squared_error: 0.1306 - val_loss: 0.4219 - val_mean_squared_error: 0.3893\n",
      "Epoch 350/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1587 - mean_squared_error: 0.1268 - val_loss: 0.4409 - val_mean_squared_error: 0.4088\n",
      "Epoch 351/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1570 - mean_squared_error: 0.1262 - val_loss: 0.4255 - val_mean_squared_error: 0.3956\n",
      "Epoch 352/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1551 - mean_squared_error: 0.1245 - val_loss: 0.4856 - val_mean_squared_error: 0.4538\n",
      "Epoch 353/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1560 - mean_squared_error: 0.1244 - val_loss: 0.5243 - val_mean_squared_error: 0.4923\n",
      "Epoch 354/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1577 - mean_squared_error: 0.1259 - val_loss: 0.4242 - val_mean_squared_error: 0.3920\n",
      "Epoch 355/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1604 - mean_squared_error: 0.1277 - val_loss: 0.4368 - val_mean_squared_error: 0.4037\n",
      "Epoch 356/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1675 - mean_squared_error: 0.1326 - val_loss: 0.4502 - val_mean_squared_error: 0.4124\n",
      "Epoch 357/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1568 - mean_squared_error: 0.1239 - val_loss: 0.4395 - val_mean_squared_error: 0.4101\n",
      "Epoch 358/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1590 - mean_squared_error: 0.1278 - val_loss: 0.4419 - val_mean_squared_error: 0.4096\n",
      "Epoch 359/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1651 - mean_squared_error: 0.1316 - val_loss: 0.4798 - val_mean_squared_error: 0.4424\n",
      "Epoch 360/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1684 - mean_squared_error: 0.1319 - val_loss: 0.4032 - val_mean_squared_error: 0.3680\n",
      "Epoch 361/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1645 - mean_squared_error: 0.1291 - val_loss: 0.4433 - val_mean_squared_error: 0.4125\n",
      "Epoch 362/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1571 - mean_squared_error: 0.1255 - val_loss: 0.4480 - val_mean_squared_error: 0.4156\n",
      "Epoch 363/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1595 - mean_squared_error: 0.1260 - val_loss: 0.4532 - val_mean_squared_error: 0.4197\n",
      "Epoch 364/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1569 - mean_squared_error: 0.1253 - val_loss: 0.4756 - val_mean_squared_error: 0.4446\n",
      "Epoch 365/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1582 - mean_squared_error: 0.1266 - val_loss: 0.4763 - val_mean_squared_error: 0.4445\n",
      "Epoch 366/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1615 - mean_squared_error: 0.1297 - val_loss: 0.4416 - val_mean_squared_error: 0.4079\n",
      "Epoch 367/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1637 - mean_squared_error: 0.1301 - val_loss: 0.4271 - val_mean_squared_error: 0.3946\n",
      "Epoch 368/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1615 - mean_squared_error: 0.1289 - val_loss: 0.4924 - val_mean_squared_error: 0.4578\n",
      "Epoch 369/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1654 - mean_squared_error: 0.1314 - val_loss: 0.4420 - val_mean_squared_error: 0.4082\n",
      "Epoch 370/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1539 - mean_squared_error: 0.1228 - val_loss: 0.4489 - val_mean_squared_error: 0.4191\n",
      "Epoch 371/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1670 - mean_squared_error: 0.1325 - val_loss: 0.4501 - val_mean_squared_error: 0.4153\n",
      "Epoch 372/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1567 - mean_squared_error: 0.1240 - val_loss: 0.4277 - val_mean_squared_error: 0.3978\n",
      "Epoch 373/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1597 - mean_squared_error: 0.1285 - val_loss: 0.4569 - val_mean_squared_error: 0.4243\n",
      "Epoch 374/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1622 - mean_squared_error: 0.1291 - val_loss: 0.4496 - val_mean_squared_error: 0.4117\n",
      "Epoch 375/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1674 - mean_squared_error: 0.1306 - val_loss: 0.4063 - val_mean_squared_error: 0.3733\n",
      "Epoch 376/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1618 - mean_squared_error: 0.1285 - val_loss: 0.4737 - val_mean_squared_error: 0.4412\n",
      "Epoch 377/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1630 - mean_squared_error: 0.1297 - val_loss: 0.4613 - val_mean_squared_error: 0.4281\n",
      "Epoch 378/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1600 - mean_squared_error: 0.1261 - val_loss: 0.4448 - val_mean_squared_error: 0.4096\n",
      "Epoch 379/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1664 - mean_squared_error: 0.1315 - val_loss: 0.4642 - val_mean_squared_error: 0.4295\n",
      "Epoch 380/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1585 - mean_squared_error: 0.1268 - val_loss: 0.4729 - val_mean_squared_error: 0.4386\n",
      "Epoch 381/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1637 - mean_squared_error: 0.1285 - val_loss: 0.4494 - val_mean_squared_error: 0.4172\n",
      "Epoch 382/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1596 - mean_squared_error: 0.1277 - val_loss: 0.4710 - val_mean_squared_error: 0.4392\n",
      "Epoch 383/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1564 - mean_squared_error: 0.1249 - val_loss: 0.4257 - val_mean_squared_error: 0.3954\n",
      "Epoch 384/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1586 - mean_squared_error: 0.1263 - val_loss: 0.4151 - val_mean_squared_error: 0.3824\n",
      "Epoch 385/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1564 - mean_squared_error: 0.1244 - val_loss: 0.4694 - val_mean_squared_error: 0.4389\n",
      "Epoch 386/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1570 - mean_squared_error: 0.1260 - val_loss: 0.4709 - val_mean_squared_error: 0.4367\n",
      "Epoch 387/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1647 - mean_squared_error: 0.1303 - val_loss: 0.4312 - val_mean_squared_error: 0.3976\n",
      "Epoch 388/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1637 - mean_squared_error: 0.1287 - val_loss: 0.4316 - val_mean_squared_error: 0.3976\n",
      "Epoch 389/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1649 - mean_squared_error: 0.1316 - val_loss: 0.4213 - val_mean_squared_error: 0.3884\n",
      "Epoch 390/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1626 - mean_squared_error: 0.1299 - val_loss: 0.4648 - val_mean_squared_error: 0.4301\n",
      "Epoch 391/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1551 - mean_squared_error: 0.1223 - val_loss: 0.4380 - val_mean_squared_error: 0.4071\n",
      "Epoch 392/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1567 - mean_squared_error: 0.1248 - val_loss: 0.4072 - val_mean_squared_error: 0.3747\n",
      "Epoch 393/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1598 - mean_squared_error: 0.1261 - val_loss: 0.4210 - val_mean_squared_error: 0.3882\n",
      "Epoch 394/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1616 - mean_squared_error: 0.1283 - val_loss: 0.4284 - val_mean_squared_error: 0.3956\n",
      "Epoch 395/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1580 - mean_squared_error: 0.1254 - val_loss: 0.4697 - val_mean_squared_error: 0.4363\n",
      "Epoch 396/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1641 - mean_squared_error: 0.1299 - val_loss: 0.4628 - val_mean_squared_error: 0.4320\n",
      "Epoch 397/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1290 - val_loss: 0.4602 - val_mean_squared_error: 0.4269\n",
      "Epoch 398/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1618 - mean_squared_error: 0.1283 - val_loss: 0.4705 - val_mean_squared_error: 0.4374\n",
      "Epoch 399/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1658 - mean_squared_error: 0.1320 - val_loss: 0.4244 - val_mean_squared_error: 0.3923\n",
      "Epoch 400/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1631 - mean_squared_error: 0.1303 - val_loss: 0.4844 - val_mean_squared_error: 0.4523\n",
      "Epoch 401/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.1587 - mean_squared_error: 0.1260 - val_loss: 0.4303 - val_mean_squared_error: 0.3988\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1573 - mean_squared_error: 0.1258\n",
      "Epoch 1/10000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2462 - mean_squared_error: 0.2084 - val_loss: 0.4529 - val_mean_squared_error: 0.4143\n",
      "Epoch 2/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2410 - mean_squared_error: 0.2027 - val_loss: 0.3920 - val_mean_squared_error: 0.3538\n",
      "Epoch 3/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2051 - val_loss: 0.3967 - val_mean_squared_error: 0.3587\n",
      "Epoch 4/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2040 - val_loss: 0.4350 - val_mean_squared_error: 0.3966\n",
      "Epoch 5/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2027 - val_loss: 0.4422 - val_mean_squared_error: 0.4036\n",
      "Epoch 6/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2469 - mean_squared_error: 0.2082 - val_loss: 0.4005 - val_mean_squared_error: 0.3620\n",
      "Epoch 7/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2396 - mean_squared_error: 0.2016 - val_loss: 0.3919 - val_mean_squared_error: 0.3541\n",
      "Epoch 8/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2041 - val_loss: 0.4300 - val_mean_squared_error: 0.3920\n",
      "Epoch 9/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2452 - mean_squared_error: 0.2073 - val_loss: 0.3932 - val_mean_squared_error: 0.3553\n",
      "Epoch 10/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2407 - mean_squared_error: 0.2034 - val_loss: 0.4300 - val_mean_squared_error: 0.3923\n",
      "Epoch 11/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2048 - val_loss: 0.4403 - val_mean_squared_error: 0.4021\n",
      "Epoch 12/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2448 - mean_squared_error: 0.2071 - val_loss: 0.4030 - val_mean_squared_error: 0.3648\n",
      "Epoch 13/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2450 - mean_squared_error: 0.2065 - val_loss: 0.4369 - val_mean_squared_error: 0.3972\n",
      "Epoch 14/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2475 - mean_squared_error: 0.2080 - val_loss: 0.4137 - val_mean_squared_error: 0.3743\n",
      "Epoch 15/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2007 - val_loss: 0.4030 - val_mean_squared_error: 0.3645\n",
      "Epoch 16/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2450 - mean_squared_error: 0.2066 - val_loss: 0.3971 - val_mean_squared_error: 0.3597\n",
      "Epoch 17/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2474 - mean_squared_error: 0.2095 - val_loss: 0.4183 - val_mean_squared_error: 0.3800\n",
      "Epoch 18/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2022 - val_loss: 0.4134 - val_mean_squared_error: 0.3758\n",
      "Epoch 19/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2420 - mean_squared_error: 0.2038 - val_loss: 0.4559 - val_mean_squared_error: 0.4167\n",
      "Epoch 20/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2035 - val_loss: 0.4203 - val_mean_squared_error: 0.3805\n",
      "Epoch 21/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2425 - mean_squared_error: 0.2030 - val_loss: 0.4443 - val_mean_squared_error: 0.4057\n",
      "Epoch 22/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2056 - val_loss: 0.4081 - val_mean_squared_error: 0.3684\n",
      "Epoch 23/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2467 - mean_squared_error: 0.2074 - val_loss: 0.4225 - val_mean_squared_error: 0.3836\n",
      "Epoch 24/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2444 - mean_squared_error: 0.2055 - val_loss: 0.4326 - val_mean_squared_error: 0.3943\n",
      "Epoch 25/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2407 - mean_squared_error: 0.2028 - val_loss: 0.4276 - val_mean_squared_error: 0.3899\n",
      "Epoch 26/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2028 - val_loss: 0.4268 - val_mean_squared_error: 0.3891\n",
      "Epoch 27/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2036 - val_loss: 0.4308 - val_mean_squared_error: 0.3927\n",
      "Epoch 28/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2056 - val_loss: 0.5011 - val_mean_squared_error: 0.4622\n",
      "Epoch 29/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2428 - mean_squared_error: 0.2049 - val_loss: 0.4232 - val_mean_squared_error: 0.3850\n",
      "Epoch 30/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2412 - mean_squared_error: 0.2031 - val_loss: 0.4225 - val_mean_squared_error: 0.3846\n",
      "Epoch 31/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2413 - mean_squared_error: 0.2037 - val_loss: 0.3932 - val_mean_squared_error: 0.3557\n",
      "Epoch 32/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2062 - val_loss: 0.4285 - val_mean_squared_error: 0.3907\n",
      "Epoch 33/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2024 - val_loss: 0.4881 - val_mean_squared_error: 0.4498\n",
      "Epoch 34/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2442 - mean_squared_error: 0.2063 - val_loss: 0.4011 - val_mean_squared_error: 0.3629\n",
      "Epoch 35/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2428 - mean_squared_error: 0.2047 - val_loss: 0.3959 - val_mean_squared_error: 0.3581\n",
      "Epoch 36/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2027 - val_loss: 0.4122 - val_mean_squared_error: 0.3742\n",
      "Epoch 37/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2460 - mean_squared_error: 0.2079 - val_loss: 0.3994 - val_mean_squared_error: 0.3613\n",
      "Epoch 38/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2027 - val_loss: 0.4095 - val_mean_squared_error: 0.3716\n",
      "Epoch 39/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2035 - val_loss: 0.4563 - val_mean_squared_error: 0.4179\n",
      "Epoch 40/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2391 - mean_squared_error: 0.2011 - val_loss: 0.4037 - val_mean_squared_error: 0.3654\n",
      "Epoch 41/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1989 - val_loss: 0.4297 - val_mean_squared_error: 0.3920\n",
      "Epoch 42/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2023 - val_loss: 0.4034 - val_mean_squared_error: 0.3656\n",
      "Epoch 43/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2400 - mean_squared_error: 0.2018 - val_loss: 0.4206 - val_mean_squared_error: 0.3813\n",
      "Epoch 44/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2427 - mean_squared_error: 0.2039 - val_loss: 0.4281 - val_mean_squared_error: 0.3900\n",
      "Epoch 45/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2053 - val_loss: 0.4384 - val_mean_squared_error: 0.4000\n",
      "Epoch 46/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2464 - mean_squared_error: 0.2080 - val_loss: 0.4217 - val_mean_squared_error: 0.3833\n",
      "Epoch 47/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2468 - mean_squared_error: 0.2087 - val_loss: 0.4383 - val_mean_squared_error: 0.3996\n",
      "Epoch 48/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2465 - mean_squared_error: 0.2084 - val_loss: 0.4314 - val_mean_squared_error: 0.3932\n",
      "Epoch 49/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2440 - mean_squared_error: 0.2059 - val_loss: 0.4572 - val_mean_squared_error: 0.4187\n",
      "Epoch 50/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2457 - mean_squared_error: 0.2071 - val_loss: 0.4372 - val_mean_squared_error: 0.3990\n",
      "Epoch 51/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2467 - mean_squared_error: 0.2086 - val_loss: 0.3938 - val_mean_squared_error: 0.3554\n",
      "Epoch 52/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2486 - mean_squared_error: 0.2102 - val_loss: 0.4129 - val_mean_squared_error: 0.3751\n",
      "Epoch 53/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2399 - mean_squared_error: 0.2018 - val_loss: 0.4240 - val_mean_squared_error: 0.3854\n",
      "Epoch 54/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2035 - val_loss: 0.3934 - val_mean_squared_error: 0.3551\n",
      "Epoch 55/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2401 - mean_squared_error: 0.2022 - val_loss: 0.4334 - val_mean_squared_error: 0.3957\n",
      "Epoch 56/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.2002 - val_loss: 0.4376 - val_mean_squared_error: 0.3991\n",
      "Epoch 57/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2386 - mean_squared_error: 0.2004 - val_loss: 0.4085 - val_mean_squared_error: 0.3708\n",
      "Epoch 58/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2044 - val_loss: 0.4626 - val_mean_squared_error: 0.4246\n",
      "Epoch 59/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2391 - mean_squared_error: 0.2013 - val_loss: 0.4149 - val_mean_squared_error: 0.3771\n",
      "Epoch 60/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2361 - mean_squared_error: 0.1984 - val_loss: 0.4506 - val_mean_squared_error: 0.4132\n",
      "Epoch 61/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2431 - mean_squared_error: 0.2053 - val_loss: 0.4039 - val_mean_squared_error: 0.3666\n",
      "Epoch 62/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2430 - mean_squared_error: 0.2046 - val_loss: 0.4424 - val_mean_squared_error: 0.4040\n",
      "Epoch 63/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2423 - mean_squared_error: 0.2045 - val_loss: 0.4152 - val_mean_squared_error: 0.3769\n",
      "Epoch 64/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2423 - mean_squared_error: 0.2031 - val_loss: 0.4095 - val_mean_squared_error: 0.3706\n",
      "Epoch 65/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2441 - mean_squared_error: 0.2057 - val_loss: 0.4068 - val_mean_squared_error: 0.3690\n",
      "Epoch 66/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2411 - mean_squared_error: 0.2031 - val_loss: 0.4543 - val_mean_squared_error: 0.4165\n",
      "Epoch 67/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2353 - mean_squared_error: 0.1978 - val_loss: 0.4471 - val_mean_squared_error: 0.4093\n",
      "Epoch 68/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2421 - mean_squared_error: 0.2045 - val_loss: 0.4107 - val_mean_squared_error: 0.3729\n",
      "Epoch 69/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2044 - val_loss: 0.4561 - val_mean_squared_error: 0.4175\n",
      "Epoch 70/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2334 - mean_squared_error: 0.1959 - val_loss: 0.4424 - val_mean_squared_error: 0.4043\n",
      "Epoch 71/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2473 - mean_squared_error: 0.2090 - val_loss: 0.3965 - val_mean_squared_error: 0.3582\n",
      "Epoch 72/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2359 - mean_squared_error: 0.1975 - val_loss: 0.4413 - val_mean_squared_error: 0.4021\n",
      "Epoch 73/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2477 - mean_squared_error: 0.2089 - val_loss: 0.4168 - val_mean_squared_error: 0.3783\n",
      "Epoch 74/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2412 - mean_squared_error: 0.2030 - val_loss: 0.4580 - val_mean_squared_error: 0.4191\n",
      "Epoch 75/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2010 - val_loss: 0.4057 - val_mean_squared_error: 0.3677\n",
      "Epoch 76/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2459 - mean_squared_error: 0.2073 - val_loss: 0.4789 - val_mean_squared_error: 0.4399\n",
      "Epoch 77/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2462 - mean_squared_error: 0.2065 - val_loss: 0.4198 - val_mean_squared_error: 0.3812\n",
      "Epoch 78/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2396 - mean_squared_error: 0.2009 - val_loss: 0.4472 - val_mean_squared_error: 0.4093\n",
      "Epoch 79/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2445 - mean_squared_error: 0.2058 - val_loss: 0.4521 - val_mean_squared_error: 0.4125\n",
      "Epoch 80/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2020 - val_loss: 0.4218 - val_mean_squared_error: 0.3831\n",
      "Epoch 81/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2348 - mean_squared_error: 0.1969 - val_loss: 0.4285 - val_mean_squared_error: 0.3907\n",
      "Epoch 82/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2429 - mean_squared_error: 0.2048 - val_loss: 0.4010 - val_mean_squared_error: 0.3629\n",
      "Epoch 83/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2023 - val_loss: 0.4087 - val_mean_squared_error: 0.3706\n",
      "Epoch 84/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2062 - val_loss: 0.3933 - val_mean_squared_error: 0.3559\n",
      "Epoch 85/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2023 - val_loss: 0.4070 - val_mean_squared_error: 0.3691\n",
      "Epoch 86/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2054 - val_loss: 0.4304 - val_mean_squared_error: 0.3918\n",
      "Epoch 87/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2052 - val_loss: 0.4499 - val_mean_squared_error: 0.4105\n",
      "Epoch 88/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2493 - mean_squared_error: 0.2084 - val_loss: 0.4318 - val_mean_squared_error: 0.3919\n",
      "Epoch 89/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2441 - mean_squared_error: 0.2045 - val_loss: 0.4227 - val_mean_squared_error: 0.3836\n",
      "Epoch 90/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2446 - mean_squared_error: 0.2064 - val_loss: 0.4438 - val_mean_squared_error: 0.4057\n",
      "Epoch 91/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2478 - mean_squared_error: 0.2096 - val_loss: 0.4301 - val_mean_squared_error: 0.3922\n",
      "Epoch 92/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2058 - val_loss: 0.4608 - val_mean_squared_error: 0.4219\n",
      "Epoch 93/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2030 - val_loss: 0.4712 - val_mean_squared_error: 0.4326\n",
      "Epoch 94/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2466 - mean_squared_error: 0.2077 - val_loss: 0.4138 - val_mean_squared_error: 0.3756\n",
      "Epoch 95/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2405 - mean_squared_error: 0.2022 - val_loss: 0.3760 - val_mean_squared_error: 0.3380\n",
      "Epoch 96/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2451 - mean_squared_error: 0.2066 - val_loss: 0.4132 - val_mean_squared_error: 0.3749\n",
      "Epoch 97/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2053 - val_loss: 0.4678 - val_mean_squared_error: 0.4290\n",
      "Epoch 98/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2452 - mean_squared_error: 0.2062 - val_loss: 0.4258 - val_mean_squared_error: 0.3867\n",
      "Epoch 99/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2017 - val_loss: 0.4120 - val_mean_squared_error: 0.3738\n",
      "Epoch 100/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2024 - val_loss: 0.4216 - val_mean_squared_error: 0.3831\n",
      "Epoch 101/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2402 - mean_squared_error: 0.2017 - val_loss: 0.3975 - val_mean_squared_error: 0.3585\n",
      "Epoch 102/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2407 - mean_squared_error: 0.2024 - val_loss: 0.4331 - val_mean_squared_error: 0.3952\n",
      "Epoch 103/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2037 - val_loss: 0.4391 - val_mean_squared_error: 0.4010\n",
      "Epoch 104/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2050 - val_loss: 0.4570 - val_mean_squared_error: 0.4189\n",
      "Epoch 105/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2053 - val_loss: 0.4248 - val_mean_squared_error: 0.3866\n",
      "Epoch 106/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2441 - mean_squared_error: 0.2061 - val_loss: 0.4100 - val_mean_squared_error: 0.3722\n",
      "Epoch 107/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2337 - mean_squared_error: 0.1963 - val_loss: 0.4092 - val_mean_squared_error: 0.3718\n",
      "Epoch 108/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2425 - mean_squared_error: 0.2039 - val_loss: 0.4037 - val_mean_squared_error: 0.3636\n",
      "Epoch 109/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2428 - mean_squared_error: 0.2040 - val_loss: 0.4464 - val_mean_squared_error: 0.4085\n",
      "Epoch 110/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2447 - mean_squared_error: 0.2065 - val_loss: 0.4334 - val_mean_squared_error: 0.3947\n",
      "Epoch 111/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1978 - val_loss: 0.3929 - val_mean_squared_error: 0.3552\n",
      "Epoch 112/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2414 - mean_squared_error: 0.2035 - val_loss: 0.4201 - val_mean_squared_error: 0.3824\n",
      "Epoch 113/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.2015 - val_loss: 0.4249 - val_mean_squared_error: 0.3877\n",
      "Epoch 114/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2026 - val_loss: 0.4422 - val_mean_squared_error: 0.4045\n",
      "Epoch 115/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2397 - mean_squared_error: 0.2015 - val_loss: 0.4235 - val_mean_squared_error: 0.3856\n",
      "Epoch 116/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2377 - mean_squared_error: 0.2002 - val_loss: 0.4350 - val_mean_squared_error: 0.3975\n",
      "Epoch 117/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2422 - mean_squared_error: 0.2042 - val_loss: 0.4529 - val_mean_squared_error: 0.4150\n",
      "Epoch 118/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2432 - mean_squared_error: 0.2055 - val_loss: 0.4642 - val_mean_squared_error: 0.4267\n",
      "Epoch 119/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2038 - val_loss: 0.4154 - val_mean_squared_error: 0.3776\n",
      "Epoch 120/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2329 - mean_squared_error: 0.1948 - val_loss: 0.4174 - val_mean_squared_error: 0.3799\n",
      "Epoch 121/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2452 - mean_squared_error: 0.2064 - val_loss: 0.3988 - val_mean_squared_error: 0.3590\n",
      "Epoch 122/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2354 - mean_squared_error: 0.1962 - val_loss: 0.4325 - val_mean_squared_error: 0.3938\n",
      "Epoch 123/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2343 - mean_squared_error: 0.1965 - val_loss: 0.3935 - val_mean_squared_error: 0.3561\n",
      "Epoch 124/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2472 - mean_squared_error: 0.2090 - val_loss: 0.3878 - val_mean_squared_error: 0.3499\n",
      "Epoch 125/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2441 - mean_squared_error: 0.2057 - val_loss: 0.4161 - val_mean_squared_error: 0.3776\n",
      "Epoch 126/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2048 - val_loss: 0.4148 - val_mean_squared_error: 0.3764\n",
      "Epoch 127/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2380 - mean_squared_error: 0.1991 - val_loss: 0.4223 - val_mean_squared_error: 0.3831\n",
      "Epoch 128/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2454 - mean_squared_error: 0.2068 - val_loss: 0.4271 - val_mean_squared_error: 0.3883\n",
      "Epoch 129/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2429 - mean_squared_error: 0.2044 - val_loss: 0.4185 - val_mean_squared_error: 0.3806\n",
      "Epoch 130/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2441 - mean_squared_error: 0.2057 - val_loss: 0.3996 - val_mean_squared_error: 0.3612\n",
      "Epoch 131/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2412 - mean_squared_error: 0.2033 - val_loss: 0.4570 - val_mean_squared_error: 0.4194\n",
      "Epoch 132/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2368 - mean_squared_error: 0.1989 - val_loss: 0.3836 - val_mean_squared_error: 0.3457\n",
      "Epoch 133/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2483 - mean_squared_error: 0.2100 - val_loss: 0.4074 - val_mean_squared_error: 0.3686\n",
      "Epoch 134/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2019 - val_loss: 0.3988 - val_mean_squared_error: 0.3609\n",
      "Epoch 135/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2462 - mean_squared_error: 0.2068 - val_loss: 0.4261 - val_mean_squared_error: 0.3875\n",
      "Epoch 136/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2386 - mean_squared_error: 0.2000 - val_loss: 0.3919 - val_mean_squared_error: 0.3540\n",
      "Epoch 137/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2397 - mean_squared_error: 0.2016 - val_loss: 0.4155 - val_mean_squared_error: 0.3775\n",
      "Epoch 138/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2020 - val_loss: 0.5200 - val_mean_squared_error: 0.4805\n",
      "Epoch 139/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1977 - val_loss: 0.4247 - val_mean_squared_error: 0.3866\n",
      "Epoch 140/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2050 - val_loss: 0.4012 - val_mean_squared_error: 0.3629\n",
      "Epoch 141/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2352 - mean_squared_error: 0.1966 - val_loss: 0.3933 - val_mean_squared_error: 0.3547\n",
      "Epoch 142/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2336 - mean_squared_error: 0.1953 - val_loss: 0.4149 - val_mean_squared_error: 0.3767\n",
      "Epoch 143/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2381 - mean_squared_error: 0.1999 - val_loss: 0.4262 - val_mean_squared_error: 0.3879\n",
      "Epoch 144/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2051 - val_loss: 0.3966 - val_mean_squared_error: 0.3577\n",
      "Epoch 145/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.2002 - val_loss: 0.3959 - val_mean_squared_error: 0.3567\n",
      "Epoch 146/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2024 - val_loss: 0.4587 - val_mean_squared_error: 0.4200\n",
      "Epoch 147/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2358 - mean_squared_error: 0.1978 - val_loss: 0.4084 - val_mean_squared_error: 0.3703\n",
      "Epoch 148/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2371 - mean_squared_error: 0.1990 - val_loss: 0.4447 - val_mean_squared_error: 0.4064\n",
      "Epoch 149/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1985 - val_loss: 0.4225 - val_mean_squared_error: 0.3843\n",
      "Epoch 150/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2456 - mean_squared_error: 0.2066 - val_loss: 0.4686 - val_mean_squared_error: 0.4293\n",
      "Epoch 151/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2401 - mean_squared_error: 0.2015 - val_loss: 0.4132 - val_mean_squared_error: 0.3745\n",
      "Epoch 152/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2051 - val_loss: 0.4171 - val_mean_squared_error: 0.3787\n",
      "Epoch 153/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2019 - val_loss: 0.4049 - val_mean_squared_error: 0.3670\n",
      "Epoch 154/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2412 - mean_squared_error: 0.2029 - val_loss: 0.4156 - val_mean_squared_error: 0.3773\n",
      "Epoch 155/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2414 - mean_squared_error: 0.2033 - val_loss: 0.4714 - val_mean_squared_error: 0.4332\n",
      "Epoch 156/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2385 - mean_squared_error: 0.2006 - val_loss: 0.5065 - val_mean_squared_error: 0.4683\n",
      "Epoch 157/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2449 - mean_squared_error: 0.2067 - val_loss: 0.3911 - val_mean_squared_error: 0.3523\n",
      "Epoch 158/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2444 - mean_squared_error: 0.2056 - val_loss: 0.4096 - val_mean_squared_error: 0.3706\n",
      "Epoch 159/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2450 - mean_squared_error: 0.2061 - val_loss: 0.3911 - val_mean_squared_error: 0.3527\n",
      "Epoch 160/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2363 - mean_squared_error: 0.1979 - val_loss: 0.3945 - val_mean_squared_error: 0.3566\n",
      "Epoch 161/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2394 - mean_squared_error: 0.2009 - val_loss: 0.4185 - val_mean_squared_error: 0.3803\n",
      "Epoch 162/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2379 - mean_squared_error: 0.1995 - val_loss: 0.3970 - val_mean_squared_error: 0.3588\n",
      "Epoch 163/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2340 - mean_squared_error: 0.1949 - val_loss: 0.4349 - val_mean_squared_error: 0.3953\n",
      "Epoch 164/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2334 - mean_squared_error: 0.1947 - val_loss: 0.4326 - val_mean_squared_error: 0.3933\n",
      "Epoch 165/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2386 - mean_squared_error: 0.1993 - val_loss: 0.4351 - val_mean_squared_error: 0.3965\n",
      "Epoch 166/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2420 - mean_squared_error: 0.2033 - val_loss: 0.4482 - val_mean_squared_error: 0.4098\n",
      "Epoch 167/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2332 - mean_squared_error: 0.1953 - val_loss: 0.4174 - val_mean_squared_error: 0.3800\n",
      "Epoch 168/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1989 - val_loss: 0.4111 - val_mean_squared_error: 0.3734\n",
      "Epoch 169/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2447 - mean_squared_error: 0.2068 - val_loss: 0.3993 - val_mean_squared_error: 0.3622\n",
      "Epoch 170/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1987 - val_loss: 0.4159 - val_mean_squared_error: 0.3775\n",
      "Epoch 171/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2388 - mean_squared_error: 0.2012 - val_loss: 0.4205 - val_mean_squared_error: 0.3835\n",
      "Epoch 172/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2036 - val_loss: 0.4480 - val_mean_squared_error: 0.4093\n",
      "Epoch 173/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1984 - val_loss: 0.4010 - val_mean_squared_error: 0.3632\n",
      "Epoch 174/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2374 - mean_squared_error: 0.1994 - val_loss: 0.4022 - val_mean_squared_error: 0.3645\n",
      "Epoch 175/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2420 - mean_squared_error: 0.2039 - val_loss: 0.4035 - val_mean_squared_error: 0.3651\n",
      "Epoch 176/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2458 - mean_squared_error: 0.2071 - val_loss: 0.4384 - val_mean_squared_error: 0.3997\n",
      "Epoch 177/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2385 - mean_squared_error: 0.2003 - val_loss: 0.4236 - val_mean_squared_error: 0.3850\n",
      "Epoch 178/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1987 - val_loss: 0.4347 - val_mean_squared_error: 0.3969\n",
      "Epoch 179/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2400 - mean_squared_error: 0.2014 - val_loss: 0.4405 - val_mean_squared_error: 0.4020\n",
      "Epoch 180/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2375 - mean_squared_error: 0.1984 - val_loss: 0.3928 - val_mean_squared_error: 0.3539\n",
      "Epoch 181/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2456 - mean_squared_error: 0.2064 - val_loss: 0.3982 - val_mean_squared_error: 0.3594\n",
      "Epoch 182/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1985 - val_loss: 0.4307 - val_mean_squared_error: 0.3921\n",
      "Epoch 183/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2398 - mean_squared_error: 0.2011 - val_loss: 0.4311 - val_mean_squared_error: 0.3926\n",
      "Epoch 184/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2414 - mean_squared_error: 0.2028 - val_loss: 0.4164 - val_mean_squared_error: 0.3781\n",
      "Epoch 185/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2382 - mean_squared_error: 0.1990 - val_loss: 0.4397 - val_mean_squared_error: 0.4000\n",
      "Epoch 186/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2395 - mean_squared_error: 0.2004 - val_loss: 0.4525 - val_mean_squared_error: 0.4138\n",
      "Epoch 187/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2413 - mean_squared_error: 0.2025 - val_loss: 0.4693 - val_mean_squared_error: 0.4302\n",
      "Epoch 188/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2373 - mean_squared_error: 0.1986 - val_loss: 0.4385 - val_mean_squared_error: 0.4003\n",
      "Epoch 189/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2384 - mean_squared_error: 0.2000 - val_loss: 0.4190 - val_mean_squared_error: 0.3802\n",
      "Epoch 190/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2344 - mean_squared_error: 0.1957 - val_loss: 0.4098 - val_mean_squared_error: 0.3718\n",
      "Epoch 191/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2023 - val_loss: 0.4033 - val_mean_squared_error: 0.3652\n",
      "Epoch 192/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2425 - mean_squared_error: 0.2040 - val_loss: 0.4243 - val_mean_squared_error: 0.3850\n",
      "Epoch 193/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2049 - val_loss: 0.4220 - val_mean_squared_error: 0.3837\n",
      "Epoch 194/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2461 - mean_squared_error: 0.2068 - val_loss: 0.4456 - val_mean_squared_error: 0.4067\n",
      "Epoch 195/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2348 - mean_squared_error: 0.1962 - val_loss: 0.4085 - val_mean_squared_error: 0.3702\n",
      "Epoch 196/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2371 - mean_squared_error: 0.1989 - val_loss: 0.4608 - val_mean_squared_error: 0.4225\n",
      "Epoch 197/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2385 - mean_squared_error: 0.1999 - val_loss: 0.4741 - val_mean_squared_error: 0.4354\n",
      "Epoch 198/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1990 - val_loss: 0.4462 - val_mean_squared_error: 0.4079\n",
      "Epoch 199/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2348 - mean_squared_error: 0.1964 - val_loss: 0.4488 - val_mean_squared_error: 0.4107\n",
      "Epoch 200/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2411 - mean_squared_error: 0.2018 - val_loss: 0.4578 - val_mean_squared_error: 0.4183\n",
      "Epoch 201/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2010 - val_loss: 0.3972 - val_mean_squared_error: 0.3584\n",
      "Epoch 202/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.1991 - val_loss: 0.4155 - val_mean_squared_error: 0.3766\n",
      "Epoch 203/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2385 - mean_squared_error: 0.1994 - val_loss: 0.4263 - val_mean_squared_error: 0.3880\n",
      "Epoch 204/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2373 - mean_squared_error: 0.1993 - val_loss: 0.4463 - val_mean_squared_error: 0.4085\n",
      "Epoch 205/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.2002 - val_loss: 0.4697 - val_mean_squared_error: 0.4310\n",
      "Epoch 206/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2382 - mean_squared_error: 0.1994 - val_loss: 0.4021 - val_mean_squared_error: 0.3638\n",
      "Epoch 207/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2031 - val_loss: 0.4412 - val_mean_squared_error: 0.4024\n",
      "Epoch 208/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2456 - mean_squared_error: 0.2071 - val_loss: 0.4074 - val_mean_squared_error: 0.3687\n",
      "Epoch 209/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2380 - mean_squared_error: 0.1994 - val_loss: 0.4185 - val_mean_squared_error: 0.3802\n",
      "Epoch 210/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2419 - mean_squared_error: 0.2037 - val_loss: 0.4195 - val_mean_squared_error: 0.3813\n",
      "Epoch 211/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2022 - val_loss: 0.4224 - val_mean_squared_error: 0.3840\n",
      "Epoch 212/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2346 - mean_squared_error: 0.1966 - val_loss: 0.3848 - val_mean_squared_error: 0.3473\n",
      "Epoch 213/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2402 - mean_squared_error: 0.2015 - val_loss: 0.4464 - val_mean_squared_error: 0.4078\n",
      "Epoch 214/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1983 - val_loss: 0.4019 - val_mean_squared_error: 0.3650\n",
      "Epoch 215/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2355 - mean_squared_error: 0.1977 - val_loss: 0.4382 - val_mean_squared_error: 0.4001\n",
      "Epoch 216/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1961 - val_loss: 0.4374 - val_mean_squared_error: 0.3993\n",
      "Epoch 217/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2376 - mean_squared_error: 0.1995 - val_loss: 0.4257 - val_mean_squared_error: 0.3880\n",
      "Epoch 218/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2024 - val_loss: 0.4295 - val_mean_squared_error: 0.3916\n",
      "Epoch 219/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2379 - mean_squared_error: 0.1990 - val_loss: 0.3877 - val_mean_squared_error: 0.3490\n",
      "Epoch 220/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2366 - mean_squared_error: 0.1983 - val_loss: 0.4327 - val_mean_squared_error: 0.3941\n",
      "Epoch 221/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2420 - mean_squared_error: 0.2039 - val_loss: 0.4585 - val_mean_squared_error: 0.4207\n",
      "Epoch 222/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.2011 - val_loss: 0.4131 - val_mean_squared_error: 0.3750\n",
      "Epoch 223/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2393 - mean_squared_error: 0.2013 - val_loss: 0.4408 - val_mean_squared_error: 0.4022\n",
      "Epoch 224/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2382 - mean_squared_error: 0.2001 - val_loss: 0.4813 - val_mean_squared_error: 0.4435\n",
      "Epoch 225/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2359 - mean_squared_error: 0.1977 - val_loss: 0.4225 - val_mean_squared_error: 0.3835\n",
      "Epoch 226/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2455 - mean_squared_error: 0.2071 - val_loss: 0.4344 - val_mean_squared_error: 0.3953\n",
      "Epoch 227/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2376 - mean_squared_error: 0.1987 - val_loss: 0.3878 - val_mean_squared_error: 0.3492\n",
      "Epoch 228/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1971 - val_loss: 0.4846 - val_mean_squared_error: 0.4459\n",
      "Epoch 229/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2323 - mean_squared_error: 0.1939 - val_loss: 0.4417 - val_mean_squared_error: 0.4030\n",
      "Epoch 230/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2430 - mean_squared_error: 0.2043 - val_loss: 0.4509 - val_mean_squared_error: 0.4124\n",
      "Epoch 231/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2399 - mean_squared_error: 0.2011 - val_loss: 0.4494 - val_mean_squared_error: 0.4108\n",
      "Epoch 232/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2428 - mean_squared_error: 0.2041 - val_loss: 0.3936 - val_mean_squared_error: 0.3552\n",
      "Epoch 233/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2022 - val_loss: 0.4120 - val_mean_squared_error: 0.3736\n",
      "Epoch 234/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1981 - val_loss: 0.3999 - val_mean_squared_error: 0.3609\n",
      "Epoch 235/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2380 - mean_squared_error: 0.2000 - val_loss: 0.4371 - val_mean_squared_error: 0.3990\n",
      "Epoch 236/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1980 - val_loss: 0.4195 - val_mean_squared_error: 0.3807\n",
      "Epoch 237/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2421 - mean_squared_error: 0.2030 - val_loss: 0.4394 - val_mean_squared_error: 0.3998\n",
      "Epoch 238/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2398 - mean_squared_error: 0.2000 - val_loss: 0.4225 - val_mean_squared_error: 0.3831\n",
      "Epoch 239/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2027 - val_loss: 0.4238 - val_mean_squared_error: 0.3852\n",
      "Epoch 240/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2395 - mean_squared_error: 0.2013 - val_loss: 0.4169 - val_mean_squared_error: 0.3786\n",
      "Epoch 241/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2037 - val_loss: 0.4017 - val_mean_squared_error: 0.3630\n",
      "Epoch 242/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2442 - mean_squared_error: 0.2050 - val_loss: 0.5214 - val_mean_squared_error: 0.4813\n",
      "Epoch 243/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2012 - val_loss: 0.4169 - val_mean_squared_error: 0.3771\n",
      "Epoch 244/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2018 - val_loss: 0.4247 - val_mean_squared_error: 0.3862\n",
      "Epoch 245/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2409 - mean_squared_error: 0.2024 - val_loss: 0.5162 - val_mean_squared_error: 0.4764\n",
      "Epoch 246/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1989 - val_loss: 0.4031 - val_mean_squared_error: 0.3645\n",
      "Epoch 247/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2450 - mean_squared_error: 0.2058 - val_loss: 0.4332 - val_mean_squared_error: 0.3946\n",
      "Epoch 248/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2396 - mean_squared_error: 0.2011 - val_loss: 0.4287 - val_mean_squared_error: 0.3906\n",
      "Epoch 249/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2361 - mean_squared_error: 0.1979 - val_loss: 0.4143 - val_mean_squared_error: 0.3758\n",
      "Epoch 250/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2372 - mean_squared_error: 0.1989 - val_loss: 0.5002 - val_mean_squared_error: 0.4613\n",
      "Epoch 251/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2364 - mean_squared_error: 0.1975 - val_loss: 0.4162 - val_mean_squared_error: 0.3771\n",
      "Epoch 252/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2469 - mean_squared_error: 0.2084 - val_loss: 0.4388 - val_mean_squared_error: 0.3997\n",
      "Epoch 253/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2034 - val_loss: 0.4330 - val_mean_squared_error: 0.3949\n",
      "Epoch 254/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2364 - mean_squared_error: 0.1980 - val_loss: 0.4850 - val_mean_squared_error: 0.4455\n",
      "Epoch 255/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1985 - val_loss: 0.4761 - val_mean_squared_error: 0.4376\n",
      "Epoch 256/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2367 - mean_squared_error: 0.1980 - val_loss: 0.4258 - val_mean_squared_error: 0.3876\n",
      "Epoch 257/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2445 - mean_squared_error: 0.2060 - val_loss: 0.4598 - val_mean_squared_error: 0.4214\n",
      "Epoch 258/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2401 - mean_squared_error: 0.2016 - val_loss: 0.4394 - val_mean_squared_error: 0.4013\n",
      "Epoch 259/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2367 - mean_squared_error: 0.1983 - val_loss: 0.4982 - val_mean_squared_error: 0.4597\n",
      "Epoch 260/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2038 - val_loss: 0.4189 - val_mean_squared_error: 0.3803\n",
      "Epoch 261/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2374 - mean_squared_error: 0.1987 - val_loss: 0.4252 - val_mean_squared_error: 0.3868\n",
      "Epoch 262/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2427 - mean_squared_error: 0.2048 - val_loss: 0.4144 - val_mean_squared_error: 0.3770\n",
      "Epoch 263/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1984 - val_loss: 0.4790 - val_mean_squared_error: 0.4413\n",
      "Epoch 264/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1963 - val_loss: 0.4657 - val_mean_squared_error: 0.4277\n",
      "Epoch 265/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2036 - val_loss: 0.4833 - val_mean_squared_error: 0.4441\n",
      "Epoch 266/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2337 - mean_squared_error: 0.1950 - val_loss: 0.4485 - val_mean_squared_error: 0.4102\n",
      "Epoch 267/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1994 - val_loss: 0.4484 - val_mean_squared_error: 0.4090\n",
      "Epoch 268/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2435 - mean_squared_error: 0.2040 - val_loss: 0.4632 - val_mean_squared_error: 0.4240\n",
      "Epoch 269/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2320 - mean_squared_error: 0.1938 - val_loss: 0.4099 - val_mean_squared_error: 0.3720\n",
      "Epoch 270/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2384 - mean_squared_error: 0.2002 - val_loss: 0.4166 - val_mean_squared_error: 0.3790\n",
      "Epoch 271/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2396 - mean_squared_error: 0.2010 - val_loss: 0.4476 - val_mean_squared_error: 0.4086\n",
      "Epoch 272/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2454 - mean_squared_error: 0.2053 - val_loss: 0.4491 - val_mean_squared_error: 0.4093\n",
      "Epoch 273/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2414 - mean_squared_error: 0.2011 - val_loss: 0.4352 - val_mean_squared_error: 0.3943\n",
      "Epoch 274/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2398 - mean_squared_error: 0.2005 - val_loss: 0.4258 - val_mean_squared_error: 0.3873\n",
      "Epoch 275/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2006 - val_loss: 0.4694 - val_mean_squared_error: 0.4306\n",
      "Epoch 276/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1957 - val_loss: 0.4531 - val_mean_squared_error: 0.4149\n",
      "Epoch 277/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2393 - mean_squared_error: 0.2015 - val_loss: 0.4292 - val_mean_squared_error: 0.3921\n",
      "Epoch 278/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2027 - val_loss: 0.4723 - val_mean_squared_error: 0.4343\n",
      "Epoch 279/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2356 - mean_squared_error: 0.1975 - val_loss: 0.4839 - val_mean_squared_error: 0.4454\n",
      "Epoch 280/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2351 - mean_squared_error: 0.1967 - val_loss: 0.4310 - val_mean_squared_error: 0.3924\n",
      "Epoch 281/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2402 - mean_squared_error: 0.2018 - val_loss: 0.4341 - val_mean_squared_error: 0.3958\n",
      "Epoch 282/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2445 - mean_squared_error: 0.2063 - val_loss: 0.4441 - val_mean_squared_error: 0.4055\n",
      "Epoch 283/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2409 - mean_squared_error: 0.2018 - val_loss: 0.4161 - val_mean_squared_error: 0.3771\n",
      "Epoch 284/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2345 - mean_squared_error: 0.1958 - val_loss: 0.4373 - val_mean_squared_error: 0.3989\n",
      "Epoch 285/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2017 - val_loss: 0.4804 - val_mean_squared_error: 0.4417\n",
      "Epoch 286/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2515 - mean_squared_error: 0.2121 - val_loss: 0.4210 - val_mean_squared_error: 0.3819\n",
      "Epoch 287/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2379 - mean_squared_error: 0.1989 - val_loss: 0.3915 - val_mean_squared_error: 0.3529\n",
      "Epoch 288/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2365 - mean_squared_error: 0.1979 - val_loss: 0.4200 - val_mean_squared_error: 0.3820\n",
      "Epoch 289/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2375 - mean_squared_error: 0.1995 - val_loss: 0.4228 - val_mean_squared_error: 0.3849\n",
      "Epoch 290/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2361 - mean_squared_error: 0.1975 - val_loss: 0.4295 - val_mean_squared_error: 0.3909\n",
      "Epoch 291/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2368 - mean_squared_error: 0.1979 - val_loss: 0.4267 - val_mean_squared_error: 0.3880\n",
      "Epoch 292/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2442 - mean_squared_error: 0.2054 - val_loss: 0.4455 - val_mean_squared_error: 0.4062\n",
      "Epoch 293/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2425 - mean_squared_error: 0.2030 - val_loss: 0.4110 - val_mean_squared_error: 0.3719\n",
      "Epoch 294/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2444 - mean_squared_error: 0.2049 - val_loss: 0.4388 - val_mean_squared_error: 0.3991\n",
      "Epoch 295/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2040 - val_loss: 0.4161 - val_mean_squared_error: 0.3764\n",
      "Epoch 296/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2002 - val_loss: 0.4488 - val_mean_squared_error: 0.4100\n",
      "Epoch 297/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2033 - val_loss: 0.4193 - val_mean_squared_error: 0.3811\n",
      "Epoch 298/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2013 - val_loss: 0.4173 - val_mean_squared_error: 0.3779\n",
      "Epoch 299/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2437 - mean_squared_error: 0.2046 - val_loss: 0.4997 - val_mean_squared_error: 0.4608\n",
      "Epoch 300/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2343 - mean_squared_error: 0.1959 - val_loss: 0.4465 - val_mean_squared_error: 0.4074\n",
      "Epoch 301/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2400 - mean_squared_error: 0.2010 - val_loss: 0.4386 - val_mean_squared_error: 0.3992\n",
      "Epoch 302/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2034 - val_loss: 0.4026 - val_mean_squared_error: 0.3637\n",
      "Epoch 303/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2016 - val_loss: 0.4247 - val_mean_squared_error: 0.3862\n",
      "Epoch 304/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2396 - mean_squared_error: 0.2006 - val_loss: 0.4390 - val_mean_squared_error: 0.3984\n",
      "Epoch 305/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2367 - mean_squared_error: 0.1972 - val_loss: 0.4317 - val_mean_squared_error: 0.3929\n",
      "Epoch 306/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2025 - val_loss: 0.4471 - val_mean_squared_error: 0.4076\n",
      "Epoch 307/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2356 - mean_squared_error: 0.1964 - val_loss: 0.4084 - val_mean_squared_error: 0.3695\n",
      "Epoch 308/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.1999 - val_loss: 0.4485 - val_mean_squared_error: 0.4097\n",
      "Epoch 309/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2018 - val_loss: 0.4466 - val_mean_squared_error: 0.4081\n",
      "Epoch 310/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2002 - val_loss: 0.4431 - val_mean_squared_error: 0.4044\n",
      "Epoch 311/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2033 - val_loss: 0.4316 - val_mean_squared_error: 0.3931\n",
      "Epoch 312/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2494 - mean_squared_error: 0.2101 - val_loss: 0.4382 - val_mean_squared_error: 0.3984\n",
      "Epoch 313/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2339 - mean_squared_error: 0.1951 - val_loss: 0.4229 - val_mean_squared_error: 0.3845\n",
      "Epoch 314/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2345 - mean_squared_error: 0.1961 - val_loss: 0.4652 - val_mean_squared_error: 0.4274\n",
      "Epoch 315/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.2007 - val_loss: 0.4977 - val_mean_squared_error: 0.4580\n",
      "Epoch 316/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2339 - mean_squared_error: 0.1947 - val_loss: 0.4106 - val_mean_squared_error: 0.3705\n",
      "Epoch 317/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2334 - mean_squared_error: 0.1945 - val_loss: 0.4582 - val_mean_squared_error: 0.4192\n",
      "Epoch 318/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2038 - val_loss: 0.4079 - val_mean_squared_error: 0.3699\n",
      "Epoch 319/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1981 - val_loss: 0.4163 - val_mean_squared_error: 0.3782\n",
      "Epoch 320/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1979 - val_loss: 0.4242 - val_mean_squared_error: 0.3860\n",
      "Epoch 321/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2375 - mean_squared_error: 0.1989 - val_loss: 0.4280 - val_mean_squared_error: 0.3897\n",
      "Epoch 322/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2449 - mean_squared_error: 0.2063 - val_loss: 0.4398 - val_mean_squared_error: 0.4013\n",
      "Epoch 323/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2425 - mean_squared_error: 0.2038 - val_loss: 0.4238 - val_mean_squared_error: 0.3847\n",
      "Epoch 324/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.1996 - val_loss: 0.4252 - val_mean_squared_error: 0.3871\n",
      "Epoch 325/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2451 - mean_squared_error: 0.2062 - val_loss: 0.4362 - val_mean_squared_error: 0.3953\n",
      "Epoch 326/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2009 - val_loss: 0.4105 - val_mean_squared_error: 0.3706\n",
      "Epoch 327/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2400 - mean_squared_error: 0.2008 - val_loss: 0.4075 - val_mean_squared_error: 0.3687\n",
      "Epoch 328/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1987 - val_loss: 0.4379 - val_mean_squared_error: 0.3999\n",
      "Epoch 329/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2387 - mean_squared_error: 0.2002 - val_loss: 0.4479 - val_mean_squared_error: 0.4089\n",
      "Epoch 330/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2384 - mean_squared_error: 0.1995 - val_loss: 0.3845 - val_mean_squared_error: 0.3464\n",
      "Epoch 331/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2352 - mean_squared_error: 0.1970 - val_loss: 0.4002 - val_mean_squared_error: 0.3617\n",
      "Epoch 332/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2406 - mean_squared_error: 0.2020 - val_loss: 0.4063 - val_mean_squared_error: 0.3672\n",
      "Epoch 333/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2381 - mean_squared_error: 0.1996 - val_loss: 0.4300 - val_mean_squared_error: 0.3912\n",
      "Epoch 334/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2430 - mean_squared_error: 0.2043 - val_loss: 0.4462 - val_mean_squared_error: 0.4066\n",
      "Epoch 335/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2347 - mean_squared_error: 0.1960 - val_loss: 0.5010 - val_mean_squared_error: 0.4624\n",
      "Epoch 336/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2344 - mean_squared_error: 0.1961 - val_loss: 0.4328 - val_mean_squared_error: 0.3942\n",
      "Epoch 337/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2411 - mean_squared_error: 0.2023 - val_loss: 0.4113 - val_mean_squared_error: 0.3732\n",
      "Epoch 338/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2430 - mean_squared_error: 0.2045 - val_loss: 0.4214 - val_mean_squared_error: 0.3824\n",
      "Epoch 339/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2375 - mean_squared_error: 0.1985 - val_loss: 0.4401 - val_mean_squared_error: 0.4019\n",
      "Epoch 340/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2387 - mean_squared_error: 0.2002 - val_loss: 0.5030 - val_mean_squared_error: 0.4643\n",
      "Epoch 341/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2436 - mean_squared_error: 0.2045 - val_loss: 0.4194 - val_mean_squared_error: 0.3807\n",
      "Epoch 342/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2329 - mean_squared_error: 0.1946 - val_loss: 0.4171 - val_mean_squared_error: 0.3787\n",
      "Epoch 343/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1975 - val_loss: 0.4928 - val_mean_squared_error: 0.4547\n",
      "Epoch 344/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2340 - mean_squared_error: 0.1956 - val_loss: 0.4277 - val_mean_squared_error: 0.3893\n",
      "Epoch 345/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2427 - mean_squared_error: 0.2042 - val_loss: 0.4652 - val_mean_squared_error: 0.4270\n",
      "Epoch 346/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.1999 - val_loss: 0.4421 - val_mean_squared_error: 0.4034\n",
      "Epoch 347/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2386 - mean_squared_error: 0.1995 - val_loss: 0.4734 - val_mean_squared_error: 0.4338\n",
      "Epoch 348/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2480 - mean_squared_error: 0.2078 - val_loss: 0.4667 - val_mean_squared_error: 0.4253\n",
      "Epoch 349/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2012 - val_loss: 0.4157 - val_mean_squared_error: 0.3769\n",
      "Epoch 350/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2363 - mean_squared_error: 0.1973 - val_loss: 0.4122 - val_mean_squared_error: 0.3735\n",
      "Epoch 351/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2424 - mean_squared_error: 0.2037 - val_loss: 0.4341 - val_mean_squared_error: 0.3954\n",
      "Epoch 352/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2366 - mean_squared_error: 0.1982 - val_loss: 0.4134 - val_mean_squared_error: 0.3750\n",
      "Epoch 353/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2377 - mean_squared_error: 0.1990 - val_loss: 0.4276 - val_mean_squared_error: 0.3888\n",
      "Epoch 354/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2352 - mean_squared_error: 0.1965 - val_loss: 0.4574 - val_mean_squared_error: 0.4186\n",
      "Epoch 355/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2349 - mean_squared_error: 0.1962 - val_loss: 0.4533 - val_mean_squared_error: 0.4156\n",
      "Epoch 356/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1980 - val_loss: 0.4922 - val_mean_squared_error: 0.4536\n",
      "Epoch 357/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2427 - mean_squared_error: 0.2039 - val_loss: 0.4723 - val_mean_squared_error: 0.4328\n",
      "Epoch 358/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2442 - mean_squared_error: 0.2046 - val_loss: 0.4013 - val_mean_squared_error: 0.3620\n",
      "Epoch 359/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2371 - mean_squared_error: 0.1980 - val_loss: 0.4286 - val_mean_squared_error: 0.3898\n",
      "Epoch 360/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2334 - mean_squared_error: 0.1948 - val_loss: 0.4636 - val_mean_squared_error: 0.4241\n",
      "Epoch 361/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2445 - mean_squared_error: 0.2053 - val_loss: 0.5087 - val_mean_squared_error: 0.4695\n",
      "Epoch 362/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2364 - mean_squared_error: 0.1975 - val_loss: 0.4312 - val_mean_squared_error: 0.3930\n",
      "Epoch 363/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2439 - mean_squared_error: 0.2053 - val_loss: 0.4151 - val_mean_squared_error: 0.3759\n",
      "Epoch 364/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.1993 - val_loss: 0.4243 - val_mean_squared_error: 0.3856\n",
      "Epoch 365/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2027 - val_loss: 0.4225 - val_mean_squared_error: 0.3842\n",
      "Epoch 366/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2370 - mean_squared_error: 0.1983 - val_loss: 0.4211 - val_mean_squared_error: 0.3823\n",
      "Epoch 367/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2353 - mean_squared_error: 0.1967 - val_loss: 0.4513 - val_mean_squared_error: 0.4125\n",
      "Epoch 368/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2368 - mean_squared_error: 0.1976 - val_loss: 0.4638 - val_mean_squared_error: 0.4243\n",
      "Epoch 369/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2377 - mean_squared_error: 0.1985 - val_loss: 0.4353 - val_mean_squared_error: 0.3960\n",
      "Epoch 370/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2028 - val_loss: 0.4013 - val_mean_squared_error: 0.3623\n",
      "Epoch 371/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1958 - val_loss: 0.4124 - val_mean_squared_error: 0.3744\n",
      "Epoch 372/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1980 - val_loss: 0.4762 - val_mean_squared_error: 0.4372\n",
      "Epoch 373/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2356 - mean_squared_error: 0.1966 - val_loss: 0.4107 - val_mean_squared_error: 0.3721\n",
      "Epoch 374/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1990 - val_loss: 0.4767 - val_mean_squared_error: 0.4375\n",
      "Epoch 375/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2371 - mean_squared_error: 0.1987 - val_loss: 0.4329 - val_mean_squared_error: 0.3945\n",
      "Epoch 376/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2369 - mean_squared_error: 0.1987 - val_loss: 0.3952 - val_mean_squared_error: 0.3577\n",
      "Epoch 377/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1998 - val_loss: 0.4049 - val_mean_squared_error: 0.3663\n",
      "Epoch 378/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2014 - val_loss: 0.4446 - val_mean_squared_error: 0.4048\n",
      "Epoch 379/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2415 - mean_squared_error: 0.2025 - val_loss: 0.3867 - val_mean_squared_error: 0.3488\n",
      "Epoch 380/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2345 - mean_squared_error: 0.1960 - val_loss: 0.3793 - val_mean_squared_error: 0.3412\n",
      "Epoch 381/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2375 - mean_squared_error: 0.1995 - val_loss: 0.4056 - val_mean_squared_error: 0.3682\n",
      "Epoch 382/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2381 - mean_squared_error: 0.2004 - val_loss: 0.4250 - val_mean_squared_error: 0.3872\n",
      "Epoch 383/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2374 - mean_squared_error: 0.1997 - val_loss: 0.4776 - val_mean_squared_error: 0.4394\n",
      "Epoch 384/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2356 - mean_squared_error: 0.1975 - val_loss: 0.4079 - val_mean_squared_error: 0.3704\n",
      "Epoch 385/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2399 - mean_squared_error: 0.2013 - val_loss: 0.4273 - val_mean_squared_error: 0.3882\n",
      "Epoch 386/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2384 - mean_squared_error: 0.1995 - val_loss: 0.4341 - val_mean_squared_error: 0.3952\n",
      "Epoch 387/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2361 - mean_squared_error: 0.1974 - val_loss: 0.3995 - val_mean_squared_error: 0.3606\n",
      "Epoch 388/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2387 - mean_squared_error: 0.2004 - val_loss: 0.4294 - val_mean_squared_error: 0.3910\n",
      "Epoch 389/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2414 - mean_squared_error: 0.2030 - val_loss: 0.4173 - val_mean_squared_error: 0.3789\n",
      "Epoch 390/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2457 - mean_squared_error: 0.2067 - val_loss: 0.3925 - val_mean_squared_error: 0.3532\n",
      "Epoch 391/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2409 - mean_squared_error: 0.2019 - val_loss: 0.4793 - val_mean_squared_error: 0.4391\n",
      "Epoch 392/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2360 - mean_squared_error: 0.1968 - val_loss: 0.4017 - val_mean_squared_error: 0.3637\n",
      "Epoch 393/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2394 - mean_squared_error: 0.2011 - val_loss: 0.4867 - val_mean_squared_error: 0.4483\n",
      "Epoch 394/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2043 - val_loss: 0.4390 - val_mean_squared_error: 0.4014\n",
      "Epoch 395/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2318 - mean_squared_error: 0.1939 - val_loss: 0.4210 - val_mean_squared_error: 0.3831\n",
      "Epoch 396/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2382 - mean_squared_error: 0.2000 - val_loss: 0.4196 - val_mean_squared_error: 0.3815\n",
      "Epoch 397/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2408 - mean_squared_error: 0.2023 - val_loss: 0.4388 - val_mean_squared_error: 0.4000\n",
      "Epoch 398/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2396 - mean_squared_error: 0.2005 - val_loss: 0.4647 - val_mean_squared_error: 0.4247\n",
      "Epoch 399/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2457 - mean_squared_error: 0.2069 - val_loss: 0.4999 - val_mean_squared_error: 0.4614\n",
      "Epoch 400/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1960 - val_loss: 0.4395 - val_mean_squared_error: 0.4024\n",
      "Epoch 401/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1997 - val_loss: 0.4455 - val_mean_squared_error: 0.4070\n",
      "Epoch 402/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2435 - mean_squared_error: 0.2045 - val_loss: 0.4260 - val_mean_squared_error: 0.3866\n",
      "Epoch 403/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2366 - mean_squared_error: 0.1974 - val_loss: 0.4497 - val_mean_squared_error: 0.4107\n",
      "Epoch 404/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2428 - mean_squared_error: 0.2036 - val_loss: 0.4554 - val_mean_squared_error: 0.4159\n",
      "Epoch 405/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2330 - mean_squared_error: 0.1944 - val_loss: 0.4682 - val_mean_squared_error: 0.4294\n",
      "Epoch 406/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2411 - mean_squared_error: 0.2023 - val_loss: 0.4699 - val_mean_squared_error: 0.4305\n",
      "Epoch 407/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2405 - mean_squared_error: 0.2012 - val_loss: 0.4167 - val_mean_squared_error: 0.3775\n",
      "Epoch 408/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2014 - val_loss: 0.4808 - val_mean_squared_error: 0.4418\n",
      "Epoch 409/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2031 - val_loss: 0.4342 - val_mean_squared_error: 0.3956\n",
      "Epoch 410/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2417 - mean_squared_error: 0.2034 - val_loss: 0.4562 - val_mean_squared_error: 0.4182\n",
      "Epoch 411/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2395 - mean_squared_error: 0.2009 - val_loss: 0.4428 - val_mean_squared_error: 0.4042\n",
      "Epoch 412/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.2000 - val_loss: 0.4681 - val_mean_squared_error: 0.4293\n",
      "Epoch 413/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2363 - mean_squared_error: 0.1976 - val_loss: 0.4280 - val_mean_squared_error: 0.3893\n",
      "Epoch 414/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2436 - mean_squared_error: 0.2052 - val_loss: 0.4124 - val_mean_squared_error: 0.3741\n",
      "Epoch 415/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2378 - mean_squared_error: 0.1991 - val_loss: 0.4523 - val_mean_squared_error: 0.4140\n",
      "Epoch 416/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2391 - mean_squared_error: 0.2008 - val_loss: 0.4125 - val_mean_squared_error: 0.3747\n",
      "Epoch 417/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2367 - mean_squared_error: 0.1983 - val_loss: 0.4395 - val_mean_squared_error: 0.4006\n",
      "Epoch 418/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2372 - mean_squared_error: 0.1988 - val_loss: 0.4381 - val_mean_squared_error: 0.4002\n",
      "Epoch 419/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2421 - mean_squared_error: 0.2028 - val_loss: 0.4244 - val_mean_squared_error: 0.3854\n",
      "Epoch 420/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2391 - mean_squared_error: 0.2003 - val_loss: 0.4025 - val_mean_squared_error: 0.3642\n",
      "Epoch 421/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2398 - mean_squared_error: 0.2014 - val_loss: 0.4363 - val_mean_squared_error: 0.3977\n",
      "Epoch 422/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2347 - mean_squared_error: 0.1965 - val_loss: 0.4363 - val_mean_squared_error: 0.3976\n",
      "Epoch 423/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2392 - mean_squared_error: 0.2003 - val_loss: 0.4530 - val_mean_squared_error: 0.4144\n",
      "Epoch 424/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2393 - mean_squared_error: 0.2011 - val_loss: 0.4317 - val_mean_squared_error: 0.3935\n",
      "Epoch 425/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2361 - mean_squared_error: 0.1976 - val_loss: 0.4288 - val_mean_squared_error: 0.3910\n",
      "Epoch 426/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2345 - mean_squared_error: 0.1966 - val_loss: 0.4448 - val_mean_squared_error: 0.4069\n",
      "Epoch 427/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2382 - mean_squared_error: 0.1998 - val_loss: 0.4326 - val_mean_squared_error: 0.3941\n",
      "Epoch 428/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2346 - mean_squared_error: 0.1959 - val_loss: 0.4618 - val_mean_squared_error: 0.4236\n",
      "Epoch 429/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2413 - mean_squared_error: 0.2024 - val_loss: 0.5176 - val_mean_squared_error: 0.4776\n",
      "Epoch 430/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2349 - mean_squared_error: 0.1962 - val_loss: 0.4466 - val_mean_squared_error: 0.4081\n",
      "Epoch 431/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2373 - mean_squared_error: 0.1984 - val_loss: 0.4556 - val_mean_squared_error: 0.4167\n",
      "Epoch 432/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2388 - mean_squared_error: 0.2001 - val_loss: 0.4572 - val_mean_squared_error: 0.4185\n",
      "Epoch 433/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2384 - mean_squared_error: 0.1992 - val_loss: 0.4616 - val_mean_squared_error: 0.4223\n",
      "Epoch 434/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.2003 - val_loss: 0.4708 - val_mean_squared_error: 0.4325\n",
      "Epoch 435/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2426 - mean_squared_error: 0.2025 - val_loss: 0.4543 - val_mean_squared_error: 0.4139\n",
      "Epoch 436/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2330 - mean_squared_error: 0.1935 - val_loss: 0.4347 - val_mean_squared_error: 0.3955\n",
      "Epoch 437/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.1997 - val_loss: 0.4243 - val_mean_squared_error: 0.3844\n",
      "Epoch 438/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1976 - val_loss: 0.3987 - val_mean_squared_error: 0.3599\n",
      "Epoch 439/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2468 - mean_squared_error: 0.2076 - val_loss: 0.4675 - val_mean_squared_error: 0.4285\n",
      "Epoch 440/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2346 - mean_squared_error: 0.1962 - val_loss: 0.4307 - val_mean_squared_error: 0.3928\n",
      "Epoch 441/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2340 - mean_squared_error: 0.1961 - val_loss: 0.4246 - val_mean_squared_error: 0.3866\n",
      "Epoch 442/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2404 - mean_squared_error: 0.2022 - val_loss: 0.4362 - val_mean_squared_error: 0.3980\n",
      "Epoch 443/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2372 - mean_squared_error: 0.1990 - val_loss: 0.4336 - val_mean_squared_error: 0.3954\n",
      "Epoch 444/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2368 - mean_squared_error: 0.1990 - val_loss: 0.4356 - val_mean_squared_error: 0.3979\n",
      "Epoch 445/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2403 - mean_squared_error: 0.2018 - val_loss: 0.4361 - val_mean_squared_error: 0.3976\n",
      "Epoch 446/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2369 - mean_squared_error: 0.1986 - val_loss: 0.4211 - val_mean_squared_error: 0.3832\n",
      "Epoch 447/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2423 - mean_squared_error: 0.2039 - val_loss: 0.4159 - val_mean_squared_error: 0.3774\n",
      "Epoch 448/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1981 - val_loss: 0.4875 - val_mean_squared_error: 0.4483\n",
      "Epoch 449/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2377 - mean_squared_error: 0.1991 - val_loss: 0.4138 - val_mean_squared_error: 0.3754\n",
      "Epoch 450/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2356 - mean_squared_error: 0.1973 - val_loss: 0.4176 - val_mean_squared_error: 0.3795\n",
      "Epoch 451/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2393 - mean_squared_error: 0.2008 - val_loss: 0.4355 - val_mean_squared_error: 0.3972\n",
      "Epoch 452/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2411 - mean_squared_error: 0.2028 - val_loss: 0.4289 - val_mean_squared_error: 0.3906\n",
      "Epoch 453/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2344 - mean_squared_error: 0.1962 - val_loss: 0.4217 - val_mean_squared_error: 0.3836\n",
      "Epoch 454/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.2003 - val_loss: 0.5131 - val_mean_squared_error: 0.4739\n",
      "Epoch 455/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.1998 - val_loss: 0.4070 - val_mean_squared_error: 0.3678\n",
      "Epoch 456/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2386 - mean_squared_error: 0.2000 - val_loss: 0.4584 - val_mean_squared_error: 0.4202\n",
      "Epoch 457/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2026 - val_loss: 0.4522 - val_mean_squared_error: 0.4120\n",
      "Epoch 458/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2397 - mean_squared_error: 0.2000 - val_loss: 0.4747 - val_mean_squared_error: 0.4355\n",
      "Epoch 459/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2366 - mean_squared_error: 0.1975 - val_loss: 0.4526 - val_mean_squared_error: 0.4150\n",
      "Epoch 460/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2433 - mean_squared_error: 0.2049 - val_loss: 0.3935 - val_mean_squared_error: 0.3547\n",
      "Epoch 461/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.1995 - val_loss: 0.4233 - val_mean_squared_error: 0.3838\n",
      "Epoch 462/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2353 - mean_squared_error: 0.1963 - val_loss: 0.4017 - val_mean_squared_error: 0.3633\n",
      "Epoch 463/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2438 - mean_squared_error: 0.2047 - val_loss: 0.3899 - val_mean_squared_error: 0.3507\n",
      "Epoch 464/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2391 - mean_squared_error: 0.2003 - val_loss: 0.4722 - val_mean_squared_error: 0.4326\n",
      "Epoch 465/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2423 - mean_squared_error: 0.2033 - val_loss: 0.4208 - val_mean_squared_error: 0.3824\n",
      "Epoch 466/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2383 - mean_squared_error: 0.2001 - val_loss: 0.4545 - val_mean_squared_error: 0.4160\n",
      "Epoch 467/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2390 - mean_squared_error: 0.2004 - val_loss: 0.4818 - val_mean_squared_error: 0.4433\n",
      "Epoch 468/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2389 - mean_squared_error: 0.2006 - val_loss: 0.4149 - val_mean_squared_error: 0.3768\n",
      "Epoch 469/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2349 - mean_squared_error: 0.1967 - val_loss: 0.4766 - val_mean_squared_error: 0.4376\n",
      "Epoch 470/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2341 - mean_squared_error: 0.1951 - val_loss: 0.4326 - val_mean_squared_error: 0.3941\n",
      "Epoch 471/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2353 - mean_squared_error: 0.1966 - val_loss: 0.4492 - val_mean_squared_error: 0.4108\n",
      "Epoch 472/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2434 - mean_squared_error: 0.2040 - val_loss: 0.4443 - val_mean_squared_error: 0.4056\n",
      "Epoch 473/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2349 - mean_squared_error: 0.1966 - val_loss: 0.4429 - val_mean_squared_error: 0.4040\n",
      "Epoch 474/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2387 - mean_squared_error: 0.1995 - val_loss: 0.4907 - val_mean_squared_error: 0.4522\n",
      "Epoch 475/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2354 - mean_squared_error: 0.1968 - val_loss: 0.4423 - val_mean_squared_error: 0.4034\n",
      "Epoch 476/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2418 - mean_squared_error: 0.2033 - val_loss: 0.4029 - val_mean_squared_error: 0.3651\n",
      "Epoch 477/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2395 - mean_squared_error: 0.2016 - val_loss: 0.4500 - val_mean_squared_error: 0.4118\n",
      "Epoch 478/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2355 - mean_squared_error: 0.1971 - val_loss: 0.4695 - val_mean_squared_error: 0.4303\n",
      "Epoch 479/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2412 - mean_squared_error: 0.2024 - val_loss: 0.4024 - val_mean_squared_error: 0.3639\n",
      "Epoch 480/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2399 - mean_squared_error: 0.2015 - val_loss: 0.4112 - val_mean_squared_error: 0.3731\n",
      "Epoch 481/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2328 - mean_squared_error: 0.1944 - val_loss: 0.4190 - val_mean_squared_error: 0.3807\n",
      "Epoch 482/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2345 - mean_squared_error: 0.1960 - val_loss: 0.4167 - val_mean_squared_error: 0.3778\n",
      "Epoch 483/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2385 - mean_squared_error: 0.1992 - val_loss: 0.4920 - val_mean_squared_error: 0.4529\n",
      "Epoch 484/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2349 - mean_squared_error: 0.1962 - val_loss: 0.4179 - val_mean_squared_error: 0.3796\n",
      "Epoch 485/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2368 - mean_squared_error: 0.1986 - val_loss: 0.4592 - val_mean_squared_error: 0.4201\n",
      "Epoch 486/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2369 - mean_squared_error: 0.1982 - val_loss: 0.4270 - val_mean_squared_error: 0.3884\n",
      "Epoch 487/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2446 - mean_squared_error: 0.2056 - val_loss: 0.4675 - val_mean_squared_error: 0.4286\n",
      "Epoch 488/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2362 - mean_squared_error: 0.1971 - val_loss: 0.4129 - val_mean_squared_error: 0.3742\n",
      "Epoch 489/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2327 - mean_squared_error: 0.1942 - val_loss: 0.4250 - val_mean_squared_error: 0.3872\n",
      "Epoch 490/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2416 - mean_squared_error: 0.2028 - val_loss: 0.4493 - val_mean_squared_error: 0.4106\n",
      "Epoch 491/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2443 - mean_squared_error: 0.2060 - val_loss: 0.4181 - val_mean_squared_error: 0.3806\n",
      "Epoch 492/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2309 - mean_squared_error: 0.1928 - val_loss: 0.4295 - val_mean_squared_error: 0.3916\n",
      "Epoch 493/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2394 - mean_squared_error: 0.2009 - val_loss: 0.4761 - val_mean_squared_error: 0.4370\n",
      "Epoch 494/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2343 - mean_squared_error: 0.1957 - val_loss: 0.4269 - val_mean_squared_error: 0.3879\n",
      "Epoch 495/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2373 - mean_squared_error: 0.1993 - val_loss: 0.4170 - val_mean_squared_error: 0.3789\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2258 - mean_squared_error: 0.1877\n",
      "Epoch 1/10000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2915 - mean_squared_error: 0.2491 - val_loss: 0.4435 - val_mean_squared_error: 0.4010\n",
      "Epoch 2/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2908 - mean_squared_error: 0.2482 - val_loss: 0.4335 - val_mean_squared_error: 0.3909\n",
      "Epoch 3/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2958 - mean_squared_error: 0.2529 - val_loss: 0.4248 - val_mean_squared_error: 0.3813\n",
      "Epoch 4/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2952 - mean_squared_error: 0.2507 - val_loss: 0.4416 - val_mean_squared_error: 0.3981\n",
      "Epoch 5/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2935 - mean_squared_error: 0.2498 - val_loss: 0.4595 - val_mean_squared_error: 0.4158\n",
      "Epoch 6/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2861 - mean_squared_error: 0.2430 - val_loss: 0.4328 - val_mean_squared_error: 0.3904\n",
      "Epoch 7/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2845 - mean_squared_error: 0.2416 - val_loss: 0.4251 - val_mean_squared_error: 0.3821\n",
      "Epoch 8/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2447 - val_loss: 0.4826 - val_mean_squared_error: 0.4396\n",
      "Epoch 9/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2482 - val_loss: 0.4437 - val_mean_squared_error: 0.4011\n",
      "Epoch 10/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2464 - val_loss: 0.4578 - val_mean_squared_error: 0.4150\n",
      "Epoch 11/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2863 - mean_squared_error: 0.2437 - val_loss: 0.4322 - val_mean_squared_error: 0.3897\n",
      "Epoch 12/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2466 - val_loss: 0.4460 - val_mean_squared_error: 0.4024\n",
      "Epoch 13/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2944 - mean_squared_error: 0.2512 - val_loss: 0.4490 - val_mean_squared_error: 0.4049\n",
      "Epoch 14/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2938 - mean_squared_error: 0.2506 - val_loss: 0.4420 - val_mean_squared_error: 0.3989\n",
      "Epoch 15/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2914 - mean_squared_error: 0.2488 - val_loss: 0.4372 - val_mean_squared_error: 0.3946\n",
      "Epoch 16/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2875 - mean_squared_error: 0.2451 - val_loss: 0.4175 - val_mean_squared_error: 0.3753\n",
      "Epoch 17/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2435 - val_loss: 0.4562 - val_mean_squared_error: 0.4136\n",
      "Epoch 18/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2490 - val_loss: 0.4478 - val_mean_squared_error: 0.4046\n",
      "Epoch 19/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2859 - mean_squared_error: 0.2431 - val_loss: 0.4374 - val_mean_squared_error: 0.3952\n",
      "Epoch 20/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2926 - mean_squared_error: 0.2499 - val_loss: 0.4265 - val_mean_squared_error: 0.3835\n",
      "Epoch 21/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2474 - val_loss: 0.4757 - val_mean_squared_error: 0.4331\n",
      "Epoch 22/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2897 - mean_squared_error: 0.2465 - val_loss: 0.4390 - val_mean_squared_error: 0.3958\n",
      "Epoch 23/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2922 - mean_squared_error: 0.2491 - val_loss: 0.4288 - val_mean_squared_error: 0.3860\n",
      "Epoch 24/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2490 - val_loss: 0.4215 - val_mean_squared_error: 0.3788\n",
      "Epoch 25/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2448 - val_loss: 0.4419 - val_mean_squared_error: 0.3993\n",
      "Epoch 26/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2854 - mean_squared_error: 0.2430 - val_loss: 0.4471 - val_mean_squared_error: 0.4044\n",
      "Epoch 27/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2858 - mean_squared_error: 0.2433 - val_loss: 0.4252 - val_mean_squared_error: 0.3827\n",
      "Epoch 28/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2810 - mean_squared_error: 0.2383 - val_loss: 0.4335 - val_mean_squared_error: 0.3907\n",
      "Epoch 29/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2940 - mean_squared_error: 0.2510 - val_loss: 0.4545 - val_mean_squared_error: 0.4114\n",
      "Epoch 30/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2951 - mean_squared_error: 0.2514 - val_loss: 0.4375 - val_mean_squared_error: 0.3941\n",
      "Epoch 31/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2470 - val_loss: 0.4481 - val_mean_squared_error: 0.4055\n",
      "Epoch 32/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2475 - val_loss: 0.4759 - val_mean_squared_error: 0.4333\n",
      "Epoch 33/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2471 - val_loss: 0.4309 - val_mean_squared_error: 0.3889\n",
      "Epoch 34/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2496 - val_loss: 0.4353 - val_mean_squared_error: 0.3924\n",
      "Epoch 35/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2912 - mean_squared_error: 0.2484 - val_loss: 0.4525 - val_mean_squared_error: 0.4098\n",
      "Epoch 36/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2475 - val_loss: 0.4497 - val_mean_squared_error: 0.4071\n",
      "Epoch 37/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2898 - mean_squared_error: 0.2469 - val_loss: 0.4350 - val_mean_squared_error: 0.3919\n",
      "Epoch 38/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2446 - val_loss: 0.4347 - val_mean_squared_error: 0.3920\n",
      "Epoch 39/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2469 - val_loss: 0.4582 - val_mean_squared_error: 0.4154\n",
      "Epoch 40/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2958 - mean_squared_error: 0.2530 - val_loss: 0.4241 - val_mean_squared_error: 0.3815\n",
      "Epoch 41/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2950 - mean_squared_error: 0.2524 - val_loss: 0.4484 - val_mean_squared_error: 0.4054\n",
      "Epoch 42/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2901 - mean_squared_error: 0.2468 - val_loss: 0.4218 - val_mean_squared_error: 0.3788\n",
      "Epoch 43/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2489 - val_loss: 0.4305 - val_mean_squared_error: 0.3881\n",
      "Epoch 44/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2479 - val_loss: 0.4454 - val_mean_squared_error: 0.4029\n",
      "Epoch 45/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2847 - mean_squared_error: 0.2425 - val_loss: 0.4885 - val_mean_squared_error: 0.4462\n",
      "Epoch 46/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2474 - val_loss: 0.4604 - val_mean_squared_error: 0.4177\n",
      "Epoch 47/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2478 - val_loss: 0.4539 - val_mean_squared_error: 0.4101\n",
      "Epoch 48/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2922 - mean_squared_error: 0.2493 - val_loss: 0.4123 - val_mean_squared_error: 0.3700\n",
      "Epoch 49/10000\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2868 - mean_squared_error: 0.2440 - val_loss: 0.4119 - val_mean_squared_error: 0.3691\n",
      "Epoch 50/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2488 - val_loss: 0.4354 - val_mean_squared_error: 0.3922\n",
      "Epoch 51/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2912 - mean_squared_error: 0.2483 - val_loss: 0.4324 - val_mean_squared_error: 0.3893\n",
      "Epoch 52/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2846 - mean_squared_error: 0.2412 - val_loss: 0.4176 - val_mean_squared_error: 0.3736\n",
      "Epoch 53/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2471 - val_loss: 0.4832 - val_mean_squared_error: 0.4404\n",
      "Epoch 54/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2919 - mean_squared_error: 0.2493 - val_loss: 0.4507 - val_mean_squared_error: 0.4078\n",
      "Epoch 55/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2876 - mean_squared_error: 0.2450 - val_loss: 0.4494 - val_mean_squared_error: 0.4063\n",
      "Epoch 56/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2445 - val_loss: 0.4381 - val_mean_squared_error: 0.3959\n",
      "Epoch 57/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2493 - val_loss: 0.4633 - val_mean_squared_error: 0.4203\n",
      "Epoch 58/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2887 - mean_squared_error: 0.2458 - val_loss: 0.4309 - val_mean_squared_error: 0.3883\n",
      "Epoch 59/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2875 - mean_squared_error: 0.2449 - val_loss: 0.4649 - val_mean_squared_error: 0.4220\n",
      "Epoch 60/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2926 - mean_squared_error: 0.2495 - val_loss: 0.4482 - val_mean_squared_error: 0.4047\n",
      "Epoch 61/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2930 - mean_squared_error: 0.2500 - val_loss: 0.4364 - val_mean_squared_error: 0.3935\n",
      "Epoch 62/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2884 - mean_squared_error: 0.2458 - val_loss: 0.4328 - val_mean_squared_error: 0.3906\n",
      "Epoch 63/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2854 - mean_squared_error: 0.2430 - val_loss: 0.4263 - val_mean_squared_error: 0.3839\n",
      "Epoch 64/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2901 - mean_squared_error: 0.2475 - val_loss: 0.4182 - val_mean_squared_error: 0.3756\n",
      "Epoch 65/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2454 - val_loss: 0.4333 - val_mean_squared_error: 0.3902\n",
      "Epoch 66/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2445 - val_loss: 0.4196 - val_mean_squared_error: 0.3769\n",
      "Epoch 67/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2923 - mean_squared_error: 0.2493 - val_loss: 0.4710 - val_mean_squared_error: 0.4285\n",
      "Epoch 68/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2475 - val_loss: 0.4496 - val_mean_squared_error: 0.4070\n",
      "Epoch 69/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2470 - val_loss: 0.4567 - val_mean_squared_error: 0.4133\n",
      "Epoch 70/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2441 - val_loss: 0.4660 - val_mean_squared_error: 0.4234\n",
      "Epoch 71/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2454 - val_loss: 0.4815 - val_mean_squared_error: 0.4383\n",
      "Epoch 72/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2943 - mean_squared_error: 0.2511 - val_loss: 0.4277 - val_mean_squared_error: 0.3844\n",
      "Epoch 73/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2840 - mean_squared_error: 0.2412 - val_loss: 0.4295 - val_mean_squared_error: 0.3871\n",
      "Epoch 74/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2483 - val_loss: 0.5067 - val_mean_squared_error: 0.4628\n",
      "Epoch 75/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2451 - val_loss: 0.4161 - val_mean_squared_error: 0.3736\n",
      "Epoch 76/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2484 - val_loss: 0.4242 - val_mean_squared_error: 0.3808\n",
      "Epoch 77/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2447 - val_loss: 0.5141 - val_mean_squared_error: 0.4710\n",
      "Epoch 78/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2886 - mean_squared_error: 0.2449 - val_loss: 0.4970 - val_mean_squared_error: 0.4537\n",
      "Epoch 79/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2485 - val_loss: 0.4569 - val_mean_squared_error: 0.4141\n",
      "Epoch 80/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2855 - mean_squared_error: 0.2425 - val_loss: 0.4731 - val_mean_squared_error: 0.4300\n",
      "Epoch 81/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2442 - val_loss: 0.4497 - val_mean_squared_error: 0.4064\n",
      "Epoch 82/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2836 - mean_squared_error: 0.2406 - val_loss: 0.4790 - val_mean_squared_error: 0.4359\n",
      "Epoch 83/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2468 - val_loss: 0.4632 - val_mean_squared_error: 0.4196\n",
      "Epoch 84/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2889 - mean_squared_error: 0.2459 - val_loss: 0.4219 - val_mean_squared_error: 0.3790\n",
      "Epoch 85/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2912 - mean_squared_error: 0.2474 - val_loss: 0.4396 - val_mean_squared_error: 0.3963\n",
      "Epoch 86/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2893 - mean_squared_error: 0.2461 - val_loss: 0.4613 - val_mean_squared_error: 0.4180\n",
      "Epoch 87/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2438 - val_loss: 0.4310 - val_mean_squared_error: 0.3886\n",
      "Epoch 88/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2828 - mean_squared_error: 0.2399 - val_loss: 0.4212 - val_mean_squared_error: 0.3785\n",
      "Epoch 89/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2897 - mean_squared_error: 0.2468 - val_loss: 0.4633 - val_mean_squared_error: 0.4199\n",
      "Epoch 90/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_squared_error: 0.2438 - val_loss: 0.4469 - val_mean_squared_error: 0.4042\n",
      "Epoch 91/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2929 - mean_squared_error: 0.2502 - val_loss: 0.4291 - val_mean_squared_error: 0.3858\n",
      "Epoch 92/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2482 - val_loss: 0.4325 - val_mean_squared_error: 0.3901\n",
      "Epoch 93/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2453 - val_loss: 0.4286 - val_mean_squared_error: 0.3863\n",
      "Epoch 94/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2936 - mean_squared_error: 0.2509 - val_loss: 0.4736 - val_mean_squared_error: 0.4307\n",
      "Epoch 95/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2932 - mean_squared_error: 0.2501 - val_loss: 0.4513 - val_mean_squared_error: 0.4077\n",
      "Epoch 96/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2442 - val_loss: 0.4499 - val_mean_squared_error: 0.4071\n",
      "Epoch 97/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2867 - mean_squared_error: 0.2437 - val_loss: 0.4378 - val_mean_squared_error: 0.3947\n",
      "Epoch 98/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2465 - val_loss: 0.4650 - val_mean_squared_error: 0.4219\n",
      "Epoch 99/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2949 - mean_squared_error: 0.2513 - val_loss: 0.4260 - val_mean_squared_error: 0.3823\n",
      "Epoch 100/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2474 - val_loss: 0.4277 - val_mean_squared_error: 0.3846\n",
      "Epoch 101/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2955 - mean_squared_error: 0.2517 - val_loss: 0.4671 - val_mean_squared_error: 0.4223\n",
      "Epoch 102/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2893 - mean_squared_error: 0.2458 - val_loss: 0.4342 - val_mean_squared_error: 0.3913\n",
      "Epoch 103/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2857 - mean_squared_error: 0.2424 - val_loss: 0.4391 - val_mean_squared_error: 0.3962\n",
      "Epoch 104/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2425 - val_loss: 0.4853 - val_mean_squared_error: 0.4422\n",
      "Epoch 105/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2446 - val_loss: 0.4884 - val_mean_squared_error: 0.4446\n",
      "Epoch 106/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2472 - val_loss: 0.4357 - val_mean_squared_error: 0.3931\n",
      "Epoch 107/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2934 - mean_squared_error: 0.2503 - val_loss: 0.4417 - val_mean_squared_error: 0.3988\n",
      "Epoch 108/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2466 - val_loss: 0.4223 - val_mean_squared_error: 0.3790\n",
      "Epoch 109/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2461 - val_loss: 0.4125 - val_mean_squared_error: 0.3694\n",
      "Epoch 110/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2425 - val_loss: 0.4288 - val_mean_squared_error: 0.3849\n",
      "Epoch 111/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2849 - mean_squared_error: 0.2416 - val_loss: 0.4209 - val_mean_squared_error: 0.3785\n",
      "Epoch 112/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2979 - mean_squared_error: 0.2542 - val_loss: 0.4420 - val_mean_squared_error: 0.3982\n",
      "Epoch 113/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2481 - val_loss: 0.4492 - val_mean_squared_error: 0.4062\n",
      "Epoch 114/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2848 - mean_squared_error: 0.2414 - val_loss: 0.4651 - val_mean_squared_error: 0.4220\n",
      "Epoch 115/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_squared_error: 0.2485 - val_loss: 0.4258 - val_mean_squared_error: 0.3823\n",
      "Epoch 116/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2921 - mean_squared_error: 0.2493 - val_loss: 0.4252 - val_mean_squared_error: 0.3825\n",
      "Epoch 117/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2472 - val_loss: 0.4463 - val_mean_squared_error: 0.4037\n",
      "Epoch 118/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2851 - mean_squared_error: 0.2429 - val_loss: 0.4717 - val_mean_squared_error: 0.4289\n",
      "Epoch 119/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2450 - val_loss: 0.4425 - val_mean_squared_error: 0.3998\n",
      "Epoch 120/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2470 - val_loss: 0.4301 - val_mean_squared_error: 0.3871\n",
      "Epoch 121/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2898 - mean_squared_error: 0.2469 - val_loss: 0.4215 - val_mean_squared_error: 0.3784\n",
      "Epoch 122/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2906 - mean_squared_error: 0.2478 - val_loss: 0.4452 - val_mean_squared_error: 0.4027\n",
      "Epoch 123/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2462 - val_loss: 0.4496 - val_mean_squared_error: 0.4064\n",
      "Epoch 124/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2875 - mean_squared_error: 0.2444 - val_loss: 0.4536 - val_mean_squared_error: 0.4099\n",
      "Epoch 125/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2436 - val_loss: 0.4569 - val_mean_squared_error: 0.4139\n",
      "Epoch 126/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2459 - val_loss: 0.4273 - val_mean_squared_error: 0.3842\n",
      "Epoch 127/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2928 - mean_squared_error: 0.2500 - val_loss: 0.4503 - val_mean_squared_error: 0.4076\n",
      "Epoch 128/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2952 - mean_squared_error: 0.2518 - val_loss: 0.4506 - val_mean_squared_error: 0.4066\n",
      "Epoch 129/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2468 - val_loss: 0.4693 - val_mean_squared_error: 0.4259\n",
      "Epoch 130/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2824 - mean_squared_error: 0.2395 - val_loss: 0.4786 - val_mean_squared_error: 0.4360\n",
      "Epoch 131/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2871 - mean_squared_error: 0.2438 - val_loss: 0.4313 - val_mean_squared_error: 0.3875\n",
      "Epoch 132/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2946 - mean_squared_error: 0.2506 - val_loss: 0.4556 - val_mean_squared_error: 0.4114\n",
      "Epoch 133/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2911 - mean_squared_error: 0.2472 - val_loss: 0.4392 - val_mean_squared_error: 0.3958\n",
      "Epoch 134/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2884 - mean_squared_error: 0.2454 - val_loss: 0.4301 - val_mean_squared_error: 0.3873\n",
      "Epoch 135/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2961 - mean_squared_error: 0.2528 - val_loss: 0.4839 - val_mean_squared_error: 0.4394\n",
      "Epoch 136/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2474 - val_loss: 0.4422 - val_mean_squared_error: 0.3988\n",
      "Epoch 137/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2942 - mean_squared_error: 0.2506 - val_loss: 0.4535 - val_mean_squared_error: 0.4106\n",
      "Epoch 138/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2875 - mean_squared_error: 0.2444 - val_loss: 0.4489 - val_mean_squared_error: 0.4057\n",
      "Epoch 139/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2473 - val_loss: 0.4344 - val_mean_squared_error: 0.3907\n",
      "Epoch 140/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2459 - val_loss: 0.4292 - val_mean_squared_error: 0.3865\n",
      "Epoch 141/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2467 - val_loss: 0.4450 - val_mean_squared_error: 0.4010\n",
      "Epoch 142/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2925 - mean_squared_error: 0.2491 - val_loss: 0.4510 - val_mean_squared_error: 0.4075\n",
      "Epoch 143/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2941 - mean_squared_error: 0.2508 - val_loss: 0.4412 - val_mean_squared_error: 0.3981\n",
      "Epoch 144/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2471 - val_loss: 0.4893 - val_mean_squared_error: 0.4465\n",
      "Epoch 145/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2473 - val_loss: 0.4226 - val_mean_squared_error: 0.3795\n",
      "Epoch 146/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2895 - mean_squared_error: 0.2462 - val_loss: 0.4217 - val_mean_squared_error: 0.3789\n",
      "Epoch 147/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2822 - mean_squared_error: 0.2393 - val_loss: 0.4435 - val_mean_squared_error: 0.4003\n",
      "Epoch 148/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2480 - val_loss: 0.4236 - val_mean_squared_error: 0.3805\n",
      "Epoch 149/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2436 - val_loss: 0.4265 - val_mean_squared_error: 0.3834\n",
      "Epoch 150/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2883 - mean_squared_error: 0.2452 - val_loss: 0.4287 - val_mean_squared_error: 0.3863\n",
      "Epoch 151/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2895 - mean_squared_error: 0.2464 - val_loss: 0.4232 - val_mean_squared_error: 0.3801\n",
      "Epoch 152/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2928 - mean_squared_error: 0.2492 - val_loss: 0.4299 - val_mean_squared_error: 0.3866\n",
      "Epoch 153/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2873 - mean_squared_error: 0.2442 - val_loss: 0.4262 - val_mean_squared_error: 0.3832\n",
      "Epoch 154/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2838 - mean_squared_error: 0.2410 - val_loss: 0.4441 - val_mean_squared_error: 0.4015\n",
      "Epoch 155/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2911 - mean_squared_error: 0.2483 - val_loss: 0.4572 - val_mean_squared_error: 0.4142\n",
      "Epoch 156/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2853 - mean_squared_error: 0.2428 - val_loss: 0.4278 - val_mean_squared_error: 0.3847\n",
      "Epoch 157/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2898 - mean_squared_error: 0.2468 - val_loss: 0.4242 - val_mean_squared_error: 0.3818\n",
      "Epoch 158/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2443 - val_loss: 0.4407 - val_mean_squared_error: 0.3979\n",
      "Epoch 159/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2851 - mean_squared_error: 0.2419 - val_loss: 0.4198 - val_mean_squared_error: 0.3771\n",
      "Epoch 160/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2883 - mean_squared_error: 0.2457 - val_loss: 0.4353 - val_mean_squared_error: 0.3925\n",
      "Epoch 161/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2476 - val_loss: 0.4361 - val_mean_squared_error: 0.3935\n",
      "Epoch 162/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2464 - val_loss: 0.4230 - val_mean_squared_error: 0.3802\n",
      "Epoch 163/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2459 - val_loss: 0.4401 - val_mean_squared_error: 0.3970\n",
      "Epoch 164/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2458 - val_loss: 0.4376 - val_mean_squared_error: 0.3945\n",
      "Epoch 165/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2942 - mean_squared_error: 0.2509 - val_loss: 0.4165 - val_mean_squared_error: 0.3732\n",
      "Epoch 166/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2925 - mean_squared_error: 0.2487 - val_loss: 0.4418 - val_mean_squared_error: 0.3983\n",
      "Epoch 167/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2478 - val_loss: 0.4437 - val_mean_squared_error: 0.4006\n",
      "Epoch 168/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2478 - val_loss: 0.4965 - val_mean_squared_error: 0.4527\n",
      "Epoch 169/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2941 - mean_squared_error: 0.2498 - val_loss: 0.4685 - val_mean_squared_error: 0.4241\n",
      "Epoch 170/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2433 - val_loss: 0.4703 - val_mean_squared_error: 0.4265\n",
      "Epoch 171/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2456 - val_loss: 0.5313 - val_mean_squared_error: 0.4883\n",
      "Epoch 172/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2435 - val_loss: 0.4899 - val_mean_squared_error: 0.4457\n",
      "Epoch 173/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2447 - val_loss: 0.4556 - val_mean_squared_error: 0.4125\n",
      "Epoch 174/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2927 - mean_squared_error: 0.2493 - val_loss: 0.4555 - val_mean_squared_error: 0.4118\n",
      "Epoch 175/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2883 - mean_squared_error: 0.2445 - val_loss: 0.4468 - val_mean_squared_error: 0.4036\n",
      "Epoch 176/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2439 - val_loss: 0.4335 - val_mean_squared_error: 0.3901\n",
      "Epoch 177/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2908 - mean_squared_error: 0.2477 - val_loss: 0.4742 - val_mean_squared_error: 0.4310\n",
      "Epoch 178/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2462 - val_loss: 0.4347 - val_mean_squared_error: 0.3910\n",
      "Epoch 179/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_squared_error: 0.2478 - val_loss: 0.4470 - val_mean_squared_error: 0.4034\n",
      "Epoch 180/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2434 - val_loss: 0.4469 - val_mean_squared_error: 0.4034\n",
      "Epoch 181/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2941 - mean_squared_error: 0.2504 - val_loss: 0.4361 - val_mean_squared_error: 0.3920\n",
      "Epoch 182/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2882 - mean_squared_error: 0.2444 - val_loss: 0.4303 - val_mean_squared_error: 0.3866\n",
      "Epoch 183/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2875 - mean_squared_error: 0.2442 - val_loss: 0.5004 - val_mean_squared_error: 0.4575\n",
      "Epoch 184/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2898 - mean_squared_error: 0.2466 - val_loss: 0.4494 - val_mean_squared_error: 0.4055\n",
      "Epoch 185/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2477 - val_loss: 0.4227 - val_mean_squared_error: 0.3780\n",
      "Epoch 186/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2456 - val_loss: 0.4290 - val_mean_squared_error: 0.3863\n",
      "Epoch 187/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2955 - mean_squared_error: 0.2520 - val_loss: 0.4454 - val_mean_squared_error: 0.4016\n",
      "Epoch 188/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2879 - mean_squared_error: 0.2443 - val_loss: 0.4610 - val_mean_squared_error: 0.4171\n",
      "Epoch 189/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2904 - mean_squared_error: 0.2467 - val_loss: 0.4255 - val_mean_squared_error: 0.3811\n",
      "Epoch 190/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2966 - mean_squared_error: 0.2528 - val_loss: 0.4658 - val_mean_squared_error: 0.4210\n",
      "Epoch 191/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2442 - val_loss: 0.4218 - val_mean_squared_error: 0.3789\n",
      "Epoch 192/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2457 - val_loss: 0.4305 - val_mean_squared_error: 0.3870\n",
      "Epoch 193/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_squared_error: 0.2482 - val_loss: 0.4403 - val_mean_squared_error: 0.3972\n",
      "Epoch 194/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2860 - mean_squared_error: 0.2429 - val_loss: 0.4617 - val_mean_squared_error: 0.4189\n",
      "Epoch 195/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2436 - val_loss: 0.4454 - val_mean_squared_error: 0.4028\n",
      "Epoch 196/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2859 - mean_squared_error: 0.2430 - val_loss: 0.4282 - val_mean_squared_error: 0.3860\n",
      "Epoch 197/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2882 - mean_squared_error: 0.2451 - val_loss: 0.4631 - val_mean_squared_error: 0.4195\n",
      "Epoch 198/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2926 - mean_squared_error: 0.2496 - val_loss: 0.4412 - val_mean_squared_error: 0.3983\n",
      "Epoch 199/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2488 - val_loss: 0.4620 - val_mean_squared_error: 0.4190\n",
      "Epoch 200/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2945 - mean_squared_error: 0.2513 - val_loss: 0.4546 - val_mean_squared_error: 0.4115\n",
      "Epoch 201/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2927 - mean_squared_error: 0.2501 - val_loss: 0.4340 - val_mean_squared_error: 0.3915\n",
      "Epoch 202/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2922 - mean_squared_error: 0.2492 - val_loss: 0.4334 - val_mean_squared_error: 0.3912\n",
      "Epoch 203/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2856 - mean_squared_error: 0.2430 - val_loss: 0.4759 - val_mean_squared_error: 0.4330\n",
      "Epoch 204/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2484 - val_loss: 0.4829 - val_mean_squared_error: 0.4393\n",
      "Epoch 205/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2479 - val_loss: 0.4717 - val_mean_squared_error: 0.4290\n",
      "Epoch 206/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2941 - mean_squared_error: 0.2512 - val_loss: 0.4373 - val_mean_squared_error: 0.3940\n",
      "Epoch 207/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_squared_error: 0.2435 - val_loss: 0.4460 - val_mean_squared_error: 0.4026\n",
      "Epoch 208/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2832 - mean_squared_error: 0.2401 - val_loss: 0.4389 - val_mean_squared_error: 0.3962\n",
      "Epoch 209/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2879 - mean_squared_error: 0.2448 - val_loss: 0.4628 - val_mean_squared_error: 0.4198\n",
      "Epoch 210/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2462 - val_loss: 0.4701 - val_mean_squared_error: 0.4265\n",
      "Epoch 211/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2824 - mean_squared_error: 0.2398 - val_loss: 0.4580 - val_mean_squared_error: 0.4156\n",
      "Epoch 212/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2859 - mean_squared_error: 0.2434 - val_loss: 0.5016 - val_mean_squared_error: 0.4585\n",
      "Epoch 213/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2897 - mean_squared_error: 0.2464 - val_loss: 0.4447 - val_mean_squared_error: 0.4021\n",
      "Epoch 214/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2467 - val_loss: 0.4433 - val_mean_squared_error: 0.4002\n",
      "Epoch 215/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2928 - mean_squared_error: 0.2498 - val_loss: 0.4618 - val_mean_squared_error: 0.4191\n",
      "Epoch 216/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2895 - mean_squared_error: 0.2466 - val_loss: 0.4281 - val_mean_squared_error: 0.3847\n",
      "Epoch 217/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2438 - val_loss: 0.4701 - val_mean_squared_error: 0.4271\n",
      "Epoch 218/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2884 - mean_squared_error: 0.2456 - val_loss: 0.4533 - val_mean_squared_error: 0.4109\n",
      "Epoch 219/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2460 - val_loss: 0.4369 - val_mean_squared_error: 0.3931\n",
      "Epoch 220/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2451 - val_loss: 0.4721 - val_mean_squared_error: 0.4287\n",
      "Epoch 221/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2851 - mean_squared_error: 0.2412 - val_loss: 0.4762 - val_mean_squared_error: 0.4322\n",
      "Epoch 222/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2886 - mean_squared_error: 0.2450 - val_loss: 0.4822 - val_mean_squared_error: 0.4386\n",
      "Epoch 223/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2873 - mean_squared_error: 0.2439 - val_loss: 0.4729 - val_mean_squared_error: 0.4288\n",
      "Epoch 224/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2475 - val_loss: 0.4279 - val_mean_squared_error: 0.3851\n",
      "Epoch 225/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2942 - mean_squared_error: 0.2512 - val_loss: 0.4428 - val_mean_squared_error: 0.3994\n",
      "Epoch 226/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_squared_error: 0.2481 - val_loss: 0.4435 - val_mean_squared_error: 0.3999\n",
      "Epoch 227/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2828 - mean_squared_error: 0.2395 - val_loss: 0.4654 - val_mean_squared_error: 0.4224\n",
      "Epoch 228/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2832 - mean_squared_error: 0.2401 - val_loss: 0.4234 - val_mean_squared_error: 0.3805\n",
      "Epoch 229/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2871 - mean_squared_error: 0.2439 - val_loss: 0.4183 - val_mean_squared_error: 0.3754\n",
      "Epoch 230/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2423 - val_loss: 0.4415 - val_mean_squared_error: 0.3980\n",
      "Epoch 231/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2451 - val_loss: 0.4209 - val_mean_squared_error: 0.3785\n",
      "Epoch 232/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2449 - val_loss: 0.4845 - val_mean_squared_error: 0.4413\n",
      "Epoch 233/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2444 - val_loss: 0.4660 - val_mean_squared_error: 0.4233\n",
      "Epoch 234/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2444 - val_loss: 0.4418 - val_mean_squared_error: 0.3997\n",
      "Epoch 235/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2433 - val_loss: 0.4374 - val_mean_squared_error: 0.3941\n",
      "Epoch 236/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2839 - mean_squared_error: 0.2413 - val_loss: 0.4986 - val_mean_squared_error: 0.4555\n",
      "Epoch 237/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2844 - mean_squared_error: 0.2418 - val_loss: 0.4898 - val_mean_squared_error: 0.4470\n",
      "Epoch 238/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2481 - val_loss: 0.4544 - val_mean_squared_error: 0.4104\n",
      "Epoch 239/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2903 - mean_squared_error: 0.2470 - val_loss: 0.4844 - val_mean_squared_error: 0.4411\n",
      "Epoch 240/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2921 - mean_squared_error: 0.2493 - val_loss: 0.4642 - val_mean_squared_error: 0.4208\n",
      "Epoch 241/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2915 - mean_squared_error: 0.2483 - val_loss: 0.4255 - val_mean_squared_error: 0.3826\n",
      "Epoch 242/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2861 - mean_squared_error: 0.2431 - val_loss: 0.4774 - val_mean_squared_error: 0.4335\n",
      "Epoch 243/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2465 - val_loss: 0.4523 - val_mean_squared_error: 0.4089\n",
      "Epoch 244/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2904 - mean_squared_error: 0.2471 - val_loss: 0.4774 - val_mean_squared_error: 0.4343\n",
      "Epoch 245/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2816 - mean_squared_error: 0.2382 - val_loss: 0.4525 - val_mean_squared_error: 0.4097\n",
      "Epoch 246/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2889 - mean_squared_error: 0.2455 - val_loss: 0.4473 - val_mean_squared_error: 0.4041\n",
      "Epoch 247/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2889 - mean_squared_error: 0.2454 - val_loss: 0.4521 - val_mean_squared_error: 0.4090\n",
      "Epoch 248/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2455 - val_loss: 0.4590 - val_mean_squared_error: 0.4159\n",
      "Epoch 249/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2879 - mean_squared_error: 0.2450 - val_loss: 0.4877 - val_mean_squared_error: 0.4446\n",
      "Epoch 250/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2924 - mean_squared_error: 0.2487 - val_loss: 0.4465 - val_mean_squared_error: 0.4028\n",
      "Epoch 251/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2917 - mean_squared_error: 0.2486 - val_loss: 0.4495 - val_mean_squared_error: 0.4071\n",
      "Epoch 252/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2867 - mean_squared_error: 0.2438 - val_loss: 0.4672 - val_mean_squared_error: 0.4247\n",
      "Epoch 253/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2444 - val_loss: 0.4539 - val_mean_squared_error: 0.4111\n",
      "Epoch 254/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2916 - mean_squared_error: 0.2482 - val_loss: 0.4212 - val_mean_squared_error: 0.3785\n",
      "Epoch 255/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2483 - val_loss: 0.4595 - val_mean_squared_error: 0.4156\n",
      "Epoch 256/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2886 - mean_squared_error: 0.2449 - val_loss: 0.4622 - val_mean_squared_error: 0.4188\n",
      "Epoch 257/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2465 - val_loss: 0.4610 - val_mean_squared_error: 0.4183\n",
      "Epoch 258/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2810 - mean_squared_error: 0.2382 - val_loss: 0.4737 - val_mean_squared_error: 0.4305\n",
      "Epoch 259/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2446 - val_loss: 0.4907 - val_mean_squared_error: 0.4483\n",
      "Epoch 260/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2849 - mean_squared_error: 0.2420 - val_loss: 0.4458 - val_mean_squared_error: 0.4032\n",
      "Epoch 261/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2850 - mean_squared_error: 0.2418 - val_loss: 0.4404 - val_mean_squared_error: 0.3971\n",
      "Epoch 262/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2836 - mean_squared_error: 0.2407 - val_loss: 0.4286 - val_mean_squared_error: 0.3861\n",
      "Epoch 263/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2416 - val_loss: 0.4297 - val_mean_squared_error: 0.3872\n",
      "Epoch 264/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2836 - mean_squared_error: 0.2408 - val_loss: 0.4434 - val_mean_squared_error: 0.4002\n",
      "Epoch 265/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2872 - mean_squared_error: 0.2446 - val_loss: 0.4582 - val_mean_squared_error: 0.4154\n",
      "Epoch 266/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2883 - mean_squared_error: 0.2453 - val_loss: 0.4905 - val_mean_squared_error: 0.4471\n",
      "Epoch 267/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2910 - mean_squared_error: 0.2477 - val_loss: 0.4340 - val_mean_squared_error: 0.3910\n",
      "Epoch 268/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2455 - val_loss: 0.4259 - val_mean_squared_error: 0.3827\n",
      "Epoch 269/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2836 - mean_squared_error: 0.2404 - val_loss: 0.4356 - val_mean_squared_error: 0.3925\n",
      "Epoch 270/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2874 - mean_squared_error: 0.2443 - val_loss: 0.4520 - val_mean_squared_error: 0.4087\n",
      "Epoch 271/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2973 - mean_squared_error: 0.2529 - val_loss: 0.4276 - val_mean_squared_error: 0.3836\n",
      "Epoch 272/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2828 - mean_squared_error: 0.2395 - val_loss: 0.4350 - val_mean_squared_error: 0.3923\n",
      "Epoch 273/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2466 - val_loss: 0.4337 - val_mean_squared_error: 0.3907\n",
      "Epoch 274/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2436 - val_loss: 0.4344 - val_mean_squared_error: 0.3905\n",
      "Epoch 275/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2873 - mean_squared_error: 0.2436 - val_loss: 0.4229 - val_mean_squared_error: 0.3800\n",
      "Epoch 276/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2956 - mean_squared_error: 0.2525 - val_loss: 0.4381 - val_mean_squared_error: 0.3936\n",
      "Epoch 277/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2840 - mean_squared_error: 0.2409 - val_loss: 0.4572 - val_mean_squared_error: 0.4139\n",
      "Epoch 278/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2414 - val_loss: 0.4507 - val_mean_squared_error: 0.4079\n",
      "Epoch 279/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2417 - val_loss: 0.4305 - val_mean_squared_error: 0.3878\n",
      "Epoch 280/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2834 - mean_squared_error: 0.2404 - val_loss: 0.4551 - val_mean_squared_error: 0.4124\n",
      "Epoch 281/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2427 - val_loss: 0.4223 - val_mean_squared_error: 0.3798\n",
      "Epoch 282/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2882 - mean_squared_error: 0.2455 - val_loss: 0.4235 - val_mean_squared_error: 0.3805\n",
      "Epoch 283/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2930 - mean_squared_error: 0.2499 - val_loss: 0.4463 - val_mean_squared_error: 0.4031\n",
      "Epoch 284/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2447 - val_loss: 0.4256 - val_mean_squared_error: 0.3823\n",
      "Epoch 285/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2876 - mean_squared_error: 0.2444 - val_loss: 0.4309 - val_mean_squared_error: 0.3873\n",
      "Epoch 286/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2462 - val_loss: 0.4358 - val_mean_squared_error: 0.3927\n",
      "Epoch 287/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2860 - mean_squared_error: 0.2429 - val_loss: 0.4567 - val_mean_squared_error: 0.4136\n",
      "Epoch 288/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2463 - val_loss: 0.4384 - val_mean_squared_error: 0.3962\n",
      "Epoch 289/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2879 - mean_squared_error: 0.2450 - val_loss: 0.4460 - val_mean_squared_error: 0.4035\n",
      "Epoch 290/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2463 - val_loss: 0.4748 - val_mean_squared_error: 0.4313\n",
      "Epoch 291/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2903 - mean_squared_error: 0.2467 - val_loss: 0.4639 - val_mean_squared_error: 0.4206\n",
      "Epoch 292/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2882 - mean_squared_error: 0.2453 - val_loss: 0.4264 - val_mean_squared_error: 0.3837\n",
      "Epoch 293/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2903 - mean_squared_error: 0.2471 - val_loss: 0.4342 - val_mean_squared_error: 0.3906\n",
      "Epoch 294/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2847 - mean_squared_error: 0.2418 - val_loss: 0.5157 - val_mean_squared_error: 0.4732\n",
      "Epoch 295/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2895 - mean_squared_error: 0.2466 - val_loss: 0.4517 - val_mean_squared_error: 0.4083\n",
      "Epoch 296/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2884 - mean_squared_error: 0.2456 - val_loss: 0.4359 - val_mean_squared_error: 0.3929\n",
      "Epoch 297/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2893 - mean_squared_error: 0.2459 - val_loss: 0.4340 - val_mean_squared_error: 0.3909\n",
      "Epoch 298/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2839 - mean_squared_error: 0.2409 - val_loss: 0.4304 - val_mean_squared_error: 0.3883\n",
      "Epoch 299/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2921 - mean_squared_error: 0.2491 - val_loss: 0.5158 - val_mean_squared_error: 0.4722\n",
      "Epoch 300/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2831 - mean_squared_error: 0.2403 - val_loss: 0.4347 - val_mean_squared_error: 0.3916\n",
      "Epoch 301/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2909 - mean_squared_error: 0.2474 - val_loss: 0.4659 - val_mean_squared_error: 0.4225\n",
      "Epoch 302/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2457 - val_loss: 0.4343 - val_mean_squared_error: 0.3908\n",
      "Epoch 303/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2879 - mean_squared_error: 0.2447 - val_loss: 0.4582 - val_mean_squared_error: 0.4153\n",
      "Epoch 304/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2838 - mean_squared_error: 0.2406 - val_loss: 0.4899 - val_mean_squared_error: 0.4465\n",
      "Epoch 305/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2850 - mean_squared_error: 0.2419 - val_loss: 0.4526 - val_mean_squared_error: 0.4094\n",
      "Epoch 306/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2433 - val_loss: 0.4371 - val_mean_squared_error: 0.3943\n",
      "Epoch 307/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2886 - mean_squared_error: 0.2457 - val_loss: 0.4479 - val_mean_squared_error: 0.4054\n",
      "Epoch 308/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2935 - mean_squared_error: 0.2497 - val_loss: 0.4527 - val_mean_squared_error: 0.4092\n",
      "Epoch 309/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2453 - val_loss: 0.4526 - val_mean_squared_error: 0.4091\n",
      "Epoch 310/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2818 - mean_squared_error: 0.2383 - val_loss: 0.4365 - val_mean_squared_error: 0.3933\n",
      "Epoch 311/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2883 - mean_squared_error: 0.2455 - val_loss: 0.4272 - val_mean_squared_error: 0.3847\n",
      "Epoch 312/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2897 - mean_squared_error: 0.2469 - val_loss: 0.4467 - val_mean_squared_error: 0.4041\n",
      "Epoch 313/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2452 - val_loss: 0.4357 - val_mean_squared_error: 0.3925\n",
      "Epoch 314/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2912 - mean_squared_error: 0.2478 - val_loss: 0.4272 - val_mean_squared_error: 0.3837\n",
      "Epoch 315/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2446 - val_loss: 0.4291 - val_mean_squared_error: 0.3854\n",
      "Epoch 316/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2429 - val_loss: 0.4245 - val_mean_squared_error: 0.3812\n",
      "Epoch 317/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2457 - val_loss: 0.4549 - val_mean_squared_error: 0.4108\n",
      "Epoch 318/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2891 - mean_squared_error: 0.2455 - val_loss: 0.4229 - val_mean_squared_error: 0.3798\n",
      "Epoch 319/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2410 - val_loss: 0.4367 - val_mean_squared_error: 0.3935\n",
      "Epoch 320/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2846 - mean_squared_error: 0.2416 - val_loss: 0.4336 - val_mean_squared_error: 0.3913\n",
      "Epoch 321/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2922 - mean_squared_error: 0.2488 - val_loss: 0.4303 - val_mean_squared_error: 0.3874\n",
      "Epoch 322/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2909 - mean_squared_error: 0.2477 - val_loss: 0.4601 - val_mean_squared_error: 0.4165\n",
      "Epoch 323/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2475 - val_loss: 0.4401 - val_mean_squared_error: 0.3971\n",
      "Epoch 324/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2805 - mean_squared_error: 0.2377 - val_loss: 0.4296 - val_mean_squared_error: 0.3869\n",
      "Epoch 325/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2859 - mean_squared_error: 0.2425 - val_loss: 0.4866 - val_mean_squared_error: 0.4425\n",
      "Epoch 326/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2796 - mean_squared_error: 0.2366 - val_loss: 0.4403 - val_mean_squared_error: 0.3977\n",
      "Epoch 327/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2810 - mean_squared_error: 0.2384 - val_loss: 0.4844 - val_mean_squared_error: 0.4425\n",
      "Epoch 328/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2892 - mean_squared_error: 0.2459 - val_loss: 0.4465 - val_mean_squared_error: 0.4036\n",
      "Epoch 329/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2906 - mean_squared_error: 0.2475 - val_loss: 0.4261 - val_mean_squared_error: 0.3832\n",
      "Epoch 330/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2853 - mean_squared_error: 0.2425 - val_loss: 0.4307 - val_mean_squared_error: 0.3881\n",
      "Epoch 331/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2453 - val_loss: 0.4322 - val_mean_squared_error: 0.3899\n",
      "Epoch 332/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2827 - mean_squared_error: 0.2401 - val_loss: 0.5239 - val_mean_squared_error: 0.4810\n",
      "Epoch 333/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2874 - mean_squared_error: 0.2445 - val_loss: 0.4445 - val_mean_squared_error: 0.4020\n",
      "Epoch 334/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2942 - mean_squared_error: 0.2516 - val_loss: 0.4479 - val_mean_squared_error: 0.4050\n",
      "Epoch 335/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2850 - mean_squared_error: 0.2423 - val_loss: 0.4266 - val_mean_squared_error: 0.3837\n",
      "Epoch 336/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2444 - val_loss: 0.4255 - val_mean_squared_error: 0.3832\n",
      "Epoch 337/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2848 - mean_squared_error: 0.2419 - val_loss: 0.4278 - val_mean_squared_error: 0.3857\n",
      "Epoch 338/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2889 - mean_squared_error: 0.2462 - val_loss: 0.4675 - val_mean_squared_error: 0.4242\n",
      "Epoch 339/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2846 - mean_squared_error: 0.2416 - val_loss: 0.4670 - val_mean_squared_error: 0.4241\n",
      "Epoch 340/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2859 - mean_squared_error: 0.2431 - val_loss: 0.4304 - val_mean_squared_error: 0.3881\n",
      "Epoch 341/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2897 - mean_squared_error: 0.2463 - val_loss: 0.4855 - val_mean_squared_error: 0.4419\n",
      "Epoch 342/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2901 - mean_squared_error: 0.2470 - val_loss: 0.4211 - val_mean_squared_error: 0.3782\n",
      "Epoch 343/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2423 - val_loss: 0.4571 - val_mean_squared_error: 0.4129\n",
      "Epoch 344/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2829 - mean_squared_error: 0.2393 - val_loss: 0.4415 - val_mean_squared_error: 0.3986\n",
      "Epoch 345/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2869 - mean_squared_error: 0.2443 - val_loss: 0.4387 - val_mean_squared_error: 0.3962\n",
      "Epoch 346/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2485 - val_loss: 0.4226 - val_mean_squared_error: 0.3800\n",
      "Epoch 347/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2888 - mean_squared_error: 0.2459 - val_loss: 0.4720 - val_mean_squared_error: 0.4290\n",
      "Epoch 348/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2906 - mean_squared_error: 0.2476 - val_loss: 0.4549 - val_mean_squared_error: 0.4116\n",
      "Epoch 349/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2833 - mean_squared_error: 0.2410 - val_loss: 0.4547 - val_mean_squared_error: 0.4127\n",
      "Epoch 350/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2906 - mean_squared_error: 0.2481 - val_loss: 0.4373 - val_mean_squared_error: 0.3946\n",
      "Epoch 351/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2884 - mean_squared_error: 0.2454 - val_loss: 0.4500 - val_mean_squared_error: 0.4071\n",
      "Epoch 352/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2831 - mean_squared_error: 0.2404 - val_loss: 0.4463 - val_mean_squared_error: 0.4035\n",
      "Epoch 353/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2838 - mean_squared_error: 0.2412 - val_loss: 0.4541 - val_mean_squared_error: 0.4112\n",
      "Epoch 354/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2845 - mean_squared_error: 0.2415 - val_loss: 0.4680 - val_mean_squared_error: 0.4236\n",
      "Epoch 355/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2975 - mean_squared_error: 0.2540 - val_loss: 0.4278 - val_mean_squared_error: 0.3851\n",
      "Epoch 356/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2873 - mean_squared_error: 0.2442 - val_loss: 0.4590 - val_mean_squared_error: 0.4160\n",
      "Epoch 357/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2857 - mean_squared_error: 0.2426 - val_loss: 0.4520 - val_mean_squared_error: 0.4087\n",
      "Epoch 358/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_squared_error: 0.2430 - val_loss: 0.4570 - val_mean_squared_error: 0.4145\n",
      "Epoch 359/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2432 - val_loss: 0.4584 - val_mean_squared_error: 0.4159\n",
      "Epoch 360/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2470 - val_loss: 0.4714 - val_mean_squared_error: 0.4274\n",
      "Epoch 361/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2407 - val_loss: 0.4594 - val_mean_squared_error: 0.4167\n",
      "Epoch 362/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2463 - val_loss: 0.4337 - val_mean_squared_error: 0.3906\n",
      "Epoch 363/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2484 - val_loss: 0.4513 - val_mean_squared_error: 0.4083\n",
      "Epoch 364/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2846 - mean_squared_error: 0.2416 - val_loss: 0.4557 - val_mean_squared_error: 0.4123\n",
      "Epoch 365/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2853 - mean_squared_error: 0.2424 - val_loss: 0.4556 - val_mean_squared_error: 0.4128\n",
      "Epoch 366/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2931 - mean_squared_error: 0.2498 - val_loss: 0.4734 - val_mean_squared_error: 0.4306\n",
      "Epoch 367/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2839 - mean_squared_error: 0.2411 - val_loss: 0.4698 - val_mean_squared_error: 0.4270\n",
      "Epoch 368/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2882 - mean_squared_error: 0.2454 - val_loss: 0.4380 - val_mean_squared_error: 0.3953\n",
      "Epoch 369/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2841 - mean_squared_error: 0.2411 - val_loss: 0.5037 - val_mean_squared_error: 0.4606\n",
      "Epoch 370/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2442 - val_loss: 0.4508 - val_mean_squared_error: 0.4080\n",
      "Epoch 371/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2831 - mean_squared_error: 0.2408 - val_loss: 0.4750 - val_mean_squared_error: 0.4324\n",
      "Epoch 372/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2907 - mean_squared_error: 0.2484 - val_loss: 0.4524 - val_mean_squared_error: 0.4097\n",
      "Epoch 373/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_squared_error: 0.2443 - val_loss: 0.4148 - val_mean_squared_error: 0.3732\n",
      "Epoch 374/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2443 - val_loss: 0.4400 - val_mean_squared_error: 0.3967\n",
      "Epoch 375/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2850 - mean_squared_error: 0.2416 - val_loss: 0.4558 - val_mean_squared_error: 0.4123\n",
      "Epoch 376/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2890 - mean_squared_error: 0.2460 - val_loss: 0.4358 - val_mean_squared_error: 0.3922\n",
      "Epoch 377/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2484 - val_loss: 0.4589 - val_mean_squared_error: 0.4159\n",
      "Epoch 378/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2902 - mean_squared_error: 0.2472 - val_loss: 0.4673 - val_mean_squared_error: 0.4240\n",
      "Epoch 379/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2886 - mean_squared_error: 0.2457 - val_loss: 0.4448 - val_mean_squared_error: 0.4021\n",
      "Epoch 380/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2467 - val_loss: 0.4808 - val_mean_squared_error: 0.4378\n",
      "Epoch 381/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2829 - mean_squared_error: 0.2403 - val_loss: 0.4367 - val_mean_squared_error: 0.3946\n",
      "Epoch 382/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2809 - mean_squared_error: 0.2383 - val_loss: 0.4628 - val_mean_squared_error: 0.4202\n",
      "Epoch 383/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2468 - val_loss: 0.4722 - val_mean_squared_error: 0.4287\n",
      "Epoch 384/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2812 - mean_squared_error: 0.2387 - val_loss: 0.4659 - val_mean_squared_error: 0.4235\n",
      "Epoch 385/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2849 - mean_squared_error: 0.2420 - val_loss: 0.4490 - val_mean_squared_error: 0.4060\n",
      "Epoch 386/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2871 - mean_squared_error: 0.2448 - val_loss: 0.4534 - val_mean_squared_error: 0.4107\n",
      "Epoch 387/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2931 - mean_squared_error: 0.2507 - val_loss: 0.4628 - val_mean_squared_error: 0.4206\n",
      "Epoch 388/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2874 - mean_squared_error: 0.2445 - val_loss: 0.4242 - val_mean_squared_error: 0.3812\n",
      "Epoch 389/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2924 - mean_squared_error: 0.2485 - val_loss: 0.4177 - val_mean_squared_error: 0.3749\n",
      "Epoch 390/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2896 - mean_squared_error: 0.2468 - val_loss: 0.4524 - val_mean_squared_error: 0.4095\n",
      "Epoch 391/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2433 - val_loss: 0.4593 - val_mean_squared_error: 0.4164\n",
      "Epoch 392/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2919 - mean_squared_error: 0.2487 - val_loss: 0.4923 - val_mean_squared_error: 0.4483\n",
      "Epoch 393/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2912 - mean_squared_error: 0.2477 - val_loss: 0.4649 - val_mean_squared_error: 0.4215\n",
      "Epoch 394/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2908 - mean_squared_error: 0.2475 - val_loss: 0.4321 - val_mean_squared_error: 0.3892\n",
      "Epoch 395/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2858 - mean_squared_error: 0.2428 - val_loss: 0.4424 - val_mean_squared_error: 0.3995\n",
      "Epoch 396/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2921 - mean_squared_error: 0.2489 - val_loss: 0.4337 - val_mean_squared_error: 0.3911\n",
      "Epoch 397/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2845 - mean_squared_error: 0.2419 - val_loss: 0.4311 - val_mean_squared_error: 0.3882\n",
      "Epoch 398/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2851 - mean_squared_error: 0.2423 - val_loss: 0.4967 - val_mean_squared_error: 0.4538\n",
      "Epoch 399/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2851 - mean_squared_error: 0.2418 - val_loss: 0.4733 - val_mean_squared_error: 0.4307\n",
      "Epoch 400/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2878 - mean_squared_error: 0.2450 - val_loss: 0.4514 - val_mean_squared_error: 0.4089\n",
      "Epoch 401/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2874 - mean_squared_error: 0.2445 - val_loss: 0.4275 - val_mean_squared_error: 0.3851\n",
      "Epoch 402/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2848 - mean_squared_error: 0.2422 - val_loss: 0.5032 - val_mean_squared_error: 0.4611\n",
      "Epoch 403/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2862 - mean_squared_error: 0.2435 - val_loss: 0.4596 - val_mean_squared_error: 0.4159\n",
      "Epoch 404/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2868 - mean_squared_error: 0.2439 - val_loss: 0.4795 - val_mean_squared_error: 0.4365\n",
      "Epoch 405/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2894 - mean_squared_error: 0.2465 - val_loss: 0.4429 - val_mean_squared_error: 0.4000\n",
      "Epoch 406/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2835 - mean_squared_error: 0.2405 - val_loss: 0.4611 - val_mean_squared_error: 0.4176\n",
      "Epoch 407/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2877 - mean_squared_error: 0.2448 - val_loss: 0.4207 - val_mean_squared_error: 0.3783\n",
      "Epoch 408/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2477 - val_loss: 0.4739 - val_mean_squared_error: 0.4305\n",
      "Epoch 409/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2842 - mean_squared_error: 0.2409 - val_loss: 0.4536 - val_mean_squared_error: 0.4106\n",
      "Epoch 410/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2451 - val_loss: 0.4591 - val_mean_squared_error: 0.4164\n",
      "Epoch 411/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2841 - mean_squared_error: 0.2409 - val_loss: 0.4564 - val_mean_squared_error: 0.4138\n",
      "Epoch 412/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2901 - mean_squared_error: 0.2463 - val_loss: 0.4744 - val_mean_squared_error: 0.4296\n",
      "Epoch 413/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2857 - mean_squared_error: 0.2418 - val_loss: 0.4846 - val_mean_squared_error: 0.4411\n",
      "Epoch 414/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2893 - mean_squared_error: 0.2459 - val_loss: 0.5170 - val_mean_squared_error: 0.4734\n",
      "Epoch 415/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2418 - val_loss: 0.5149 - val_mean_squared_error: 0.4720\n",
      "Epoch 416/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2844 - mean_squared_error: 0.2414 - val_loss: 0.4634 - val_mean_squared_error: 0.4202\n",
      "Epoch 417/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2468 - val_loss: 0.4662 - val_mean_squared_error: 0.4226\n",
      "Epoch 418/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2795 - mean_squared_error: 0.2366 - val_loss: 0.4305 - val_mean_squared_error: 0.3881\n",
      "Epoch 419/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2871 - mean_squared_error: 0.2440 - val_loss: 0.4774 - val_mean_squared_error: 0.4352\n",
      "Epoch 420/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2901 - mean_squared_error: 0.2474 - val_loss: 0.4796 - val_mean_squared_error: 0.4370\n",
      "Epoch 421/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2873 - mean_squared_error: 0.2445 - val_loss: 0.4528 - val_mean_squared_error: 0.4103\n",
      "Epoch 422/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2831 - mean_squared_error: 0.2404 - val_loss: 0.4312 - val_mean_squared_error: 0.3884\n",
      "Epoch 423/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2837 - mean_squared_error: 0.2407 - val_loss: 0.4442 - val_mean_squared_error: 0.4013\n",
      "Epoch 424/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2905 - mean_squared_error: 0.2478 - val_loss: 0.4330 - val_mean_squared_error: 0.3904\n",
      "Epoch 425/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2807 - mean_squared_error: 0.2380 - val_loss: 0.4856 - val_mean_squared_error: 0.4428\n",
      "Epoch 426/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2918 - mean_squared_error: 0.2487 - val_loss: 0.4800 - val_mean_squared_error: 0.4371\n",
      "Epoch 427/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2963 - mean_squared_error: 0.2533 - val_loss: 0.4277 - val_mean_squared_error: 0.3846\n",
      "Epoch 428/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2794 - mean_squared_error: 0.2369 - val_loss: 0.4738 - val_mean_squared_error: 0.4316\n",
      "Epoch 429/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2852 - mean_squared_error: 0.2427 - val_loss: 0.5236 - val_mean_squared_error: 0.4804\n",
      "Epoch 430/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2899 - mean_squared_error: 0.2464 - val_loss: 0.4565 - val_mean_squared_error: 0.4112\n",
      "Epoch 431/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2939 - mean_squared_error: 0.2489 - val_loss: 0.4545 - val_mean_squared_error: 0.4111\n",
      "Epoch 432/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2809 - mean_squared_error: 0.2383 - val_loss: 0.4507 - val_mean_squared_error: 0.4083\n",
      "Epoch 433/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2784 - mean_squared_error: 0.2359 - val_loss: 0.4290 - val_mean_squared_error: 0.3861\n",
      "Epoch 434/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2853 - mean_squared_error: 0.2425 - val_loss: 0.4343 - val_mean_squared_error: 0.3920\n",
      "Epoch 435/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2903 - mean_squared_error: 0.2476 - val_loss: 0.4826 - val_mean_squared_error: 0.4392\n",
      "Epoch 436/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2844 - mean_squared_error: 0.2417 - val_loss: 0.4674 - val_mean_squared_error: 0.4246\n",
      "Epoch 437/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2866 - mean_squared_error: 0.2439 - val_loss: 0.4809 - val_mean_squared_error: 0.4383\n",
      "Epoch 438/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2900 - mean_squared_error: 0.2472 - val_loss: 0.4822 - val_mean_squared_error: 0.4393\n",
      "Epoch 439/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2822 - mean_squared_error: 0.2394 - val_loss: 0.4663 - val_mean_squared_error: 0.4232\n",
      "Epoch 440/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2870 - mean_squared_error: 0.2442 - val_loss: 0.4502 - val_mean_squared_error: 0.4069\n",
      "Epoch 441/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2804 - mean_squared_error: 0.2378 - val_loss: 0.4518 - val_mean_squared_error: 0.4089\n",
      "Epoch 442/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2881 - mean_squared_error: 0.2454 - val_loss: 0.4543 - val_mean_squared_error: 0.4113\n",
      "Epoch 443/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2908 - mean_squared_error: 0.2478 - val_loss: 0.4916 - val_mean_squared_error: 0.4485\n",
      "Epoch 444/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2913 - mean_squared_error: 0.2485 - val_loss: 0.4570 - val_mean_squared_error: 0.4139\n",
      "Epoch 445/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2848 - mean_squared_error: 0.2418 - val_loss: 0.4650 - val_mean_squared_error: 0.4217\n",
      "Epoch 446/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2920 - mean_squared_error: 0.2490 - val_loss: 0.5338 - val_mean_squared_error: 0.4902\n",
      "Epoch 447/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2857 - mean_squared_error: 0.2428 - val_loss: 0.4535 - val_mean_squared_error: 0.4109\n",
      "Epoch 448/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2880 - mean_squared_error: 0.2451 - val_loss: 0.4579 - val_mean_squared_error: 0.4146\n",
      "Epoch 449/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2874 - mean_squared_error: 0.2445 - val_loss: 0.4373 - val_mean_squared_error: 0.3944\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2632 - mean_squared_error: 0.2203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, batch_size, num_epochs, goal,class_weights_dict):\"\"\"\n",
    "\n",
    "\n",
    "model_history_model_merged_1L, score_model_merged_1L = compile_and_fit_simple('merged',checkpoints_model_merged_1L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000, 'fit',class_weights_dict)\n",
    "model_history_model_merged_2L, score_model_merged_2L = compile_and_fit_simple('merged',checkpoints_model_merged_2L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)\n",
    "model_history_model_merged_3L, score_model_merged_3L = compile_and_fit_simple('merged',checkpoints_model_merged_3L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)\n",
    "model_history_model_merged_4L, score_model_merged_4L = compile_and_fit_simple('merged',checkpoints_model_merged_4L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "564424ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_alternative/checkpoints_model_merged_1L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_merged_1L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_2L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_merged_2L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_3L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_merged_3L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_4L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_merged_4L_alternative_final.h5')\n",
    "\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_1L_alternative.h5', 'checkpoints_alternative/checkpoints_model_merged_1L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_2L_alternative.h5', 'checkpoints_alternative/checkpoints_model_merged_2L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_3L_alternative.h5', 'checkpoints_alternative/checkpoints_model_merged_3L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_merged_4L_alternative.h5', 'checkpoints_alternative/checkpoints_model_merged_4L_alternative.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b21355",
   "metadata": {},
   "source": [
    "### Factor Feedforward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "feaf151e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5268 - mean_squared_error: 0.5268 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 670/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 671/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6870 - val_mean_squared_error: 0.6870\n",
      "Epoch 672/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 673/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 674/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5268 - mean_squared_error: 0.5268 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 675/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 676/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5266 - mean_squared_error: 0.5266 - val_loss: 0.6870 - val_mean_squared_error: 0.6870\n",
      "Epoch 677/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6865 - val_mean_squared_error: 0.6865\n",
      "Epoch 678/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 679/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 680/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5267 - mean_squared_error: 0.5267 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 681/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 682/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5267 - mean_squared_error: 0.5267 - val_loss: 0.6900 - val_mean_squared_error: 0.6900\n",
      "Epoch 683/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5266 - mean_squared_error: 0.5266 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 684/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 685/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6907 - val_mean_squared_error: 0.6907\n",
      "Epoch 686/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 687/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6891 - val_mean_squared_error: 0.6890\n",
      "Epoch 688/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6870 - val_mean_squared_error: 0.6870\n",
      "Epoch 689/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5260 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 690/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 691/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 692/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6907 - val_mean_squared_error: 0.6907\n",
      "Epoch 693/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 694/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 695/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 696/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 697/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 698/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 699/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 700/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 701/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 702/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 703/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 704/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 705/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6861 - val_mean_squared_error: 0.6861\n",
      "Epoch 706/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 707/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 708/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6873 - val_mean_squared_error: 0.6873\n",
      "Epoch 709/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 710/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 711/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 712/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 713/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 714/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6858 - val_mean_squared_error: 0.6858\n",
      "Epoch 715/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 716/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 717/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 718/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 719/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 720/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6870 - val_mean_squared_error: 0.6870\n",
      "Epoch 721/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 722/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 723/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 724/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 725/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 726/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 727/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 728/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 729/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 730/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 731/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 732/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 733/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 734/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6866 - val_mean_squared_error: 0.6866\n",
      "Epoch 735/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 736/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 737/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 738/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 739/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 740/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 741/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6900 - val_mean_squared_error: 0.6900\n",
      "Epoch 742/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6900 - val_mean_squared_error: 0.6900\n",
      "Epoch 743/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 744/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 745/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 746/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 747/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 748/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6859 - val_mean_squared_error: 0.6859\n",
      "Epoch 749/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 750/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6900 - val_mean_squared_error: 0.6900\n",
      "Epoch 751/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 752/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 753/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6909 - val_mean_squared_error: 0.6909\n",
      "Epoch 754/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6902 - val_mean_squared_error: 0.6902\n",
      "Epoch 755/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 756/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 757/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 758/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 759/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 760/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 761/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 762/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 763/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5264 - mean_squared_error: 0.5264 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 764/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 765/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 766/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 767/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6867 - val_mean_squared_error: 0.6867\n",
      "Epoch 768/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6859 - val_mean_squared_error: 0.6859\n",
      "Epoch 769/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6864 - val_mean_squared_error: 0.6864\n",
      "Epoch 770/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6861 - val_mean_squared_error: 0.6861\n",
      "Epoch 771/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 772/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 773/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6871 - val_mean_squared_error: 0.6871\n",
      "Epoch 774/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 775/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 776/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6869 - val_mean_squared_error: 0.6869\n",
      "Epoch 777/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 778/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 779/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 780/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 781/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 782/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 783/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 784/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 785/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 786/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6869 - val_mean_squared_error: 0.6869\n",
      "Epoch 787/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 788/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5265 - mean_squared_error: 0.5265 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 789/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 790/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6917 - val_mean_squared_error: 0.6917\n",
      "Epoch 791/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6921 - val_mean_squared_error: 0.6921\n",
      "Epoch 792/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 793/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5263 - mean_squared_error: 0.5263 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 794/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 795/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6900 - val_mean_squared_error: 0.6900\n",
      "Epoch 796/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 797/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6906 - val_mean_squared_error: 0.6906\n",
      "Epoch 798/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6871 - val_mean_squared_error: 0.6871\n",
      "Epoch 799/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 800/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 801/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 802/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6874 - val_mean_squared_error: 0.6874\n",
      "Epoch 803/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 804/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 805/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 806/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 807/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 808/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 809/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6902 - val_mean_squared_error: 0.6902\n",
      "Epoch 810/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 811/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 812/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6866 - val_mean_squared_error: 0.6866\n",
      "Epoch 813/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6873 - val_mean_squared_error: 0.6873\n",
      "Epoch 814/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 815/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6869 - val_mean_squared_error: 0.6869\n",
      "Epoch 816/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 817/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 818/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 819/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6861 - val_mean_squared_error: 0.6861\n",
      "Epoch 820/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 821/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 822/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6862 - val_mean_squared_error: 0.6862\n",
      "Epoch 823/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 824/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 825/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6874 - val_mean_squared_error: 0.6874\n",
      "Epoch 826/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 827/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 828/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 829/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 830/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 831/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 832/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 833/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6870 - val_mean_squared_error: 0.6870\n",
      "Epoch 834/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 835/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6874 - val_mean_squared_error: 0.6874\n",
      "Epoch 836/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 837/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6901 - val_mean_squared_error: 0.6901\n",
      "Epoch 838/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 839/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 840/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 841/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 842/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 843/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 844/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 845/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 846/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 847/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 848/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 849/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 850/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5260 - mean_squared_error: 0.5260 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 851/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6906 - val_mean_squared_error: 0.6906\n",
      "Epoch 852/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 853/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 854/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 855/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 856/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 857/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 858/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 859/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 860/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 861/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 862/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 863/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 864/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6915 - val_mean_squared_error: 0.6915\n",
      "Epoch 865/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 866/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 867/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 868/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 869/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 870/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6873 - val_mean_squared_error: 0.6873\n",
      "Epoch 871/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 872/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 873/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 874/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 875/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 876/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 877/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 878/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5261 - mean_squared_error: 0.5261 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 879/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 880/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 881/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 882/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6874 - val_mean_squared_error: 0.6874\n",
      "Epoch 883/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 884/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 885/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 886/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 887/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6906 - val_mean_squared_error: 0.6906\n",
      "Epoch 888/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 889/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 890/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 891/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6902 - val_mean_squared_error: 0.6902\n",
      "Epoch 892/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 893/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 894/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 895/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 896/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 897/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 898/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6880 - val_mean_squared_error: 0.6880\n",
      "Epoch 899/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 900/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 901/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6903 - val_mean_squared_error: 0.6903\n",
      "Epoch 902/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 903/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 904/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6903 - val_mean_squared_error: 0.6903\n",
      "Epoch 905/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 906/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 907/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6901 - val_mean_squared_error: 0.6901\n",
      "Epoch 908/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 909/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 910/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 911/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 912/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 913/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 914/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 915/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 916/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 917/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 918/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 919/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 920/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 921/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 922/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 923/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 924/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 925/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 926/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 927/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_squared_error: 0.5259 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 928/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 929/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 930/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 931/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 932/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 933/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 934/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 935/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 936/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 937/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 938/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6912 - val_mean_squared_error: 0.6912\n",
      "Epoch 939/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 940/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 941/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 942/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6901 - val_mean_squared_error: 0.6901\n",
      "Epoch 943/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 944/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 945/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 946/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 947/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6904 - val_mean_squared_error: 0.6904\n",
      "Epoch 948/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 949/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6907 - val_mean_squared_error: 0.6907\n",
      "Epoch 950/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 951/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 952/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 953/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6890 - val_mean_squared_error: 0.6889\n",
      "Epoch 954/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 955/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 956/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 957/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 958/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 959/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 960/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 961/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 962/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 963/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 964/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 965/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5258 - mean_squared_error: 0.5258 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 966/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 967/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 968/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 969/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 970/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6903 - val_mean_squared_error: 0.6903\n",
      "Epoch 971/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 972/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 973/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 974/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 975/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 976/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6911 - val_mean_squared_error: 0.6911\n",
      "Epoch 977/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 978/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6901 - val_mean_squared_error: 0.6901\n",
      "Epoch 979/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 980/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 981/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 982/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6912 - val_mean_squared_error: 0.6912\n",
      "Epoch 983/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 984/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 985/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 986/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6905 - val_mean_squared_error: 0.6905\n",
      "Epoch 987/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 988/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 989/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 990/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 991/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6873 - val_mean_squared_error: 0.6873\n",
      "Epoch 992/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 993/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 994/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 995/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 996/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6868 - val_mean_squared_error: 0.6868\n",
      "Epoch 997/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 998/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 999/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 1000/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 1001/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 1002/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 1003/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 1004/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6885 - val_mean_squared_error: 0.6885\n",
      "Epoch 1005/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 1006/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 1007/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6885 - val_mean_squared_error: 0.6884\n",
      "Epoch 1008/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 1009/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 1010/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6892 - val_mean_squared_error: 0.6892\n",
      "Epoch 1011/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 1012/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5257 - mean_squared_error: 0.5257 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 1013/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 1014/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1015/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5253 - mean_squared_error: 0.5253 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 1016/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 1017/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 1018/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6878 - val_mean_squared_error: 0.6877\n",
      "Epoch 1019/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 1020/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 1021/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 1022/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 1023/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6872 - val_mean_squared_error: 0.6872\n",
      "Epoch 1024/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 1025/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 1026/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 1027/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6884 - val_mean_squared_error: 0.6884\n",
      "Epoch 1028/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 1029/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 1030/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6875 - val_mean_squared_error: 0.6875\n",
      "Epoch 1031/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 1032/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6895 - val_mean_squared_error: 0.6895\n",
      "Epoch 1033/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6878 - val_mean_squared_error: 0.6878\n",
      "Epoch 1034/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 1035/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1036/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1037/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 1038/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6881 - val_mean_squared_error: 0.6881\n",
      "Epoch 1039/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 1040/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6879 - val_mean_squared_error: 0.6879\n",
      "Epoch 1041/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 1042/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6887 - val_mean_squared_error: 0.6887\n",
      "Epoch 1043/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6898 - val_mean_squared_error: 0.6898\n",
      "Epoch 1044/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6890 - val_mean_squared_error: 0.6890\n",
      "Epoch 1045/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 1046/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 1047/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6899 - val_mean_squared_error: 0.6899\n",
      "Epoch 1048/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 1049/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5254 - mean_squared_error: 0.5254 - val_loss: 0.6897 - val_mean_squared_error: 0.6897\n",
      "Epoch 1050/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 1051/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6896 - val_mean_squared_error: 0.6896\n",
      "Epoch 1052/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6886 - val_mean_squared_error: 0.6886\n",
      "Epoch 1053/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1054/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6894 - val_mean_squared_error: 0.6894\n",
      "Epoch 1055/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 1056/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6893 - val_mean_squared_error: 0.6893\n",
      "Epoch 1057/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6882 - val_mean_squared_error: 0.6882\n",
      "Epoch 1058/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1059/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6888 - val_mean_squared_error: 0.6888\n",
      "Epoch 1060/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6905 - val_mean_squared_error: 0.6904\n",
      "Epoch 1061/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6896 - val_mean_squared_error: 0.6895\n",
      "Epoch 1062/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 1063/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6876 - val_mean_squared_error: 0.6876\n",
      "Epoch 1064/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6891 - val_mean_squared_error: 0.6891\n",
      "Epoch 1065/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "Epoch 1066/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5256 - mean_squared_error: 0.5256 - val_loss: 0.6889 - val_mean_squared_error: 0.6889\n",
      "Epoch 1067/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6877 - val_mean_squared_error: 0.6877\n",
      "Epoch 1068/10000\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5255 - mean_squared_error: 0.5255 - val_loss: 0.6883 - val_mean_squared_error: 0.6883\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.5403 - mean_squared_error: 0.5403\n"
     ]
    }
   ],
   "source": [
    "model_history_model_1L, score_model_factor_1L= compile_and_fit_simple('factors',checkpoints_model_1L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit', class_weights_dict)\n",
    "model_history_model_2L, score_model_factor_2L= compile_and_fit_simple('factors',checkpoints_model_2L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)\n",
    "model_history_model_3L, score_model_factor_3L = compile_and_fit_simple('factors',checkpoints_model_3L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)\n",
    "model_history_model_4L, score_model_factor_4L = compile_and_fit_simple('factors',checkpoints_model_4L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "579b7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_alternative/checkpoints_model_1L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_1L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_2L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_2L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_3L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_3L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_4L_alternative_final.h5', 'checkpoints_alternative/checkpoints_model_4L_alternative_final.h5')\n",
    "\n",
    "file_saver('checkpoints_alternative/checkpoints_model_1L_alternative.h5', 'checkpoints_alternative/checkpoints_model_1L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_2L_alternative.h5', 'checkpoints_alternative/checkpoints_model_2L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_3L_alternative.h5', 'checkpoints_alternative/checkpoints_model_3L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_model_4L_alternative.h5', 'checkpoints_alternative/checkpoints_model_4L_alternative.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c09e2",
   "metadata": {},
   "source": [
    "### LSTM merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f885cf70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1512 - mean_squared_error: 0.1508 - val_loss: 0.5194 - val_mean_squared_error: 0.5190\n",
      "Epoch 766/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5277 - val_mean_squared_error: 0.5273\n",
      "Epoch 767/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1511 - mean_squared_error: 0.1507 - val_loss: 0.5295 - val_mean_squared_error: 0.5291\n",
      "Epoch 768/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5270 - val_mean_squared_error: 0.5266\n",
      "Epoch 769/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1512 - mean_squared_error: 0.1508 - val_loss: 0.5322 - val_mean_squared_error: 0.5318\n",
      "Epoch 770/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1511 - mean_squared_error: 0.1507 - val_loss: 0.5115 - val_mean_squared_error: 0.5111\n",
      "Epoch 771/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5369 - val_mean_squared_error: 0.5365\n",
      "Epoch 772/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5431 - val_mean_squared_error: 0.5427\n",
      "Epoch 773/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1511 - mean_squared_error: 0.1508 - val_loss: 0.5487 - val_mean_squared_error: 0.5483\n",
      "Epoch 774/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1516 - mean_squared_error: 0.1512 - val_loss: 0.5438 - val_mean_squared_error: 0.5434\n",
      "Epoch 775/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1519 - mean_squared_error: 0.1516 - val_loss: 0.5246 - val_mean_squared_error: 0.5242\n",
      "Epoch 776/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1515 - mean_squared_error: 0.1512 - val_loss: 0.5306 - val_mean_squared_error: 0.5302\n",
      "Epoch 777/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5386 - val_mean_squared_error: 0.5382\n",
      "Epoch 778/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5353 - val_mean_squared_error: 0.5349\n",
      "Epoch 779/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1510 - val_loss: 0.5260 - val_mean_squared_error: 0.5256\n",
      "Epoch 780/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1516 - mean_squared_error: 0.1513 - val_loss: 0.5315 - val_mean_squared_error: 0.5311\n",
      "Epoch 781/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1514 - mean_squared_error: 0.1510 - val_loss: 0.5167 - val_mean_squared_error: 0.5164\n",
      "Epoch 782/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1512 - mean_squared_error: 0.1508 - val_loss: 0.5330 - val_mean_squared_error: 0.5327\n",
      "Epoch 783/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1514 - mean_squared_error: 0.1510 - val_loss: 0.5203 - val_mean_squared_error: 0.5199\n",
      "Epoch 784/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1511 - mean_squared_error: 0.1507 - val_loss: 0.5203 - val_mean_squared_error: 0.5199\n",
      "Epoch 785/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1513 - mean_squared_error: 0.1509 - val_loss: 0.5215 - val_mean_squared_error: 0.5211\n",
      "Epoch 786/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1512 - mean_squared_error: 0.1509 - val_loss: 0.5099 - val_mean_squared_error: 0.5096\n",
      "Epoch 787/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1514 - mean_squared_error: 0.1510 - val_loss: 0.5270 - val_mean_squared_error: 0.5266\n",
      "Epoch 788/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1512 - mean_squared_error: 0.1508 - val_loss: 0.5060 - val_mean_squared_error: 0.5057\n",
      "Epoch 789/5000\n",
      "94/94 [==============================] - 1s 11ms/step - loss: 0.1511 - mean_squared_error: 0.1508 - val_loss: 0.5204 - val_mean_squared_error: 0.5200\n",
      "103/103 [==============================] - 0s 3ms/step - loss: 0.1843 - mean_squared_error: 0.1839\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"compile_and_fit_simple(model_type,checkpoint_path,config,stock_data, x_train_factors,x_train_macro,x_train_merged,y_train, patience, batch_size, num_epochs, goal, dict):\"\"\"\n",
    "\n",
    "\n",
    "model_history_LSTM_model_merged_1L, score_LSTM_model_merged_1L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_1L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000, 'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_2L, score_LSTM_model_merged_2L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_2L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_3L,score_LSTM_model_merged_3L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_3L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_merged_4L, score_LSTM_model_merged_4L = compile_and_fit_simple('merged',checkpoints_LSTM_model_merged_4L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "65975415",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_1L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_1L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_2L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_2L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_3L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_3L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_4L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_4L_alternative_final.h5')\n",
    "\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_1L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_1L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_2L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_2L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_3L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_3L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_merged_4L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_merged_4L_alternative.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2e55c",
   "metadata": {},
   "source": [
    "### Factor LSTM NNs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff64456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "94/94 [==============================] - 4s 12ms/step - loss: 4.0569 - mean_squared_error: 4.0342 - val_loss: 1.3325 - val_mean_squared_error: 1.3093\n",
      "Epoch 2/5000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.5748 - mean_squared_error: 0.5520 - val_loss: 1.0615 - val_mean_squared_error: 1.0390\n",
      "Epoch 3/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.5620 - mean_squared_error: 0.5398 - val_loss: 1.0730 - val_mean_squared_error: 1.0511\n",
      "Epoch 4/5000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.5602 - mean_squared_error: 0.5386 - val_loss: 0.9556 - val_mean_squared_error: 0.9344\n",
      "Epoch 5/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.5527 - mean_squared_error: 0.5319 - val_loss: 0.9842 - val_mean_squared_error: 0.9637\n",
      "Epoch 6/5000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.5460 - mean_squared_error: 0.5258 - val_loss: 0.9312 - val_mean_squared_error: 0.9114\n",
      "Epoch 7/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.5419 - mean_squared_error: 0.5225 - val_loss: 0.9404 - val_mean_squared_error: 0.9212\n",
      "Epoch 8/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.5352 - mean_squared_error: 0.5162 - val_loss: 0.9552 - val_mean_squared_error: 0.9366\n",
      "Epoch 9/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.5139 - mean_squared_error: 0.4955 - val_loss: 0.9543 - val_mean_squared_error: 0.9360\n",
      "Epoch 10/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.4807 - mean_squared_error: 0.4624 - val_loss: 0.9610 - val_mean_squared_error: 0.9428\n",
      "Epoch 11/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.4460 - mean_squared_error: 0.4280 - val_loss: 0.9423 - val_mean_squared_error: 0.9245\n",
      "Epoch 12/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.4204 - mean_squared_error: 0.4026 - val_loss: 0.9621 - val_mean_squared_error: 0.9444\n",
      "Epoch 13/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.3868 - mean_squared_error: 0.3692 - val_loss: 0.9827 - val_mean_squared_error: 0.9652\n",
      "Epoch 14/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.3523 - mean_squared_error: 0.3349 - val_loss: 0.9661 - val_mean_squared_error: 0.9488\n",
      "Epoch 15/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.3297 - mean_squared_error: 0.3126 - val_loss: 0.9894 - val_mean_squared_error: 0.9723\n",
      "Epoch 16/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.3095 - mean_squared_error: 0.2926 - val_loss: 0.9467 - val_mean_squared_error: 0.9299\n",
      "Epoch 17/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2929 - mean_squared_error: 0.2763 - val_loss: 0.9604 - val_mean_squared_error: 0.9439\n",
      "Epoch 18/5000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.2851 - mean_squared_error: 0.2688 - val_loss: 0.9115 - val_mean_squared_error: 0.8954\n",
      "Epoch 19/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2725 - mean_squared_error: 0.2565 - val_loss: 0.9422 - val_mean_squared_error: 0.9263\n",
      "Epoch 20/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2656 - mean_squared_error: 0.2499 - val_loss: 0.9137 - val_mean_squared_error: 0.8981\n",
      "Epoch 21/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2572 - mean_squared_error: 0.2417 - val_loss: 0.9377 - val_mean_squared_error: 0.9224\n",
      "Epoch 22/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2500 - mean_squared_error: 0.2349 - val_loss: 0.9160 - val_mean_squared_error: 0.9010\n",
      "Epoch 23/5000\n",
      "94/94 [==============================] - 0s 5ms/step - loss: 0.2432 - mean_squared_error: 0.2283 - val_loss: 0.9079 - val_mean_squared_error: 0.8932\n",
      "Epoch 24/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2380 - mean_squared_error: 0.2234 - val_loss: 0.9358 - val_mean_squared_error: 0.9212\n",
      "Epoch 25/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2308 - mean_squared_error: 0.2164 - val_loss: 0.9413 - val_mean_squared_error: 0.9270\n",
      "Epoch 26/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2248 - mean_squared_error: 0.2107 - val_loss: 0.9512 - val_mean_squared_error: 0.9372\n",
      "Epoch 27/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2183 - mean_squared_error: 0.2044 - val_loss: 0.9717 - val_mean_squared_error: 0.9578\n",
      "Epoch 28/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2153 - mean_squared_error: 0.2016 - val_loss: 0.9284 - val_mean_squared_error: 0.9148\n",
      "Epoch 29/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2122 - mean_squared_error: 0.1987 - val_loss: 0.9387 - val_mean_squared_error: 0.9253\n",
      "Epoch 30/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.2055 - mean_squared_error: 0.1922 - val_loss: 0.9647 - val_mean_squared_error: 0.9515\n",
      "Epoch 31/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1992 - mean_squared_error: 0.1860 - val_loss: 0.9727 - val_mean_squared_error: 0.9597\n",
      "Epoch 32/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1981 - mean_squared_error: 0.1851 - val_loss: 0.9571 - val_mean_squared_error: 0.9442\n",
      "Epoch 33/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1931 - mean_squared_error: 0.1802 - val_loss: 0.9892 - val_mean_squared_error: 0.9765\n",
      "Epoch 34/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1890 - mean_squared_error: 0.1763 - val_loss: 0.9678 - val_mean_squared_error: 0.9552\n",
      "Epoch 35/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1840 - mean_squared_error: 0.1714 - val_loss: 1.0074 - val_mean_squared_error: 0.9948\n",
      "Epoch 36/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1808 - mean_squared_error: 0.1683 - val_loss: 1.0332 - val_mean_squared_error: 1.0208\n",
      "Epoch 37/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1756 - mean_squared_error: 0.1633 - val_loss: 1.0222 - val_mean_squared_error: 1.0099\n",
      "Epoch 38/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1703 - mean_squared_error: 0.1581 - val_loss: 1.0136 - val_mean_squared_error: 1.0014\n",
      "Epoch 39/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1662 - mean_squared_error: 0.1540 - val_loss: 1.0353 - val_mean_squared_error: 1.0232\n",
      "Epoch 40/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1638 - mean_squared_error: 0.1518 - val_loss: 1.0295 - val_mean_squared_error: 1.0175\n",
      "Epoch 41/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1596 - mean_squared_error: 0.1478 - val_loss: 1.0134 - val_mean_squared_error: 1.0017\n",
      "Epoch 42/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1559 - mean_squared_error: 0.1441 - val_loss: 1.0751 - val_mean_squared_error: 1.0635\n",
      "Epoch 43/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1542 - mean_squared_error: 0.1426 - val_loss: 1.0438 - val_mean_squared_error: 1.0323\n",
      "Epoch 44/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1515 - mean_squared_error: 0.1401 - val_loss: 1.1109 - val_mean_squared_error: 1.0995\n",
      "Epoch 45/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1481 - mean_squared_error: 0.1368 - val_loss: 1.1091 - val_mean_squared_error: 1.0979\n",
      "Epoch 46/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1485 - mean_squared_error: 0.1373 - val_loss: 1.0413 - val_mean_squared_error: 1.0302\n",
      "Epoch 47/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1443 - mean_squared_error: 0.1332 - val_loss: 1.1242 - val_mean_squared_error: 1.1131\n",
      "Epoch 48/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1430 - mean_squared_error: 0.1321 - val_loss: 1.1393 - val_mean_squared_error: 1.1284\n",
      "Epoch 49/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1400 - mean_squared_error: 0.1291 - val_loss: 1.0803 - val_mean_squared_error: 1.0696\n",
      "Epoch 50/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1370 - mean_squared_error: 0.1263 - val_loss: 1.0911 - val_mean_squared_error: 1.0805\n",
      "Epoch 51/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1354 - mean_squared_error: 0.1248 - val_loss: 1.1015 - val_mean_squared_error: 1.0910\n",
      "Epoch 52/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1328 - mean_squared_error: 0.1224 - val_loss: 1.0649 - val_mean_squared_error: 1.0545\n",
      "Epoch 53/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1320 - mean_squared_error: 0.1216 - val_loss: 1.0832 - val_mean_squared_error: 1.0730\n",
      "Epoch 54/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1322 - mean_squared_error: 0.1220 - val_loss: 1.0938 - val_mean_squared_error: 1.0837\n",
      "Epoch 55/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1304 - mean_squared_error: 0.1203 - val_loss: 1.1307 - val_mean_squared_error: 1.1207\n",
      "Epoch 56/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1296 - mean_squared_error: 0.1195 - val_loss: 1.1357 - val_mean_squared_error: 1.1258\n",
      "Epoch 57/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1270 - mean_squared_error: 0.1171 - val_loss: 1.0480 - val_mean_squared_error: 1.0382\n",
      "Epoch 58/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1239 - mean_squared_error: 0.1141 - val_loss: 1.1092 - val_mean_squared_error: 1.0994\n",
      "Epoch 59/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1217 - mean_squared_error: 0.1121 - val_loss: 1.1412 - val_mean_squared_error: 1.1316\n",
      "Epoch 60/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1220 - mean_squared_error: 0.1124 - val_loss: 1.1136 - val_mean_squared_error: 1.1041\n",
      "Epoch 61/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1210 - mean_squared_error: 0.1116 - val_loss: 1.0995 - val_mean_squared_error: 1.0902\n",
      "Epoch 62/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1205 - mean_squared_error: 0.1111 - val_loss: 1.1054 - val_mean_squared_error: 1.0961\n",
      "Epoch 63/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1180 - mean_squared_error: 0.1087 - val_loss: 1.1091 - val_mean_squared_error: 1.0999\n",
      "Epoch 64/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1178 - mean_squared_error: 0.1087 - val_loss: 1.0991 - val_mean_squared_error: 1.0900\n",
      "Epoch 65/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1168 - mean_squared_error: 0.1078 - val_loss: 1.1022 - val_mean_squared_error: 1.0932\n",
      "Epoch 66/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1154 - mean_squared_error: 0.1065 - val_loss: 1.1156 - val_mean_squared_error: 1.1068\n",
      "Epoch 67/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1153 - mean_squared_error: 0.1065 - val_loss: 1.0728 - val_mean_squared_error: 1.0640\n",
      "Epoch 68/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1148 - mean_squared_error: 0.1061 - val_loss: 1.0998 - val_mean_squared_error: 1.0911\n",
      "Epoch 69/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1147 - mean_squared_error: 0.1061 - val_loss: 1.0785 - val_mean_squared_error: 1.0699\n",
      "Epoch 70/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1119 - mean_squared_error: 0.1033 - val_loss: 1.0855 - val_mean_squared_error: 1.0770\n",
      "Epoch 71/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1108 - mean_squared_error: 0.1023 - val_loss: 1.1002 - val_mean_squared_error: 1.0917\n",
      "Epoch 72/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1114 - mean_squared_error: 0.1030 - val_loss: 1.0691 - val_mean_squared_error: 1.0608\n",
      "Epoch 73/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1122 - mean_squared_error: 0.1039 - val_loss: 1.1278 - val_mean_squared_error: 1.1195\n",
      "Epoch 74/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1110 - mean_squared_error: 0.1027 - val_loss: 1.0824 - val_mean_squared_error: 1.0741\n",
      "Epoch 75/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1081 - mean_squared_error: 0.0999 - val_loss: 1.0858 - val_mean_squared_error: 1.0777\n",
      "Epoch 76/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1067 - mean_squared_error: 0.0986 - val_loss: 1.0846 - val_mean_squared_error: 1.0765\n",
      "Epoch 77/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1074 - mean_squared_error: 0.0994 - val_loss: 1.1134 - val_mean_squared_error: 1.1054\n",
      "Epoch 78/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1080 - mean_squared_error: 0.1000 - val_loss: 1.1219 - val_mean_squared_error: 1.1139\n",
      "Epoch 79/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1060 - mean_squared_error: 0.0980 - val_loss: 1.1096 - val_mean_squared_error: 1.1017\n",
      "Epoch 80/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1043 - mean_squared_error: 0.0964 - val_loss: 1.0971 - val_mean_squared_error: 1.0893\n",
      "Epoch 81/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1025 - mean_squared_error: 0.0947 - val_loss: 1.1047 - val_mean_squared_error: 1.0969\n",
      "Epoch 82/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1000 - mean_squared_error: 0.0923 - val_loss: 1.1039 - val_mean_squared_error: 1.0963\n",
      "Epoch 83/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0993 - mean_squared_error: 0.0917 - val_loss: 1.0993 - val_mean_squared_error: 1.0918\n",
      "Epoch 84/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0995 - mean_squared_error: 0.0920 - val_loss: 1.0987 - val_mean_squared_error: 1.0913\n",
      "Epoch 85/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1008 - mean_squared_error: 0.0934 - val_loss: 1.1389 - val_mean_squared_error: 1.1316\n",
      "Epoch 86/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1015 - mean_squared_error: 0.0941 - val_loss: 1.1008 - val_mean_squared_error: 1.0935\n",
      "Epoch 87/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1003 - mean_squared_error: 0.0930 - val_loss: 1.1082 - val_mean_squared_error: 1.1010\n",
      "Epoch 88/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1005 - mean_squared_error: 0.0933 - val_loss: 1.0697 - val_mean_squared_error: 1.0625\n",
      "Epoch 89/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1005 - mean_squared_error: 0.0933 - val_loss: 1.1126 - val_mean_squared_error: 1.1055\n",
      "Epoch 90/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1024 - mean_squared_error: 0.0952 - val_loss: 1.0921 - val_mean_squared_error: 1.0849\n",
      "Epoch 91/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1006 - mean_squared_error: 0.0935 - val_loss: 1.1037 - val_mean_squared_error: 1.0967\n",
      "Epoch 92/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0984 - mean_squared_error: 0.0914 - val_loss: 1.0826 - val_mean_squared_error: 1.0756\n",
      "Epoch 93/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0970 - mean_squared_error: 0.0900 - val_loss: 1.0816 - val_mean_squared_error: 1.0747\n",
      "Epoch 94/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - mean_squared_error: 0.0884 - val_loss: 1.0852 - val_mean_squared_error: 1.0783\n",
      "Epoch 95/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0953 - mean_squared_error: 0.0885 - val_loss: 1.1269 - val_mean_squared_error: 1.1201\n",
      "Epoch 96/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0957 - mean_squared_error: 0.0889 - val_loss: 1.1062 - val_mean_squared_error: 1.0995\n",
      "Epoch 97/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0946 - mean_squared_error: 0.0879 - val_loss: 1.0962 - val_mean_squared_error: 1.0895\n",
      "Epoch 98/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0929 - mean_squared_error: 0.0863 - val_loss: 1.1080 - val_mean_squared_error: 1.1013\n",
      "Epoch 99/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0929 - mean_squared_error: 0.0863 - val_loss: 1.1079 - val_mean_squared_error: 1.1013\n",
      "Epoch 100/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0927 - mean_squared_error: 0.0862 - val_loss: 1.1094 - val_mean_squared_error: 1.1029\n",
      "Epoch 101/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0927 - mean_squared_error: 0.0863 - val_loss: 1.1001 - val_mean_squared_error: 1.0937\n",
      "Epoch 102/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0928 - mean_squared_error: 0.0863 - val_loss: 1.1072 - val_mean_squared_error: 1.1009\n",
      "Epoch 103/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0929 - mean_squared_error: 0.0865 - val_loss: 1.1227 - val_mean_squared_error: 1.1164\n",
      "Epoch 104/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0932 - mean_squared_error: 0.0869 - val_loss: 1.0933 - val_mean_squared_error: 1.0870\n",
      "Epoch 105/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0927 - mean_squared_error: 0.0864 - val_loss: 1.1161 - val_mean_squared_error: 1.1098\n",
      "Epoch 106/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0911 - mean_squared_error: 0.0849 - val_loss: 1.0964 - val_mean_squared_error: 1.0902\n",
      "Epoch 107/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0927 - mean_squared_error: 0.0866 - val_loss: 1.0749 - val_mean_squared_error: 1.0688\n",
      "Epoch 108/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0923 - mean_squared_error: 0.0861 - val_loss: 1.0873 - val_mean_squared_error: 1.0812\n",
      "Epoch 109/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0919 - mean_squared_error: 0.0858 - val_loss: 1.0786 - val_mean_squared_error: 1.0725\n",
      "Epoch 110/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0936 - mean_squared_error: 0.0876 - val_loss: 1.0713 - val_mean_squared_error: 1.0652\n",
      "Epoch 111/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0925 - mean_squared_error: 0.0864 - val_loss: 1.0836 - val_mean_squared_error: 1.0776\n",
      "Epoch 112/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0908 - mean_squared_error: 0.0848 - val_loss: 1.0811 - val_mean_squared_error: 1.0751\n",
      "Epoch 113/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0905 - mean_squared_error: 0.0846 - val_loss: 1.0664 - val_mean_squared_error: 1.0605\n",
      "Epoch 114/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0880 - mean_squared_error: 0.0821 - val_loss: 1.0712 - val_mean_squared_error: 1.0653\n",
      "Epoch 115/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0870 - mean_squared_error: 0.0811 - val_loss: 1.0647 - val_mean_squared_error: 1.0589\n",
      "Epoch 116/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0886 - mean_squared_error: 0.0828 - val_loss: 1.0873 - val_mean_squared_error: 1.0815\n",
      "Epoch 117/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0887 - mean_squared_error: 0.0829 - val_loss: 1.0861 - val_mean_squared_error: 1.0803\n",
      "Epoch 118/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0866 - mean_squared_error: 0.0808 - val_loss: 1.0870 - val_mean_squared_error: 1.0812\n",
      "Epoch 119/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0862 - mean_squared_error: 0.0805 - val_loss: 1.0703 - val_mean_squared_error: 1.0646\n",
      "Epoch 120/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0854 - mean_squared_error: 0.0798 - val_loss: 1.0933 - val_mean_squared_error: 1.0876\n",
      "Epoch 121/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0856 - mean_squared_error: 0.0800 - val_loss: 1.0887 - val_mean_squared_error: 1.0831\n",
      "Epoch 122/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0845 - mean_squared_error: 0.0790 - val_loss: 1.0881 - val_mean_squared_error: 1.0825\n",
      "Epoch 123/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0846 - mean_squared_error: 0.0791 - val_loss: 1.0569 - val_mean_squared_error: 1.0514\n",
      "Epoch 124/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0874 - mean_squared_error: 0.0819 - val_loss: 1.1165 - val_mean_squared_error: 1.1110\n",
      "Epoch 125/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0969 - mean_squared_error: 0.0914 - val_loss: 1.0772 - val_mean_squared_error: 1.0717\n",
      "Epoch 126/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0923 - mean_squared_error: 0.0868 - val_loss: 1.0808 - val_mean_squared_error: 1.0753\n",
      "Epoch 127/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0865 - mean_squared_error: 0.0809 - val_loss: 1.0603 - val_mean_squared_error: 1.0548\n",
      "Epoch 128/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0838 - mean_squared_error: 0.0784 - val_loss: 1.0702 - val_mean_squared_error: 1.0648\n",
      "Epoch 129/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0823 - mean_squared_error: 0.0768 - val_loss: 1.0610 - val_mean_squared_error: 1.0556\n",
      "Epoch 130/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0821 - mean_squared_error: 0.0767 - val_loss: 1.0892 - val_mean_squared_error: 1.0839\n",
      "Epoch 131/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0826 - mean_squared_error: 0.0773 - val_loss: 1.0705 - val_mean_squared_error: 1.0652\n",
      "Epoch 132/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0838 - mean_squared_error: 0.0785 - val_loss: 1.1082 - val_mean_squared_error: 1.1030\n",
      "Epoch 133/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0869 - mean_squared_error: 0.0817 - val_loss: 1.0676 - val_mean_squared_error: 1.0623\n",
      "Epoch 134/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0849 - mean_squared_error: 0.0796 - val_loss: 1.0439 - val_mean_squared_error: 1.0386\n",
      "Epoch 135/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0845 - mean_squared_error: 0.0792 - val_loss: 1.0716 - val_mean_squared_error: 1.0663\n",
      "Epoch 136/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0827 - mean_squared_error: 0.0775 - val_loss: 1.0655 - val_mean_squared_error: 1.0604\n",
      "Epoch 137/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0813 - mean_squared_error: 0.0762 - val_loss: 1.0691 - val_mean_squared_error: 1.0640\n",
      "Epoch 138/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0803 - mean_squared_error: 0.0752 - val_loss: 1.0670 - val_mean_squared_error: 1.0619\n",
      "Epoch 139/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0799 - mean_squared_error: 0.0748 - val_loss: 1.0703 - val_mean_squared_error: 1.0652\n",
      "Epoch 140/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0800 - mean_squared_error: 0.0750 - val_loss: 1.0720 - val_mean_squared_error: 1.0670\n",
      "Epoch 141/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0804 - mean_squared_error: 0.0754 - val_loss: 1.0761 - val_mean_squared_error: 1.0711\n",
      "Epoch 142/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0804 - mean_squared_error: 0.0754 - val_loss: 1.0717 - val_mean_squared_error: 1.0667\n",
      "Epoch 143/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0848 - mean_squared_error: 0.0798 - val_loss: 1.0864 - val_mean_squared_error: 1.0815\n",
      "Epoch 144/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0837 - mean_squared_error: 0.0788 - val_loss: 1.0638 - val_mean_squared_error: 1.0589\n",
      "Epoch 145/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0817 - mean_squared_error: 0.0768 - val_loss: 1.0641 - val_mean_squared_error: 1.0592\n",
      "Epoch 146/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0840 - mean_squared_error: 0.0791 - val_loss: 1.0635 - val_mean_squared_error: 1.0586\n",
      "Epoch 147/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0825 - mean_squared_error: 0.0777 - val_loss: 1.0537 - val_mean_squared_error: 1.0488\n",
      "Epoch 148/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0830 - mean_squared_error: 0.0781 - val_loss: 1.0683 - val_mean_squared_error: 1.0635\n",
      "Epoch 149/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0815 - mean_squared_error: 0.0767 - val_loss: 1.0389 - val_mean_squared_error: 1.0341\n",
      "Epoch 150/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0803 - mean_squared_error: 0.0755 - val_loss: 1.0678 - val_mean_squared_error: 1.0630\n",
      "Epoch 151/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0833 - mean_squared_error: 0.0785 - val_loss: 1.0359 - val_mean_squared_error: 1.0311\n",
      "Epoch 152/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0817 - mean_squared_error: 0.0769 - val_loss: 1.0282 - val_mean_squared_error: 1.0234\n",
      "Epoch 153/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0804 - mean_squared_error: 0.0756 - val_loss: 1.0815 - val_mean_squared_error: 1.0768\n",
      "Epoch 154/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0800 - mean_squared_error: 0.0753 - val_loss: 1.0567 - val_mean_squared_error: 1.0520\n",
      "Epoch 155/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0796 - mean_squared_error: 0.0749 - val_loss: 1.0589 - val_mean_squared_error: 1.0542\n",
      "Epoch 156/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0783 - mean_squared_error: 0.0736 - val_loss: 1.0617 - val_mean_squared_error: 1.0571\n",
      "Epoch 157/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0780 - mean_squared_error: 0.0734 - val_loss: 1.0611 - val_mean_squared_error: 1.0565\n",
      "Epoch 158/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0774 - mean_squared_error: 0.0728 - val_loss: 1.0751 - val_mean_squared_error: 1.0705\n",
      "Epoch 159/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0770 - mean_squared_error: 0.0725 - val_loss: 1.0588 - val_mean_squared_error: 1.0542\n",
      "Epoch 160/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0786 - mean_squared_error: 0.0741 - val_loss: 1.0689 - val_mean_squared_error: 1.0644\n",
      "Epoch 161/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0793 - mean_squared_error: 0.0748 - val_loss: 1.0659 - val_mean_squared_error: 1.0614\n",
      "Epoch 162/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0795 - mean_squared_error: 0.0750 - val_loss: 1.0510 - val_mean_squared_error: 1.0465\n",
      "Epoch 163/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0829 - mean_squared_error: 0.0784 - val_loss: 1.0538 - val_mean_squared_error: 1.0493\n",
      "Epoch 164/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0803 - mean_squared_error: 0.0758 - val_loss: 1.0585 - val_mean_squared_error: 1.0540\n",
      "Epoch 165/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0779 - mean_squared_error: 0.0734 - val_loss: 1.0440 - val_mean_squared_error: 1.0395\n",
      "Epoch 166/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0788 - mean_squared_error: 0.0743 - val_loss: 1.0462 - val_mean_squared_error: 1.0418\n",
      "Epoch 167/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0787 - mean_squared_error: 0.0743 - val_loss: 1.0652 - val_mean_squared_error: 1.0608\n",
      "Epoch 168/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0776 - mean_squared_error: 0.0732 - val_loss: 1.0521 - val_mean_squared_error: 1.0477\n",
      "Epoch 169/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0772 - mean_squared_error: 0.0728 - val_loss: 1.0333 - val_mean_squared_error: 1.0289\n",
      "Epoch 170/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0774 - mean_squared_error: 0.0730 - val_loss: 1.0805 - val_mean_squared_error: 1.0761\n",
      "Epoch 171/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0770 - mean_squared_error: 0.0727 - val_loss: 1.0669 - val_mean_squared_error: 1.0626\n",
      "Epoch 172/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0760 - mean_squared_error: 0.0717 - val_loss: 1.0518 - val_mean_squared_error: 1.0475\n",
      "Epoch 173/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0760 - mean_squared_error: 0.0717 - val_loss: 1.0676 - val_mean_squared_error: 1.0633\n",
      "Epoch 174/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0755 - mean_squared_error: 0.0713 - val_loss: 1.0589 - val_mean_squared_error: 1.0547\n",
      "Epoch 175/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0757 - mean_squared_error: 0.0715 - val_loss: 1.0360 - val_mean_squared_error: 1.0318\n",
      "Epoch 176/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0753 - mean_squared_error: 0.0711 - val_loss: 1.0466 - val_mean_squared_error: 1.0425\n",
      "Epoch 177/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0763 - mean_squared_error: 0.0722 - val_loss: 1.0537 - val_mean_squared_error: 1.0495\n",
      "Epoch 178/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0788 - mean_squared_error: 0.0746 - val_loss: 1.0523 - val_mean_squared_error: 1.0481\n",
      "Epoch 179/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0821 - mean_squared_error: 0.0779 - val_loss: 1.0360 - val_mean_squared_error: 1.0318\n",
      "Epoch 180/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0842 - mean_squared_error: 0.0800 - val_loss: 1.0461 - val_mean_squared_error: 1.0419\n",
      "Epoch 181/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0801 - mean_squared_error: 0.0758 - val_loss: 1.0292 - val_mean_squared_error: 1.0250\n",
      "Epoch 182/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0795 - mean_squared_error: 0.0753 - val_loss: 1.0128 - val_mean_squared_error: 1.0086\n",
      "Epoch 183/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0764 - mean_squared_error: 0.0722 - val_loss: 1.0467 - val_mean_squared_error: 1.0425\n",
      "Epoch 184/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0748 - mean_squared_error: 0.0706 - val_loss: 1.0652 - val_mean_squared_error: 1.0611\n",
      "Epoch 185/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0738 - mean_squared_error: 0.0697 - val_loss: 1.0504 - val_mean_squared_error: 1.0463\n",
      "Epoch 186/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0728 - mean_squared_error: 0.0688 - val_loss: 1.0575 - val_mean_squared_error: 1.0534\n",
      "Epoch 187/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0749 - mean_squared_error: 0.0709 - val_loss: 1.0322 - val_mean_squared_error: 1.0282\n",
      "Epoch 188/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0754 - mean_squared_error: 0.0714 - val_loss: 1.0386 - val_mean_squared_error: 1.0346\n",
      "Epoch 189/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0748 - mean_squared_error: 0.0708 - val_loss: 1.0527 - val_mean_squared_error: 1.0487\n",
      "Epoch 190/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0758 - mean_squared_error: 0.0718 - val_loss: 1.0468 - val_mean_squared_error: 1.0428\n",
      "Epoch 191/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0758 - mean_squared_error: 0.0718 - val_loss: 1.0423 - val_mean_squared_error: 1.0383\n",
      "Epoch 192/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0760 - mean_squared_error: 0.0720 - val_loss: 1.0709 - val_mean_squared_error: 1.0669\n",
      "Epoch 193/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0786 - mean_squared_error: 0.0746 - val_loss: 1.0663 - val_mean_squared_error: 1.0624\n",
      "Epoch 194/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0764 - mean_squared_error: 0.0725 - val_loss: 1.0425 - val_mean_squared_error: 1.0386\n",
      "Epoch 195/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0749 - mean_squared_error: 0.0710 - val_loss: 1.0282 - val_mean_squared_error: 1.0243\n",
      "Epoch 196/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0741 - mean_squared_error: 0.0702 - val_loss: 1.0367 - val_mean_squared_error: 1.0328\n",
      "Epoch 197/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0732 - mean_squared_error: 0.0693 - val_loss: 1.0617 - val_mean_squared_error: 1.0578\n",
      "Epoch 198/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0737 - mean_squared_error: 0.0699 - val_loss: 1.0408 - val_mean_squared_error: 1.0370\n",
      "Epoch 199/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0744 - mean_squared_error: 0.0706 - val_loss: 1.0549 - val_mean_squared_error: 1.0511\n",
      "Epoch 200/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0741 - mean_squared_error: 0.0703 - val_loss: 1.0674 - val_mean_squared_error: 1.0635\n",
      "Epoch 201/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0742 - mean_squared_error: 0.0704 - val_loss: 1.0536 - val_mean_squared_error: 1.0499\n",
      "Epoch 202/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0770 - mean_squared_error: 0.0732 - val_loss: 1.0746 - val_mean_squared_error: 1.0708\n",
      "Epoch 203/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0778 - mean_squared_error: 0.0740 - val_loss: 1.0237 - val_mean_squared_error: 1.0199\n",
      "Epoch 204/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0769 - mean_squared_error: 0.0731 - val_loss: 1.0478 - val_mean_squared_error: 1.0440\n",
      "Epoch 205/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0779 - mean_squared_error: 0.0741 - val_loss: 1.0458 - val_mean_squared_error: 1.0420\n",
      "Epoch 206/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0753 - mean_squared_error: 0.0715 - val_loss: 1.0217 - val_mean_squared_error: 1.0179\n",
      "Epoch 207/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0728 - mean_squared_error: 0.0690 - val_loss: 1.0415 - val_mean_squared_error: 1.0377\n",
      "Epoch 208/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0712 - mean_squared_error: 0.0675 - val_loss: 1.0370 - val_mean_squared_error: 1.0333\n",
      "Epoch 209/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0716 - mean_squared_error: 0.0679 - val_loss: 1.0474 - val_mean_squared_error: 1.0437\n",
      "Epoch 210/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0712 - mean_squared_error: 0.0676 - val_loss: 1.0469 - val_mean_squared_error: 1.0432\n",
      "Epoch 211/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0706 - mean_squared_error: 0.0670 - val_loss: 1.0425 - val_mean_squared_error: 1.0388\n",
      "Epoch 212/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0735 - mean_squared_error: 0.0699 - val_loss: 1.0559 - val_mean_squared_error: 1.0523\n",
      "Epoch 213/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0744 - mean_squared_error: 0.0708 - val_loss: 1.0365 - val_mean_squared_error: 1.0329\n",
      "Epoch 214/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0774 - mean_squared_error: 0.0738 - val_loss: 1.0301 - val_mean_squared_error: 1.0265\n",
      "Epoch 215/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0755 - mean_squared_error: 0.0719 - val_loss: 1.0550 - val_mean_squared_error: 1.0514\n",
      "Epoch 216/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0745 - mean_squared_error: 0.0709 - val_loss: 1.0510 - val_mean_squared_error: 1.0474\n",
      "Epoch 217/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0733 - mean_squared_error: 0.0697 - val_loss: 1.0580 - val_mean_squared_error: 1.0544\n",
      "Epoch 218/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0723 - mean_squared_error: 0.0687 - val_loss: 1.0652 - val_mean_squared_error: 1.0616\n",
      "Epoch 219/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0725 - mean_squared_error: 0.0689 - val_loss: 1.0383 - val_mean_squared_error: 1.0348\n",
      "Epoch 220/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0719 - mean_squared_error: 0.0684 - val_loss: 1.0341 - val_mean_squared_error: 1.0306\n",
      "Epoch 221/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0729 - mean_squared_error: 0.0694 - val_loss: 1.0348 - val_mean_squared_error: 1.0312\n",
      "Epoch 222/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0746 - mean_squared_error: 0.0711 - val_loss: 1.0673 - val_mean_squared_error: 1.0638\n",
      "Epoch 223/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0746 - mean_squared_error: 0.0711 - val_loss: 1.0438 - val_mean_squared_error: 1.0403\n",
      "Epoch 224/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0730 - mean_squared_error: 0.0695 - val_loss: 1.0377 - val_mean_squared_error: 1.0341\n",
      "Epoch 225/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0725 - mean_squared_error: 0.0690 - val_loss: 1.0511 - val_mean_squared_error: 1.0476\n",
      "Epoch 226/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0721 - mean_squared_error: 0.0687 - val_loss: 1.0365 - val_mean_squared_error: 1.0331\n",
      "Epoch 227/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0709 - mean_squared_error: 0.0675 - val_loss: 1.0391 - val_mean_squared_error: 1.0356\n",
      "Epoch 228/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0704 - mean_squared_error: 0.0669 - val_loss: 1.0311 - val_mean_squared_error: 1.0277\n",
      "Epoch 229/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0725 - mean_squared_error: 0.0691 - val_loss: 1.0380 - val_mean_squared_error: 1.0346\n",
      "Epoch 230/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0716 - mean_squared_error: 0.0682 - val_loss: 1.0564 - val_mean_squared_error: 1.0530\n",
      "Epoch 231/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0712 - mean_squared_error: 0.0679 - val_loss: 1.0518 - val_mean_squared_error: 1.0484\n",
      "Epoch 232/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0709 - mean_squared_error: 0.0675 - val_loss: 1.0585 - val_mean_squared_error: 1.0552\n",
      "Epoch 233/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0718 - mean_squared_error: 0.0685 - val_loss: 1.0492 - val_mean_squared_error: 1.0458\n",
      "Epoch 234/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0727 - mean_squared_error: 0.0693 - val_loss: 1.0386 - val_mean_squared_error: 1.0353\n",
      "Epoch 235/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0764 - mean_squared_error: 0.0730 - val_loss: 1.0750 - val_mean_squared_error: 1.0716\n",
      "Epoch 236/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0776 - mean_squared_error: 0.0742 - val_loss: 1.0593 - val_mean_squared_error: 1.0559\n",
      "Epoch 237/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0752 - mean_squared_error: 0.0718 - val_loss: 1.0340 - val_mean_squared_error: 1.0306\n",
      "Epoch 238/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0717 - mean_squared_error: 0.0683 - val_loss: 1.0355 - val_mean_squared_error: 1.0321\n",
      "Epoch 239/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0702 - mean_squared_error: 0.0669 - val_loss: 1.0337 - val_mean_squared_error: 1.0304\n",
      "Epoch 240/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_squared_error: 0.0666 - val_loss: 1.0480 - val_mean_squared_error: 1.0447\n",
      "Epoch 241/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0694 - mean_squared_error: 0.0661 - val_loss: 1.0482 - val_mean_squared_error: 1.0449\n",
      "Epoch 242/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0693 - mean_squared_error: 0.0660 - val_loss: 1.0352 - val_mean_squared_error: 1.0319\n",
      "Epoch 243/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0689 - mean_squared_error: 0.0657 - val_loss: 1.0389 - val_mean_squared_error: 1.0356\n",
      "Epoch 244/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0690 - mean_squared_error: 0.0658 - val_loss: 1.0375 - val_mean_squared_error: 1.0344\n",
      "Epoch 245/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0694 - mean_squared_error: 0.0662 - val_loss: 1.0165 - val_mean_squared_error: 1.0134\n",
      "Epoch 246/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_squared_error: 0.0668 - val_loss: 1.0540 - val_mean_squared_error: 1.0509\n",
      "Epoch 247/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0727 - mean_squared_error: 0.0695 - val_loss: 1.0418 - val_mean_squared_error: 1.0386\n",
      "Epoch 248/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0758 - mean_squared_error: 0.0726 - val_loss: 1.0549 - val_mean_squared_error: 1.0517\n",
      "Epoch 249/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0827 - mean_squared_error: 0.0795 - val_loss: 1.0546 - val_mean_squared_error: 1.0513\n",
      "Epoch 250/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0771 - mean_squared_error: 0.0738 - val_loss: 1.0427 - val_mean_squared_error: 1.0394\n",
      "Epoch 251/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0730 - mean_squared_error: 0.0697 - val_loss: 1.0317 - val_mean_squared_error: 1.0285\n",
      "Epoch 252/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0698 - mean_squared_error: 0.0666 - val_loss: 1.0306 - val_mean_squared_error: 1.0273\n",
      "Epoch 253/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0693 - mean_squared_error: 0.0661 - val_loss: 1.0257 - val_mean_squared_error: 1.0225\n",
      "Epoch 254/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0692 - mean_squared_error: 0.0660 - val_loss: 1.0356 - val_mean_squared_error: 1.0324\n",
      "Epoch 255/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0687 - mean_squared_error: 0.0655 - val_loss: 1.0385 - val_mean_squared_error: 1.0354\n",
      "Epoch 256/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0689 - mean_squared_error: 0.0658 - val_loss: 1.0316 - val_mean_squared_error: 1.0285\n",
      "Epoch 257/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0692 - mean_squared_error: 0.0661 - val_loss: 1.0394 - val_mean_squared_error: 1.0363\n",
      "Epoch 258/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0701 - mean_squared_error: 0.0670 - val_loss: 1.0325 - val_mean_squared_error: 1.0294\n",
      "Epoch 259/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0704 - mean_squared_error: 0.0673 - val_loss: 1.0295 - val_mean_squared_error: 1.0264\n",
      "Epoch 260/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0737 - mean_squared_error: 0.0706 - val_loss: 1.0275 - val_mean_squared_error: 1.0244\n",
      "Epoch 261/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0732 - mean_squared_error: 0.0701 - val_loss: 1.0397 - val_mean_squared_error: 1.0366\n",
      "Epoch 262/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0726 - mean_squared_error: 0.0695 - val_loss: 1.0273 - val_mean_squared_error: 1.0242\n",
      "Epoch 263/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0717 - mean_squared_error: 0.0686 - val_loss: 1.0051 - val_mean_squared_error: 1.0020\n",
      "Epoch 264/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0717 - mean_squared_error: 0.0686 - val_loss: 1.0381 - val_mean_squared_error: 1.0350\n",
      "Epoch 265/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0707 - mean_squared_error: 0.0676 - val_loss: 1.0170 - val_mean_squared_error: 1.0139\n",
      "Epoch 266/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_squared_error: 0.0668 - val_loss: 1.0466 - val_mean_squared_error: 1.0435\n",
      "Epoch 267/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0693 - mean_squared_error: 0.0662 - val_loss: 1.0171 - val_mean_squared_error: 1.0141\n",
      "Epoch 268/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0684 - mean_squared_error: 0.0654 - val_loss: 1.0276 - val_mean_squared_error: 1.0246\n",
      "Epoch 269/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_error: 0.0653 - val_loss: 1.0224 - val_mean_squared_error: 1.0194\n",
      "Epoch 270/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0685 - mean_squared_error: 0.0656 - val_loss: 1.0454 - val_mean_squared_error: 1.0424\n",
      "Epoch 271/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0705 - mean_squared_error: 0.0676 - val_loss: 1.0479 - val_mean_squared_error: 1.0450\n",
      "Epoch 272/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0713 - mean_squared_error: 0.0684 - val_loss: 1.0295 - val_mean_squared_error: 1.0266\n",
      "Epoch 273/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0698 - mean_squared_error: 0.0668 - val_loss: 1.0456 - val_mean_squared_error: 1.0426\n",
      "Epoch 274/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0696 - mean_squared_error: 0.0666 - val_loss: 1.0300 - val_mean_squared_error: 1.0270\n",
      "Epoch 275/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0690 - mean_squared_error: 0.0661 - val_loss: 1.0410 - val_mean_squared_error: 1.0380\n",
      "Epoch 276/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0687 - mean_squared_error: 0.0658 - val_loss: 1.0427 - val_mean_squared_error: 1.0398\n",
      "Epoch 277/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0690 - mean_squared_error: 0.0661 - val_loss: 1.0344 - val_mean_squared_error: 1.0315\n",
      "Epoch 278/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_squared_error: 0.0671 - val_loss: 1.0363 - val_mean_squared_error: 1.0335\n",
      "Epoch 279/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0724 - mean_squared_error: 0.0696 - val_loss: 1.0108 - val_mean_squared_error: 1.0079\n",
      "Epoch 280/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0746 - mean_squared_error: 0.0717 - val_loss: 1.0355 - val_mean_squared_error: 1.0326\n",
      "Epoch 281/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0735 - mean_squared_error: 0.0705 - val_loss: 1.0014 - val_mean_squared_error: 0.9985\n",
      "Epoch 282/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0722 - mean_squared_error: 0.0693 - val_loss: 1.0179 - val_mean_squared_error: 1.0149\n",
      "Epoch 283/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0736 - mean_squared_error: 0.0706 - val_loss: 1.0181 - val_mean_squared_error: 1.0152\n",
      "Epoch 284/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0697 - mean_squared_error: 0.0667 - val_loss: 1.0315 - val_mean_squared_error: 1.0286\n",
      "Epoch 285/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0684 - mean_squared_error: 0.0654 - val_loss: 1.0317 - val_mean_squared_error: 1.0288\n",
      "Epoch 286/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0680 - mean_squared_error: 0.0652 - val_loss: 1.0167 - val_mean_squared_error: 1.0138\n",
      "Epoch 287/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0673 - mean_squared_error: 0.0645 - val_loss: 1.0330 - val_mean_squared_error: 1.0302\n",
      "Epoch 288/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0671 - mean_squared_error: 0.0643 - val_loss: 1.0317 - val_mean_squared_error: 1.0289\n",
      "Epoch 289/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0679 - mean_squared_error: 0.0650 - val_loss: 1.0122 - val_mean_squared_error: 1.0094\n",
      "Epoch 290/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0653 - val_loss: 1.0319 - val_mean_squared_error: 1.0291\n",
      "Epoch 291/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0686 - mean_squared_error: 0.0658 - val_loss: 1.0479 - val_mean_squared_error: 1.0452\n",
      "Epoch 292/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0693 - mean_squared_error: 0.0666 - val_loss: 1.0163 - val_mean_squared_error: 1.0136\n",
      "Epoch 293/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0707 - mean_squared_error: 0.0679 - val_loss: 1.0014 - val_mean_squared_error: 0.9986\n",
      "Epoch 294/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0723 - mean_squared_error: 0.0696 - val_loss: 1.0515 - val_mean_squared_error: 1.0487\n",
      "Epoch 295/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0730 - mean_squared_error: 0.0702 - val_loss: 1.0577 - val_mean_squared_error: 1.0548\n",
      "Epoch 296/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0712 - mean_squared_error: 0.0684 - val_loss: 1.0231 - val_mean_squared_error: 1.0203\n",
      "Epoch 297/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0718 - mean_squared_error: 0.0690 - val_loss: 1.0217 - val_mean_squared_error: 1.0189\n",
      "Epoch 298/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0705 - mean_squared_error: 0.0677 - val_loss: 1.0274 - val_mean_squared_error: 1.0246\n",
      "Epoch 299/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0688 - mean_squared_error: 0.0660 - val_loss: 1.0214 - val_mean_squared_error: 1.0187\n",
      "Epoch 300/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0677 - mean_squared_error: 0.0649 - val_loss: 1.0391 - val_mean_squared_error: 1.0363\n",
      "Epoch 301/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0676 - mean_squared_error: 0.0648 - val_loss: 1.0322 - val_mean_squared_error: 1.0294\n",
      "Epoch 302/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0688 - mean_squared_error: 0.0661 - val_loss: 1.0281 - val_mean_squared_error: 1.0254\n",
      "Epoch 303/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_error: 0.0656 - val_loss: 1.0404 - val_mean_squared_error: 1.0376\n",
      "Epoch 304/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_error: 0.0656 - val_loss: 1.0432 - val_mean_squared_error: 1.0405\n",
      "Epoch 305/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0686 - mean_squared_error: 0.0659 - val_loss: 1.0328 - val_mean_squared_error: 1.0301\n",
      "Epoch 306/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0685 - mean_squared_error: 0.0658 - val_loss: 1.0283 - val_mean_squared_error: 1.0256\n",
      "Epoch 307/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0689 - mean_squared_error: 0.0663 - val_loss: 1.0226 - val_mean_squared_error: 1.0199\n",
      "Epoch 308/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0698 - mean_squared_error: 0.0671 - val_loss: 1.0366 - val_mean_squared_error: 1.0339\n",
      "Epoch 309/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0682 - mean_squared_error: 0.0655 - val_loss: 1.0095 - val_mean_squared_error: 1.0069\n",
      "Epoch 310/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0654 - val_loss: 1.0165 - val_mean_squared_error: 1.0138\n",
      "Epoch 311/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0654 - val_loss: 1.0276 - val_mean_squared_error: 1.0250\n",
      "Epoch 312/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_error: 0.0656 - val_loss: 1.0181 - val_mean_squared_error: 1.0155\n",
      "Epoch 313/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0684 - mean_squared_error: 0.0657 - val_loss: 1.0263 - val_mean_squared_error: 1.0237\n",
      "Epoch 314/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0678 - mean_squared_error: 0.0652 - val_loss: 1.0221 - val_mean_squared_error: 1.0195\n",
      "Epoch 315/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0683 - mean_squared_error: 0.0657 - val_loss: 1.0221 - val_mean_squared_error: 1.0195\n",
      "Epoch 316/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0691 - mean_squared_error: 0.0665 - val_loss: 1.0090 - val_mean_squared_error: 1.0064\n",
      "Epoch 317/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0805 - mean_squared_error: 0.0779 - val_loss: 1.0092 - val_mean_squared_error: 1.0066\n",
      "Epoch 318/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0758 - mean_squared_error: 0.0731 - val_loss: 1.0229 - val_mean_squared_error: 1.0202\n",
      "Epoch 319/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0720 - mean_squared_error: 0.0693 - val_loss: 1.0304 - val_mean_squared_error: 1.0276\n",
      "Epoch 320/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0684 - mean_squared_error: 0.0657 - val_loss: 1.0196 - val_mean_squared_error: 1.0170\n",
      "Epoch 321/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0668 - mean_squared_error: 0.0641 - val_loss: 1.0124 - val_mean_squared_error: 1.0097\n",
      "Epoch 322/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0659 - mean_squared_error: 0.0632 - val_loss: 1.0299 - val_mean_squared_error: 1.0273\n",
      "Epoch 323/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0658 - mean_squared_error: 0.0632 - val_loss: 1.0207 - val_mean_squared_error: 1.0181\n",
      "Epoch 324/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0656 - mean_squared_error: 0.0630 - val_loss: 1.0306 - val_mean_squared_error: 1.0280\n",
      "Epoch 325/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0656 - mean_squared_error: 0.0630 - val_loss: 1.0316 - val_mean_squared_error: 1.0290\n",
      "Epoch 326/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0662 - mean_squared_error: 0.0637 - val_loss: 1.0238 - val_mean_squared_error: 1.0213\n",
      "Epoch 327/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0673 - mean_squared_error: 0.0647 - val_loss: 1.0214 - val_mean_squared_error: 1.0189\n",
      "Epoch 328/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0704 - mean_squared_error: 0.0678 - val_loss: 1.0211 - val_mean_squared_error: 1.0186\n",
      "Epoch 329/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0724 - mean_squared_error: 0.0699 - val_loss: 1.0167 - val_mean_squared_error: 1.0142\n",
      "Epoch 330/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0729 - mean_squared_error: 0.0704 - val_loss: 0.9933 - val_mean_squared_error: 0.9907\n",
      "Epoch 331/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0745 - mean_squared_error: 0.0719 - val_loss: 1.0330 - val_mean_squared_error: 1.0303\n",
      "Epoch 332/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0694 - mean_squared_error: 0.0668 - val_loss: 1.0163 - val_mean_squared_error: 1.0137\n",
      "Epoch 333/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0685 - mean_squared_error: 0.0659 - val_loss: 1.0085 - val_mean_squared_error: 1.0059\n",
      "Epoch 334/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_squared_error: 0.0643 - val_loss: 1.0202 - val_mean_squared_error: 1.0176\n",
      "Epoch 335/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0661 - mean_squared_error: 0.0635 - val_loss: 1.0180 - val_mean_squared_error: 1.0154\n",
      "Epoch 336/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0658 - mean_squared_error: 0.0633 - val_loss: 1.0100 - val_mean_squared_error: 1.0075\n",
      "Epoch 337/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0657 - mean_squared_error: 0.0632 - val_loss: 1.0199 - val_mean_squared_error: 1.0174\n",
      "Epoch 338/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0662 - mean_squared_error: 0.0637 - val_loss: 1.0190 - val_mean_squared_error: 1.0165\n",
      "Epoch 339/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0663 - mean_squared_error: 0.0638 - val_loss: 1.0216 - val_mean_squared_error: 1.0191\n",
      "Epoch 340/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0663 - mean_squared_error: 0.0638 - val_loss: 1.0208 - val_mean_squared_error: 1.0183\n",
      "Epoch 341/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_squared_error: 0.0645 - val_loss: 1.0225 - val_mean_squared_error: 1.0200\n",
      "Epoch 342/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0668 - mean_squared_error: 0.0644 - val_loss: 1.0205 - val_mean_squared_error: 1.0181\n",
      "Epoch 343/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0689 - mean_squared_error: 0.0664 - val_loss: 1.0299 - val_mean_squared_error: 1.0274\n",
      "Epoch 344/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0767 - mean_squared_error: 0.0742 - val_loss: 1.0133 - val_mean_squared_error: 1.0108\n",
      "Epoch 345/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0728 - mean_squared_error: 0.0703 - val_loss: 1.0335 - val_mean_squared_error: 1.0309\n",
      "Epoch 346/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0656 - val_loss: 1.0070 - val_mean_squared_error: 1.0045\n",
      "Epoch 347/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0670 - mean_squared_error: 0.0645 - val_loss: 1.0076 - val_mean_squared_error: 1.0051\n",
      "Epoch 348/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0660 - mean_squared_error: 0.0636 - val_loss: 1.0168 - val_mean_squared_error: 1.0144\n",
      "Epoch 349/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0656 - mean_squared_error: 0.0632 - val_loss: 1.0228 - val_mean_squared_error: 1.0204\n",
      "Epoch 350/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0658 - mean_squared_error: 0.0633 - val_loss: 1.0178 - val_mean_squared_error: 1.0154\n",
      "Epoch 351/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0654 - mean_squared_error: 0.0630 - val_loss: 1.0104 - val_mean_squared_error: 1.0080\n",
      "Epoch 352/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0657 - mean_squared_error: 0.0633 - val_loss: 1.0143 - val_mean_squared_error: 1.0119\n",
      "Epoch 353/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0665 - mean_squared_error: 0.0641 - val_loss: 1.0155 - val_mean_squared_error: 1.0131\n",
      "Epoch 354/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0677 - mean_squared_error: 0.0654 - val_loss: 1.0044 - val_mean_squared_error: 1.0020\n",
      "Epoch 355/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0698 - mean_squared_error: 0.0674 - val_loss: 1.0476 - val_mean_squared_error: 1.0451\n",
      "Epoch 356/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0705 - mean_squared_error: 0.0681 - val_loss: 0.9858 - val_mean_squared_error: 0.9834\n",
      "Epoch 357/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0704 - mean_squared_error: 0.0680 - val_loss: 1.0177 - val_mean_squared_error: 1.0153\n",
      "Epoch 358/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0685 - mean_squared_error: 0.0661 - val_loss: 0.9998 - val_mean_squared_error: 0.9974\n",
      "Epoch 359/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0691 - mean_squared_error: 0.0667 - val_loss: 1.0252 - val_mean_squared_error: 1.0228\n",
      "Epoch 360/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0678 - mean_squared_error: 0.0654 - val_loss: 1.0216 - val_mean_squared_error: 1.0192\n",
      "Epoch 361/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0672 - mean_squared_error: 0.0648 - val_loss: 1.0268 - val_mean_squared_error: 1.0244\n",
      "Epoch 362/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0672 - mean_squared_error: 0.0648 - val_loss: 1.0234 - val_mean_squared_error: 1.0210\n",
      "Epoch 363/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0672 - mean_squared_error: 0.0648 - val_loss: 1.0019 - val_mean_squared_error: 0.9995\n",
      "Epoch 364/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0667 - mean_squared_error: 0.0643 - val_loss: 1.0046 - val_mean_squared_error: 1.0023\n",
      "Epoch 365/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0670 - mean_squared_error: 0.0646 - val_loss: 1.0298 - val_mean_squared_error: 1.0274\n",
      "Epoch 366/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_squared_error: 0.0645 - val_loss: 1.0150 - val_mean_squared_error: 1.0126\n",
      "Epoch 367/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0664 - mean_squared_error: 0.0641 - val_loss: 1.0156 - val_mean_squared_error: 1.0133\n",
      "Epoch 368/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0691 - mean_squared_error: 0.0668 - val_loss: 1.0099 - val_mean_squared_error: 1.0076\n",
      "Epoch 369/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0702 - mean_squared_error: 0.0679 - val_loss: 1.0108 - val_mean_squared_error: 1.0084\n",
      "Epoch 370/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0690 - mean_squared_error: 0.0666 - val_loss: 1.0063 - val_mean_squared_error: 1.0039\n",
      "Epoch 371/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0676 - mean_squared_error: 0.0652 - val_loss: 1.0202 - val_mean_squared_error: 1.0179\n",
      "Epoch 372/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0657 - val_loss: 1.0141 - val_mean_squared_error: 1.0117\n",
      "Epoch 373/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0671 - mean_squared_error: 0.0647 - val_loss: 1.0134 - val_mean_squared_error: 1.0111\n",
      "Epoch 374/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0672 - mean_squared_error: 0.0649 - val_loss: 1.0226 - val_mean_squared_error: 1.0203\n",
      "Epoch 375/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0666 - mean_squared_error: 0.0643 - val_loss: 1.0110 - val_mean_squared_error: 1.0087\n",
      "Epoch 376/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0663 - mean_squared_error: 0.0640 - val_loss: 0.9978 - val_mean_squared_error: 0.9955\n",
      "Epoch 377/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0661 - mean_squared_error: 0.0638 - val_loss: 0.9928 - val_mean_squared_error: 0.9905\n",
      "Epoch 378/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0655 - mean_squared_error: 0.0632 - val_loss: 1.0121 - val_mean_squared_error: 1.0098\n",
      "Epoch 379/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0661 - mean_squared_error: 0.0638 - val_loss: 1.0159 - val_mean_squared_error: 1.0136\n",
      "Epoch 380/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0659 - mean_squared_error: 0.0637 - val_loss: 1.0136 - val_mean_squared_error: 1.0114\n",
      "Epoch 381/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0668 - mean_squared_error: 0.0646 - val_loss: 1.0255 - val_mean_squared_error: 1.0232\n",
      "Epoch 382/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0679 - mean_squared_error: 0.0657 - val_loss: 1.0161 - val_mean_squared_error: 1.0138\n",
      "Epoch 383/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0681 - mean_squared_error: 0.0658 - val_loss: 1.0016 - val_mean_squared_error: 0.9993\n",
      "Epoch 384/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0674 - mean_squared_error: 0.0651 - val_loss: 1.0020 - val_mean_squared_error: 0.9997\n",
      "Epoch 385/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0675 - mean_squared_error: 0.0653 - val_loss: 1.0130 - val_mean_squared_error: 1.0108\n",
      "Epoch 386/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0678 - mean_squared_error: 0.0656 - val_loss: 0.9933 - val_mean_squared_error: 0.9910\n",
      "Epoch 387/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0670 - mean_squared_error: 0.0648 - val_loss: 0.9946 - val_mean_squared_error: 0.9924\n",
      "Epoch 388/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0669 - mean_squared_error: 0.0647 - val_loss: 1.0076 - val_mean_squared_error: 1.0053\n",
      "Epoch 389/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0682 - mean_squared_error: 0.0659 - val_loss: 0.9910 - val_mean_squared_error: 0.9888\n",
      "Epoch 390/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0687 - mean_squared_error: 0.0665 - val_loss: 0.9854 - val_mean_squared_error: 0.9832\n",
      "Epoch 391/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0698 - mean_squared_error: 0.0676 - val_loss: 1.0002 - val_mean_squared_error: 0.9979\n",
      "Epoch 392/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0680 - mean_squared_error: 0.0657 - val_loss: 1.0065 - val_mean_squared_error: 1.0042\n",
      "Epoch 393/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0671 - mean_squared_error: 0.0648 - val_loss: 1.0184 - val_mean_squared_error: 1.0162\n",
      "Epoch 394/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0666 - mean_squared_error: 0.0643 - val_loss: 1.0088 - val_mean_squared_error: 1.0065\n",
      "Epoch 395/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0652 - mean_squared_error: 0.0630 - val_loss: 1.0037 - val_mean_squared_error: 1.0014\n",
      "Epoch 396/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0649 - mean_squared_error: 0.0627 - val_loss: 1.0031 - val_mean_squared_error: 1.0009\n",
      "Epoch 397/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0651 - mean_squared_error: 0.0629 - val_loss: 1.0095 - val_mean_squared_error: 1.0073\n",
      "Epoch 398/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0654 - mean_squared_error: 0.0632 - val_loss: 1.0091 - val_mean_squared_error: 1.0069\n",
      "Epoch 399/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0663 - mean_squared_error: 0.0641 - val_loss: 1.0121 - val_mean_squared_error: 1.0099\n",
      "Epoch 400/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0667 - mean_squared_error: 0.0646 - val_loss: 1.0118 - val_mean_squared_error: 1.0096\n",
      "Epoch 401/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0662 - mean_squared_error: 0.0640 - val_loss: 0.9913 - val_mean_squared_error: 0.9891\n",
      "Epoch 402/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0686 - mean_squared_error: 0.0665 - val_loss: 1.0176 - val_mean_squared_error: 1.0154\n",
      "Epoch 403/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0707 - mean_squared_error: 0.0685 - val_loss: 1.0013 - val_mean_squared_error: 0.9991\n",
      "Epoch 404/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0680 - mean_squared_error: 0.0658 - val_loss: 1.0102 - val_mean_squared_error: 1.0080\n",
      "Epoch 405/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0676 - mean_squared_error: 0.0654 - val_loss: 1.0212 - val_mean_squared_error: 1.0190\n",
      "Epoch 406/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0674 - mean_squared_error: 0.0652 - val_loss: 0.9875 - val_mean_squared_error: 0.9853\n",
      "Epoch 407/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0682 - mean_squared_error: 0.0660 - val_loss: 1.0071 - val_mean_squared_error: 1.0049\n",
      "Epoch 408/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0660 - mean_squared_error: 0.0638 - val_loss: 0.9994 - val_mean_squared_error: 0.9972\n",
      "Epoch 409/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0653 - mean_squared_error: 0.0631 - val_loss: 1.0022 - val_mean_squared_error: 1.0000\n",
      "Epoch 410/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0655 - mean_squared_error: 0.0633 - val_loss: 1.0062 - val_mean_squared_error: 1.0041\n",
      "Epoch 411/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0648 - mean_squared_error: 0.0626 - val_loss: 0.9917 - val_mean_squared_error: 0.9895\n",
      "Epoch 412/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0650 - mean_squared_error: 0.0629 - val_loss: 0.9970 - val_mean_squared_error: 0.9948\n",
      "Epoch 413/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0656 - mean_squared_error: 0.0635 - val_loss: 1.0263 - val_mean_squared_error: 1.0242\n",
      "Epoch 414/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0701 - mean_squared_error: 0.0680 - val_loss: 0.9962 - val_mean_squared_error: 0.9940\n",
      "Epoch 415/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0699 - mean_squared_error: 0.0677 - val_loss: 0.9894 - val_mean_squared_error: 0.9872\n",
      "Epoch 416/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0694 - mean_squared_error: 0.0672 - val_loss: 0.9964 - val_mean_squared_error: 0.9942\n",
      "Epoch 417/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0667 - mean_squared_error: 0.0645 - val_loss: 1.0036 - val_mean_squared_error: 1.0014\n",
      "Epoch 418/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0653 - mean_squared_error: 0.0632 - val_loss: 1.0012 - val_mean_squared_error: 0.9991\n",
      "Epoch 419/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0650 - mean_squared_error: 0.0629 - val_loss: 0.9903 - val_mean_squared_error: 0.9881\n",
      "Epoch 420/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0648 - mean_squared_error: 0.0627 - val_loss: 0.9982 - val_mean_squared_error: 0.9960\n",
      "Epoch 421/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0644 - mean_squared_error: 0.0623 - val_loss: 1.0007 - val_mean_squared_error: 0.9986\n",
      "Epoch 422/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0643 - mean_squared_error: 0.0622 - val_loss: 1.0012 - val_mean_squared_error: 0.9991\n",
      "Epoch 423/5000\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.0644 - mean_squared_error: 0.0624 - val_loss: 0.9944 - val_mean_squared_error: 0.9924\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1491 - mean_squared_error: 0.1470\n",
      "Epoch 1/5000\n",
      "94/94 [==============================] - 10s 22ms/step - loss: 4.7209 - mean_squared_error: 4.7085 - val_loss: 1.2376 - val_mean_squared_error: 1.2241\n",
      "Epoch 2/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.5484 - mean_squared_error: 0.5350 - val_loss: 1.1353 - val_mean_squared_error: 1.1220\n",
      "Epoch 3/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.5448 - mean_squared_error: 0.5316 - val_loss: 1.0966 - val_mean_squared_error: 1.0835\n",
      "Epoch 4/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.5438 - mean_squared_error: 0.5309 - val_loss: 1.1286 - val_mean_squared_error: 1.1157\n",
      "Epoch 5/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.5435 - mean_squared_error: 0.5309 - val_loss: 1.0516 - val_mean_squared_error: 1.0390\n",
      "Epoch 6/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.5422 - mean_squared_error: 0.5298 - val_loss: 1.0589 - val_mean_squared_error: 1.0466\n",
      "Epoch 7/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.5333 - mean_squared_error: 0.5211 - val_loss: 1.0273 - val_mean_squared_error: 1.0153\n",
      "Epoch 8/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.5062 - mean_squared_error: 0.4942 - val_loss: 0.9496 - val_mean_squared_error: 0.9376\n",
      "Epoch 9/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4709 - mean_squared_error: 0.4589 - val_loss: 0.8378 - val_mean_squared_error: 0.8259\n",
      "Epoch 10/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4526 - mean_squared_error: 0.4408 - val_loss: 0.8928 - val_mean_squared_error: 0.8811\n",
      "Epoch 11/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4423 - mean_squared_error: 0.4308 - val_loss: 0.8414 - val_mean_squared_error: 0.8300\n",
      "Epoch 12/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4353 - mean_squared_error: 0.4240 - val_loss: 0.8044 - val_mean_squared_error: 0.7932\n",
      "Epoch 13/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4267 - mean_squared_error: 0.4156 - val_loss: 0.7576 - val_mean_squared_error: 0.7466\n",
      "Epoch 14/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.4163 - mean_squared_error: 0.4053 - val_loss: 0.8857 - val_mean_squared_error: 0.8746\n",
      "Epoch 15/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3990 - mean_squared_error: 0.3879 - val_loss: 0.8521 - val_mean_squared_error: 0.8409\n",
      "Epoch 16/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3629 - mean_squared_error: 0.3516 - val_loss: 0.8015 - val_mean_squared_error: 0.7902\n",
      "Epoch 17/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3408 - mean_squared_error: 0.3295 - val_loss: 0.8712 - val_mean_squared_error: 0.8599\n",
      "Epoch 18/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3224 - mean_squared_error: 0.3111 - val_loss: 0.8243 - val_mean_squared_error: 0.8131\n",
      "Epoch 19/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.3086 - mean_squared_error: 0.2973 - val_loss: 0.8909 - val_mean_squared_error: 0.8797\n",
      "Epoch 20/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2964 - mean_squared_error: 0.2851 - val_loss: 0.8546 - val_mean_squared_error: 0.8434\n",
      "Epoch 21/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2826 - mean_squared_error: 0.2714 - val_loss: 0.8994 - val_mean_squared_error: 0.8881\n",
      "Epoch 22/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2695 - mean_squared_error: 0.2582 - val_loss: 0.8580 - val_mean_squared_error: 0.8467\n",
      "Epoch 23/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2562 - mean_squared_error: 0.2450 - val_loss: 0.8583 - val_mean_squared_error: 0.8471\n",
      "Epoch 24/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2450 - mean_squared_error: 0.2338 - val_loss: 0.8039 - val_mean_squared_error: 0.7927\n",
      "Epoch 25/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2362 - mean_squared_error: 0.2250 - val_loss: 0.8383 - val_mean_squared_error: 0.8272\n",
      "Epoch 26/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2290 - mean_squared_error: 0.2179 - val_loss: 0.8154 - val_mean_squared_error: 0.8043\n",
      "Epoch 27/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2236 - mean_squared_error: 0.2126 - val_loss: 0.7847 - val_mean_squared_error: 0.7737\n",
      "Epoch 28/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2177 - mean_squared_error: 0.2068 - val_loss: 0.8030 - val_mean_squared_error: 0.7922\n",
      "Epoch 29/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2146 - mean_squared_error: 0.2038 - val_loss: 0.8458 - val_mean_squared_error: 0.8351\n",
      "Epoch 30/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2099 - mean_squared_error: 0.1992 - val_loss: 0.8584 - val_mean_squared_error: 0.8478\n",
      "Epoch 31/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2061 - mean_squared_error: 0.1956 - val_loss: 0.7862 - val_mean_squared_error: 0.7756\n",
      "Epoch 32/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.2008 - mean_squared_error: 0.1903 - val_loss: 0.8365 - val_mean_squared_error: 0.8260\n",
      "Epoch 33/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1959 - mean_squared_error: 0.1855 - val_loss: 0.7875 - val_mean_squared_error: 0.7771\n",
      "Epoch 34/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1920 - mean_squared_error: 0.1816 - val_loss: 0.8325 - val_mean_squared_error: 0.8223\n",
      "Epoch 35/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1885 - mean_squared_error: 0.1783 - val_loss: 0.8492 - val_mean_squared_error: 0.8390\n",
      "Epoch 36/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1856 - mean_squared_error: 0.1755 - val_loss: 0.8390 - val_mean_squared_error: 0.8289\n",
      "Epoch 37/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1835 - mean_squared_error: 0.1734 - val_loss: 0.8221 - val_mean_squared_error: 0.8122\n",
      "Epoch 38/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1809 - mean_squared_error: 0.1710 - val_loss: 0.8403 - val_mean_squared_error: 0.8304\n",
      "Epoch 39/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1782 - mean_squared_error: 0.1684 - val_loss: 0.8543 - val_mean_squared_error: 0.8445\n",
      "Epoch 40/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1770 - mean_squared_error: 0.1673 - val_loss: 0.8135 - val_mean_squared_error: 0.8039\n",
      "Epoch 41/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1746 - mean_squared_error: 0.1650 - val_loss: 0.8549 - val_mean_squared_error: 0.8453\n",
      "Epoch 42/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1723 - mean_squared_error: 0.1628 - val_loss: 0.8627 - val_mean_squared_error: 0.8532\n",
      "Epoch 43/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1704 - mean_squared_error: 0.1609 - val_loss: 0.8403 - val_mean_squared_error: 0.8309\n",
      "Epoch 44/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1679 - mean_squared_error: 0.1586 - val_loss: 0.8555 - val_mean_squared_error: 0.8462\n",
      "Epoch 45/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1658 - mean_squared_error: 0.1565 - val_loss: 0.8510 - val_mean_squared_error: 0.8418\n",
      "Epoch 46/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1635 - mean_squared_error: 0.1544 - val_loss: 0.8475 - val_mean_squared_error: 0.8384\n",
      "Epoch 47/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1612 - mean_squared_error: 0.1521 - val_loss: 0.8636 - val_mean_squared_error: 0.8545\n",
      "Epoch 48/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1597 - mean_squared_error: 0.1507 - val_loss: 0.8634 - val_mean_squared_error: 0.8544\n",
      "Epoch 49/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1596 - mean_squared_error: 0.1506 - val_loss: 0.8519 - val_mean_squared_error: 0.8430\n",
      "Epoch 50/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1561 - mean_squared_error: 0.1472 - val_loss: 0.8216 - val_mean_squared_error: 0.8127\n",
      "Epoch 51/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1541 - mean_squared_error: 0.1453 - val_loss: 0.8359 - val_mean_squared_error: 0.8271\n",
      "Epoch 52/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1511 - mean_squared_error: 0.1424 - val_loss: 0.8006 - val_mean_squared_error: 0.7919\n",
      "Epoch 53/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1492 - mean_squared_error: 0.1406 - val_loss: 0.8527 - val_mean_squared_error: 0.8441\n",
      "Epoch 54/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1474 - mean_squared_error: 0.1388 - val_loss: 0.8517 - val_mean_squared_error: 0.8432\n",
      "Epoch 55/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1456 - mean_squared_error: 0.1371 - val_loss: 0.8565 - val_mean_squared_error: 0.8481\n",
      "Epoch 56/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1440 - mean_squared_error: 0.1357 - val_loss: 0.8426 - val_mean_squared_error: 0.8343\n",
      "Epoch 57/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1430 - mean_squared_error: 0.1348 - val_loss: 0.8560 - val_mean_squared_error: 0.8478\n",
      "Epoch 58/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1424 - mean_squared_error: 0.1342 - val_loss: 0.8658 - val_mean_squared_error: 0.8577\n",
      "Epoch 59/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1420 - mean_squared_error: 0.1339 - val_loss: 0.8692 - val_mean_squared_error: 0.8612\n",
      "Epoch 60/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1398 - mean_squared_error: 0.1318 - val_loss: 0.8681 - val_mean_squared_error: 0.8602\n",
      "Epoch 61/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1389 - mean_squared_error: 0.1310 - val_loss: 0.8548 - val_mean_squared_error: 0.8470\n",
      "Epoch 62/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1380 - mean_squared_error: 0.1302 - val_loss: 0.8430 - val_mean_squared_error: 0.8353\n",
      "Epoch 63/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1374 - mean_squared_error: 0.1297 - val_loss: 0.8725 - val_mean_squared_error: 0.8649\n",
      "Epoch 64/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1364 - mean_squared_error: 0.1288 - val_loss: 0.9038 - val_mean_squared_error: 0.8962\n",
      "Epoch 65/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1357 - mean_squared_error: 0.1282 - val_loss: 0.8464 - val_mean_squared_error: 0.8389\n",
      "Epoch 66/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1339 - mean_squared_error: 0.1265 - val_loss: 0.8527 - val_mean_squared_error: 0.8453\n",
      "Epoch 67/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1332 - mean_squared_error: 0.1258 - val_loss: 0.8801 - val_mean_squared_error: 0.8727\n",
      "Epoch 68/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1326 - mean_squared_error: 0.1253 - val_loss: 0.8897 - val_mean_squared_error: 0.8825\n",
      "Epoch 69/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1306 - mean_squared_error: 0.1234 - val_loss: 0.8642 - val_mean_squared_error: 0.8570\n",
      "Epoch 70/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1297 - mean_squared_error: 0.1225 - val_loss: 0.8758 - val_mean_squared_error: 0.8687\n",
      "Epoch 71/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1289 - mean_squared_error: 0.1219 - val_loss: 0.8755 - val_mean_squared_error: 0.8685\n",
      "Epoch 72/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1293 - mean_squared_error: 0.1223 - val_loss: 0.8849 - val_mean_squared_error: 0.8779\n",
      "Epoch 73/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1283 - mean_squared_error: 0.1214 - val_loss: 0.9088 - val_mean_squared_error: 0.9019\n",
      "Epoch 74/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1280 - mean_squared_error: 0.1211 - val_loss: 0.8922 - val_mean_squared_error: 0.8854\n",
      "Epoch 75/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1268 - mean_squared_error: 0.1200 - val_loss: 0.8527 - val_mean_squared_error: 0.8459\n",
      "Epoch 76/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1252 - mean_squared_error: 0.1185 - val_loss: 0.8925 - val_mean_squared_error: 0.8858\n",
      "Epoch 77/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1244 - mean_squared_error: 0.1177 - val_loss: 0.8739 - val_mean_squared_error: 0.8673\n",
      "Epoch 78/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1233 - mean_squared_error: 0.1167 - val_loss: 0.9097 - val_mean_squared_error: 0.9031\n",
      "Epoch 79/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1224 - mean_squared_error: 0.1158 - val_loss: 0.9048 - val_mean_squared_error: 0.8983\n",
      "Epoch 80/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1215 - mean_squared_error: 0.1151 - val_loss: 0.9025 - val_mean_squared_error: 0.8961\n",
      "Epoch 81/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1215 - mean_squared_error: 0.1151 - val_loss: 0.9006 - val_mean_squared_error: 0.8942\n",
      "Epoch 82/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1212 - mean_squared_error: 0.1149 - val_loss: 0.9034 - val_mean_squared_error: 0.8972\n",
      "Epoch 83/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1206 - mean_squared_error: 0.1144 - val_loss: 0.9131 - val_mean_squared_error: 0.9069\n",
      "Epoch 84/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1206 - mean_squared_error: 0.1144 - val_loss: 0.9372 - val_mean_squared_error: 0.9310\n",
      "Epoch 85/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1203 - mean_squared_error: 0.1142 - val_loss: 0.9192 - val_mean_squared_error: 0.9131\n",
      "Epoch 86/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1202 - mean_squared_error: 0.1141 - val_loss: 0.8584 - val_mean_squared_error: 0.8524\n",
      "Epoch 87/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1200 - mean_squared_error: 0.1140 - val_loss: 0.9071 - val_mean_squared_error: 0.9012\n",
      "Epoch 88/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1189 - mean_squared_error: 0.1130 - val_loss: 0.9284 - val_mean_squared_error: 0.9225\n",
      "Epoch 89/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1182 - mean_squared_error: 0.1123 - val_loss: 0.9143 - val_mean_squared_error: 0.9084\n",
      "Epoch 90/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1180 - mean_squared_error: 0.1121 - val_loss: 0.8907 - val_mean_squared_error: 0.8848\n",
      "Epoch 91/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1171 - mean_squared_error: 0.1113 - val_loss: 0.9149 - val_mean_squared_error: 0.9092\n",
      "Epoch 92/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1165 - mean_squared_error: 0.1108 - val_loss: 0.9045 - val_mean_squared_error: 0.8988\n",
      "Epoch 93/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1161 - mean_squared_error: 0.1104 - val_loss: 0.8917 - val_mean_squared_error: 0.8861\n",
      "Epoch 94/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1159 - mean_squared_error: 0.1103 - val_loss: 0.9021 - val_mean_squared_error: 0.8966\n",
      "Epoch 95/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1164 - mean_squared_error: 0.1108 - val_loss: 0.9345 - val_mean_squared_error: 0.9290\n",
      "Epoch 96/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1158 - mean_squared_error: 0.1103 - val_loss: 0.9084 - val_mean_squared_error: 0.9029\n",
      "Epoch 97/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1149 - mean_squared_error: 0.1094 - val_loss: 0.9149 - val_mean_squared_error: 0.9094\n",
      "Epoch 98/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1152 - mean_squared_error: 0.1098 - val_loss: 0.9099 - val_mean_squared_error: 0.9045\n",
      "Epoch 99/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1150 - mean_squared_error: 0.1096 - val_loss: 0.9014 - val_mean_squared_error: 0.8960\n",
      "Epoch 100/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1141 - mean_squared_error: 0.1088 - val_loss: 0.9149 - val_mean_squared_error: 0.9096\n",
      "Epoch 101/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1136 - mean_squared_error: 0.1083 - val_loss: 0.9049 - val_mean_squared_error: 0.8996\n",
      "Epoch 102/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1130 - mean_squared_error: 0.1077 - val_loss: 0.9310 - val_mean_squared_error: 0.9258\n",
      "Epoch 103/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1122 - mean_squared_error: 0.1070 - val_loss: 0.9271 - val_mean_squared_error: 0.9220\n",
      "Epoch 104/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1116 - mean_squared_error: 0.1065 - val_loss: 0.9042 - val_mean_squared_error: 0.8991\n",
      "Epoch 105/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1119 - mean_squared_error: 0.1068 - val_loss: 0.9236 - val_mean_squared_error: 0.9185\n",
      "Epoch 106/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1115 - mean_squared_error: 0.1065 - val_loss: 0.9144 - val_mean_squared_error: 0.9094\n",
      "Epoch 107/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1116 - mean_squared_error: 0.1066 - val_loss: 0.9036 - val_mean_squared_error: 0.8987\n",
      "Epoch 108/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1114 - mean_squared_error: 0.1065 - val_loss: 0.9316 - val_mean_squared_error: 0.9267\n",
      "Epoch 109/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1109 - mean_squared_error: 0.1060 - val_loss: 0.9346 - val_mean_squared_error: 0.9297\n",
      "Epoch 110/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1112 - mean_squared_error: 0.1063 - val_loss: 0.8994 - val_mean_squared_error: 0.8946\n",
      "Epoch 111/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1127 - mean_squared_error: 0.1079 - val_loss: 0.9086 - val_mean_squared_error: 0.9038\n",
      "Epoch 112/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1136 - mean_squared_error: 0.1088 - val_loss: 0.9338 - val_mean_squared_error: 0.9290\n",
      "Epoch 113/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1127 - mean_squared_error: 0.1079 - val_loss: 0.9345 - val_mean_squared_error: 0.9298\n",
      "Epoch 114/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1101 - mean_squared_error: 0.1053 - val_loss: 0.9302 - val_mean_squared_error: 0.9255\n",
      "Epoch 115/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1090 - mean_squared_error: 0.1043 - val_loss: 0.9430 - val_mean_squared_error: 0.9383\n",
      "Epoch 116/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1086 - mean_squared_error: 0.1039 - val_loss: 0.9342 - val_mean_squared_error: 0.9295\n",
      "Epoch 117/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1076 - mean_squared_error: 0.1029 - val_loss: 0.9157 - val_mean_squared_error: 0.9111\n",
      "Epoch 118/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1069 - mean_squared_error: 0.1023 - val_loss: 0.9191 - val_mean_squared_error: 0.9145\n",
      "Epoch 119/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1068 - mean_squared_error: 0.1022 - val_loss: 0.9248 - val_mean_squared_error: 0.9203\n",
      "Epoch 120/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1070 - mean_squared_error: 0.1025 - val_loss: 0.9414 - val_mean_squared_error: 0.9369\n",
      "Epoch 121/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1070 - mean_squared_error: 0.1025 - val_loss: 0.9254 - val_mean_squared_error: 0.9209\n",
      "Epoch 122/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1073 - mean_squared_error: 0.1029 - val_loss: 0.9264 - val_mean_squared_error: 0.9219\n",
      "Epoch 123/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1076 - mean_squared_error: 0.1031 - val_loss: 0.9355 - val_mean_squared_error: 0.9311\n",
      "Epoch 124/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1076 - mean_squared_error: 0.1032 - val_loss: 0.9446 - val_mean_squared_error: 0.9402\n",
      "Epoch 125/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1073 - mean_squared_error: 0.1029 - val_loss: 0.9726 - val_mean_squared_error: 0.9682\n",
      "Epoch 126/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1068 - mean_squared_error: 0.1025 - val_loss: 0.9268 - val_mean_squared_error: 0.9225\n",
      "Epoch 127/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1061 - mean_squared_error: 0.1018 - val_loss: 0.9179 - val_mean_squared_error: 0.9137\n",
      "Epoch 128/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1059 - mean_squared_error: 0.1016 - val_loss: 0.9352 - val_mean_squared_error: 0.9310\n",
      "Epoch 129/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1054 - mean_squared_error: 0.1011 - val_loss: 0.9564 - val_mean_squared_error: 0.9522\n",
      "Epoch 130/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1049 - mean_squared_error: 0.1007 - val_loss: 0.9265 - val_mean_squared_error: 0.9223\n",
      "Epoch 131/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1044 - mean_squared_error: 0.1002 - val_loss: 0.9325 - val_mean_squared_error: 0.9283\n",
      "Epoch 132/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1037 - mean_squared_error: 0.0996 - val_loss: 0.9249 - val_mean_squared_error: 0.9208\n",
      "Epoch 133/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1038 - mean_squared_error: 0.0997 - val_loss: 0.9207 - val_mean_squared_error: 0.9167\n",
      "Epoch 134/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1045 - mean_squared_error: 0.1004 - val_loss: 0.9601 - val_mean_squared_error: 0.9560\n",
      "Epoch 135/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1075 - mean_squared_error: 0.1035 - val_loss: 0.8891 - val_mean_squared_error: 0.8851\n",
      "Epoch 136/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1062 - mean_squared_error: 0.1022 - val_loss: 0.9627 - val_mean_squared_error: 0.9587\n",
      "Epoch 137/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1045 - mean_squared_error: 0.1006 - val_loss: 0.9339 - val_mean_squared_error: 0.9299\n",
      "Epoch 138/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1038 - mean_squared_error: 0.0998 - val_loss: 0.9557 - val_mean_squared_error: 0.9518\n",
      "Epoch 139/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1039 - mean_squared_error: 0.0999 - val_loss: 0.9437 - val_mean_squared_error: 0.9398\n",
      "Epoch 140/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1039 - mean_squared_error: 0.1000 - val_loss: 0.9579 - val_mean_squared_error: 0.9541\n",
      "Epoch 141/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1036 - mean_squared_error: 0.0997 - val_loss: 0.9239 - val_mean_squared_error: 0.9201\n",
      "Epoch 142/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1032 - mean_squared_error: 0.0994 - val_loss: 0.9450 - val_mean_squared_error: 0.9412\n",
      "Epoch 143/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1033 - mean_squared_error: 0.0995 - val_loss: 0.9320 - val_mean_squared_error: 0.9282\n",
      "Epoch 144/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1031 - mean_squared_error: 0.0993 - val_loss: 0.9474 - val_mean_squared_error: 0.9436\n",
      "Epoch 145/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1028 - mean_squared_error: 0.0991 - val_loss: 0.9122 - val_mean_squared_error: 0.9084\n",
      "Epoch 146/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1025 - mean_squared_error: 0.0988 - val_loss: 0.9437 - val_mean_squared_error: 0.9400\n",
      "Epoch 147/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1029 - mean_squared_error: 0.0993 - val_loss: 0.9028 - val_mean_squared_error: 0.8992\n",
      "Epoch 148/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1038 - mean_squared_error: 0.1002 - val_loss: 0.9315 - val_mean_squared_error: 0.9279\n",
      "Epoch 149/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1028 - mean_squared_error: 0.0992 - val_loss: 0.9224 - val_mean_squared_error: 0.9188\n",
      "Epoch 150/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1031 - mean_squared_error: 0.0995 - val_loss: 0.9087 - val_mean_squared_error: 0.9051\n",
      "Epoch 151/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1036 - mean_squared_error: 0.1000 - val_loss: 0.9336 - val_mean_squared_error: 0.9301\n",
      "Epoch 152/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1035 - mean_squared_error: 0.1000 - val_loss: 0.9165 - val_mean_squared_error: 0.9130\n",
      "Epoch 153/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1028 - mean_squared_error: 0.0993 - val_loss: 0.9356 - val_mean_squared_error: 0.9321\n",
      "Epoch 154/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1036 - mean_squared_error: 0.1001 - val_loss: 0.9854 - val_mean_squared_error: 0.9819\n",
      "Epoch 155/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1033 - mean_squared_error: 0.0998 - val_loss: 0.9594 - val_mean_squared_error: 0.9559\n",
      "Epoch 156/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1021 - mean_squared_error: 0.0986 - val_loss: 0.9376 - val_mean_squared_error: 0.9342\n",
      "Epoch 157/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1016 - mean_squared_error: 0.0982 - val_loss: 0.8975 - val_mean_squared_error: 0.8941\n",
      "Epoch 158/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1021 - mean_squared_error: 0.0987 - val_loss: 0.9455 - val_mean_squared_error: 0.9421\n",
      "Epoch 159/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1017 - mean_squared_error: 0.0983 - val_loss: 0.9044 - val_mean_squared_error: 0.9010\n",
      "Epoch 160/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1014 - mean_squared_error: 0.0981 - val_loss: 0.9393 - val_mean_squared_error: 0.9360\n",
      "Epoch 161/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1013 - mean_squared_error: 0.0979 - val_loss: 0.9545 - val_mean_squared_error: 0.9512\n",
      "Epoch 162/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1012 - mean_squared_error: 0.0979 - val_loss: 0.9388 - val_mean_squared_error: 0.9355\n",
      "Epoch 163/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1013 - mean_squared_error: 0.0981 - val_loss: 0.9276 - val_mean_squared_error: 0.9244\n",
      "Epoch 164/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1016 - mean_squared_error: 0.0984 - val_loss: 0.9240 - val_mean_squared_error: 0.9208\n",
      "Epoch 165/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1018 - mean_squared_error: 0.0985 - val_loss: 0.9582 - val_mean_squared_error: 0.9550\n",
      "Epoch 166/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1017 - mean_squared_error: 0.0985 - val_loss: 0.9417 - val_mean_squared_error: 0.9385\n",
      "Epoch 167/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1018 - mean_squared_error: 0.0986 - val_loss: 0.9185 - val_mean_squared_error: 0.9154\n",
      "Epoch 168/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1019 - mean_squared_error: 0.0987 - val_loss: 0.9430 - val_mean_squared_error: 0.9399\n",
      "Epoch 169/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1018 - mean_squared_error: 0.0987 - val_loss: 0.9501 - val_mean_squared_error: 0.9470\n",
      "Epoch 170/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1022 - mean_squared_error: 0.0991 - val_loss: 0.9374 - val_mean_squared_error: 0.9343\n",
      "Epoch 171/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1018 - mean_squared_error: 0.0987 - val_loss: 0.9153 - val_mean_squared_error: 0.9122\n",
      "Epoch 172/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1012 - mean_squared_error: 0.0981 - val_loss: 0.9199 - val_mean_squared_error: 0.9168\n",
      "Epoch 173/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1008 - mean_squared_error: 0.0978 - val_loss: 0.9099 - val_mean_squared_error: 0.9069\n",
      "Epoch 174/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1006 - mean_squared_error: 0.0976 - val_loss: 0.9267 - val_mean_squared_error: 0.9237\n",
      "Epoch 175/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1009 - mean_squared_error: 0.0979 - val_loss: 0.9553 - val_mean_squared_error: 0.9523\n",
      "Epoch 176/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1009 - mean_squared_error: 0.0979 - val_loss: 0.9188 - val_mean_squared_error: 0.9159\n",
      "Epoch 177/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1010 - mean_squared_error: 0.0980 - val_loss: 0.9265 - val_mean_squared_error: 0.9236\n",
      "Epoch 178/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1019 - mean_squared_error: 0.0989 - val_loss: 0.9281 - val_mean_squared_error: 0.9251\n",
      "Epoch 179/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1026 - mean_squared_error: 0.0996 - val_loss: 0.8965 - val_mean_squared_error: 0.8936\n",
      "Epoch 180/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1041 - mean_squared_error: 0.1012 - val_loss: 0.9361 - val_mean_squared_error: 0.9332\n",
      "Epoch 181/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1023 - mean_squared_error: 0.0994 - val_loss: 0.9454 - val_mean_squared_error: 0.9425\n",
      "Epoch 182/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1011 - mean_squared_error: 0.0982 - val_loss: 0.9510 - val_mean_squared_error: 0.9481\n",
      "Epoch 183/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1006 - mean_squared_error: 0.0977 - val_loss: 0.9321 - val_mean_squared_error: 0.9292\n",
      "Epoch 184/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0998 - mean_squared_error: 0.0969 - val_loss: 0.9370 - val_mean_squared_error: 0.9342\n",
      "Epoch 185/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0994 - mean_squared_error: 0.0965 - val_loss: 0.9065 - val_mean_squared_error: 0.9037\n",
      "Epoch 186/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0992 - mean_squared_error: 0.0964 - val_loss: 0.9132 - val_mean_squared_error: 0.9104\n",
      "Epoch 187/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0965 - val_loss: 0.9052 - val_mean_squared_error: 0.9025\n",
      "Epoch 188/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0994 - mean_squared_error: 0.0967 - val_loss: 0.9313 - val_mean_squared_error: 0.9285\n",
      "Epoch 189/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0995 - mean_squared_error: 0.0968 - val_loss: 0.9034 - val_mean_squared_error: 0.9007\n",
      "Epoch 190/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0998 - mean_squared_error: 0.0971 - val_loss: 0.9083 - val_mean_squared_error: 0.9056\n",
      "Epoch 191/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0972 - val_loss: 0.9487 - val_mean_squared_error: 0.9460\n",
      "Epoch 192/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1007 - mean_squared_error: 0.0980 - val_loss: 0.9310 - val_mean_squared_error: 0.9283\n",
      "Epoch 193/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1020 - mean_squared_error: 0.0993 - val_loss: 0.9487 - val_mean_squared_error: 0.9460\n",
      "Epoch 194/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1027 - mean_squared_error: 0.1000 - val_loss: 0.9530 - val_mean_squared_error: 0.9503\n",
      "Epoch 195/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1020 - mean_squared_error: 0.0994 - val_loss: 0.9207 - val_mean_squared_error: 0.9180\n",
      "Epoch 196/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1012 - mean_squared_error: 0.0985 - val_loss: 0.9348 - val_mean_squared_error: 0.9322\n",
      "Epoch 197/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1005 - mean_squared_error: 0.0979 - val_loss: 0.9093 - val_mean_squared_error: 0.9067\n",
      "Epoch 198/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0970 - val_loss: 0.9267 - val_mean_squared_error: 0.9240\n",
      "Epoch 199/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0967 - val_loss: 0.9366 - val_mean_squared_error: 0.9340\n",
      "Epoch 200/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0967 - val_loss: 0.9225 - val_mean_squared_error: 0.9199\n",
      "Epoch 201/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0966 - val_loss: 0.9071 - val_mean_squared_error: 0.9045\n",
      "Epoch 202/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0965 - val_loss: 0.9441 - val_mean_squared_error: 0.9416\n",
      "Epoch 203/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0971 - val_loss: 0.9029 - val_mean_squared_error: 0.9003\n",
      "Epoch 204/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0974 - val_loss: 0.9335 - val_mean_squared_error: 0.9310\n",
      "Epoch 205/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1003 - mean_squared_error: 0.0978 - val_loss: 0.9342 - val_mean_squared_error: 0.9317\n",
      "Epoch 206/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1003 - mean_squared_error: 0.0978 - val_loss: 0.8985 - val_mean_squared_error: 0.8960\n",
      "Epoch 207/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1003 - mean_squared_error: 0.0978 - val_loss: 0.9173 - val_mean_squared_error: 0.9149\n",
      "Epoch 208/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1007 - mean_squared_error: 0.0983 - val_loss: 0.9007 - val_mean_squared_error: 0.8982\n",
      "Epoch 209/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1008 - mean_squared_error: 0.0983 - val_loss: 0.8976 - val_mean_squared_error: 0.8952\n",
      "Epoch 210/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0971 - val_loss: 0.9075 - val_mean_squared_error: 0.9051\n",
      "Epoch 211/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0966 - val_loss: 0.9210 - val_mean_squared_error: 0.9185\n",
      "Epoch 212/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0967 - val_loss: 0.8762 - val_mean_squared_error: 0.8738\n",
      "Epoch 213/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0967 - val_loss: 0.9172 - val_mean_squared_error: 0.9148\n",
      "Epoch 214/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0963 - val_loss: 0.9253 - val_mean_squared_error: 0.9229\n",
      "Epoch 215/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0965 - val_loss: 0.8952 - val_mean_squared_error: 0.8928\n",
      "Epoch 216/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0992 - mean_squared_error: 0.0969 - val_loss: 0.9113 - val_mean_squared_error: 0.9090\n",
      "Epoch 217/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0973 - val_loss: 0.9017 - val_mean_squared_error: 0.8994\n",
      "Epoch 218/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0976 - val_loss: 0.9275 - val_mean_squared_error: 0.9252\n",
      "Epoch 219/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1002 - mean_squared_error: 0.0979 - val_loss: 0.9308 - val_mean_squared_error: 0.9284\n",
      "Epoch 220/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1001 - mean_squared_error: 0.0978 - val_loss: 0.9234 - val_mean_squared_error: 0.9211\n",
      "Epoch 221/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0976 - val_loss: 0.9225 - val_mean_squared_error: 0.9202\n",
      "Epoch 222/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0970 - val_loss: 0.9388 - val_mean_squared_error: 0.9365\n",
      "Epoch 223/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0992 - mean_squared_error: 0.0970 - val_loss: 0.8852 - val_mean_squared_error: 0.8830\n",
      "Epoch 224/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0966 - val_loss: 0.9276 - val_mean_squared_error: 0.9253\n",
      "Epoch 225/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0967 - val_loss: 0.9048 - val_mean_squared_error: 0.9026\n",
      "Epoch 226/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0988 - mean_squared_error: 0.0966 - val_loss: 0.9310 - val_mean_squared_error: 0.9288\n",
      "Epoch 227/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0971 - val_loss: 0.9007 - val_mean_squared_error: 0.8984\n",
      "Epoch 228/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0967 - val_loss: 0.8830 - val_mean_squared_error: 0.8808\n",
      "Epoch 229/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0968 - val_loss: 0.9093 - val_mean_squared_error: 0.9071\n",
      "Epoch 230/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0975 - val_loss: 0.8949 - val_mean_squared_error: 0.8927\n",
      "Epoch 231/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1005 - mean_squared_error: 0.0983 - val_loss: 0.8910 - val_mean_squared_error: 0.8888\n",
      "Epoch 232/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1001 - mean_squared_error: 0.0979 - val_loss: 0.8875 - val_mean_squared_error: 0.8853\n",
      "Epoch 233/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0988 - mean_squared_error: 0.0967 - val_loss: 0.9286 - val_mean_squared_error: 0.9264\n",
      "Epoch 234/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0963 - val_loss: 0.9075 - val_mean_squared_error: 0.9054\n",
      "Epoch 235/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0994 - mean_squared_error: 0.0972 - val_loss: 0.8899 - val_mean_squared_error: 0.8878\n",
      "Epoch 236/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0996 - mean_squared_error: 0.0975 - val_loss: 0.8970 - val_mean_squared_error: 0.8948\n",
      "Epoch 237/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0969 - val_loss: 0.9082 - val_mean_squared_error: 0.9061\n",
      "Epoch 238/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0962 - val_loss: 0.9056 - val_mean_squared_error: 0.9034\n",
      "Epoch 239/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0961 - val_loss: 0.9203 - val_mean_squared_error: 0.9182\n",
      "Epoch 240/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0961 - val_loss: 0.9021 - val_mean_squared_error: 0.9000\n",
      "Epoch 241/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0962 - val_loss: 0.9075 - val_mean_squared_error: 0.9054\n",
      "Epoch 242/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0981 - mean_squared_error: 0.0961 - val_loss: 0.9012 - val_mean_squared_error: 0.8991\n",
      "Epoch 243/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0965 - val_loss: 0.9220 - val_mean_squared_error: 0.9199\n",
      "Epoch 244/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0967 - val_loss: 0.9075 - val_mean_squared_error: 0.9055\n",
      "Epoch 245/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0988 - mean_squared_error: 0.0968 - val_loss: 0.8814 - val_mean_squared_error: 0.8793\n",
      "Epoch 246/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1001 - mean_squared_error: 0.0981 - val_loss: 0.9159 - val_mean_squared_error: 0.9139\n",
      "Epoch 247/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0995 - mean_squared_error: 0.0975 - val_loss: 0.9189 - val_mean_squared_error: 0.9169\n",
      "Epoch 248/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0971 - val_loss: 0.8752 - val_mean_squared_error: 0.8732\n",
      "Epoch 249/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0970 - val_loss: 0.8795 - val_mean_squared_error: 0.8776\n",
      "Epoch 250/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0993 - mean_squared_error: 0.0973 - val_loss: 0.9103 - val_mean_squared_error: 0.9083\n",
      "Epoch 251/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0991 - mean_squared_error: 0.0971 - val_loss: 0.8788 - val_mean_squared_error: 0.8768\n",
      "Epoch 252/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0964 - val_loss: 0.8920 - val_mean_squared_error: 0.8901\n",
      "Epoch 253/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0965 - val_loss: 0.8891 - val_mean_squared_error: 0.8871\n",
      "Epoch 254/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0961 - val_loss: 0.9136 - val_mean_squared_error: 0.9116\n",
      "Epoch 255/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0958 - val_loss: 0.8881 - val_mean_squared_error: 0.8861\n",
      "Epoch 256/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0979 - mean_squared_error: 0.0959 - val_loss: 0.9021 - val_mean_squared_error: 0.9002\n",
      "Epoch 257/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0979 - mean_squared_error: 0.0959 - val_loss: 0.8970 - val_mean_squared_error: 0.8951\n",
      "Epoch 258/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0976 - mean_squared_error: 0.0957 - val_loss: 0.8953 - val_mean_squared_error: 0.8933\n",
      "Epoch 259/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0963 - val_loss: 0.8988 - val_mean_squared_error: 0.8969\n",
      "Epoch 260/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0966 - val_loss: 0.9033 - val_mean_squared_error: 0.9015\n",
      "Epoch 261/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0981 - mean_squared_error: 0.0962 - val_loss: 0.9171 - val_mean_squared_error: 0.9152\n",
      "Epoch 262/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0961 - val_loss: 0.9110 - val_mean_squared_error: 0.9091\n",
      "Epoch 263/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0979 - mean_squared_error: 0.0961 - val_loss: 0.9267 - val_mean_squared_error: 0.9248\n",
      "Epoch 264/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0981 - mean_squared_error: 0.0962 - val_loss: 0.8900 - val_mean_squared_error: 0.8881\n",
      "Epoch 265/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0964 - val_loss: 0.9196 - val_mean_squared_error: 0.9178\n",
      "Epoch 266/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0966 - val_loss: 0.9421 - val_mean_squared_error: 0.9402\n",
      "Epoch 267/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0969 - val_loss: 0.9196 - val_mean_squared_error: 0.9177\n",
      "Epoch 268/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0971 - val_loss: 0.9257 - val_mean_squared_error: 0.9239\n",
      "Epoch 269/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0966 - val_loss: 0.9037 - val_mean_squared_error: 0.9019\n",
      "Epoch 270/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0967 - val_loss: 0.8659 - val_mean_squared_error: 0.8641\n",
      "Epoch 271/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0969 - val_loss: 0.8885 - val_mean_squared_error: 0.8867\n",
      "Epoch 272/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0995 - mean_squared_error: 0.0977 - val_loss: 0.8713 - val_mean_squared_error: 0.8695\n",
      "Epoch 273/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0971 - val_loss: 0.8862 - val_mean_squared_error: 0.8844\n",
      "Epoch 274/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0962 - val_loss: 0.8927 - val_mean_squared_error: 0.8910\n",
      "Epoch 275/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0960 - val_loss: 0.9033 - val_mean_squared_error: 0.9016\n",
      "Epoch 276/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0965 - val_loss: 0.9063 - val_mean_squared_error: 0.9045\n",
      "Epoch 277/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0960 - val_loss: 0.9027 - val_mean_squared_error: 0.9010\n",
      "Epoch 278/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0957 - val_loss: 0.9132 - val_mean_squared_error: 0.9114\n",
      "Epoch 279/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0985 - mean_squared_error: 0.0968 - val_loss: 0.8989 - val_mean_squared_error: 0.8972\n",
      "Epoch 280/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1003 - mean_squared_error: 0.0986 - val_loss: 0.8936 - val_mean_squared_error: 0.8919\n",
      "Epoch 281/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0972 - val_loss: 0.9447 - val_mean_squared_error: 0.9430\n",
      "Epoch 282/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0964 - val_loss: 0.8972 - val_mean_squared_error: 0.8955\n",
      "Epoch 283/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0986 - mean_squared_error: 0.0968 - val_loss: 0.9098 - val_mean_squared_error: 0.9080\n",
      "Epoch 284/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0981 - mean_squared_error: 0.0964 - val_loss: 0.9090 - val_mean_squared_error: 0.9073\n",
      "Epoch 285/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0965 - val_loss: 0.8948 - val_mean_squared_error: 0.8931\n",
      "Epoch 286/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0988 - mean_squared_error: 0.0971 - val_loss: 0.8834 - val_mean_squared_error: 0.8817\n",
      "Epoch 287/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0973 - val_loss: 0.8729 - val_mean_squared_error: 0.8712\n",
      "Epoch 288/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0970 - val_loss: 0.9016 - val_mean_squared_error: 0.8999\n",
      "Epoch 289/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0974 - mean_squared_error: 0.0956 - val_loss: 0.8951 - val_mean_squared_error: 0.8934\n",
      "Epoch 290/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0955 - val_loss: 0.8704 - val_mean_squared_error: 0.8687\n",
      "Epoch 291/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0955 - val_loss: 0.8759 - val_mean_squared_error: 0.8742\n",
      "Epoch 292/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0969 - mean_squared_error: 0.0953 - val_loss: 0.8910 - val_mean_squared_error: 0.8894\n",
      "Epoch 293/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0965 - mean_squared_error: 0.0948 - val_loss: 0.9017 - val_mean_squared_error: 0.9000\n",
      "Epoch 294/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0968 - mean_squared_error: 0.0951 - val_loss: 0.8837 - val_mean_squared_error: 0.8821\n",
      "Epoch 295/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0973 - mean_squared_error: 0.0957 - val_loss: 0.8955 - val_mean_squared_error: 0.8939\n",
      "Epoch 296/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0954 - val_loss: 0.9101 - val_mean_squared_error: 0.9085\n",
      "Epoch 297/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0969 - mean_squared_error: 0.0953 - val_loss: 0.8867 - val_mean_squared_error: 0.8851\n",
      "Epoch 298/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0956 - val_loss: 0.8901 - val_mean_squared_error: 0.8884\n",
      "Epoch 299/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0977 - mean_squared_error: 0.0961 - val_loss: 0.8823 - val_mean_squared_error: 0.8807\n",
      "Epoch 300/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0962 - val_loss: 0.9103 - val_mean_squared_error: 0.9087\n",
      "Epoch 301/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0986 - mean_squared_error: 0.0970 - val_loss: 0.9131 - val_mean_squared_error: 0.9115\n",
      "Epoch 302/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0992 - mean_squared_error: 0.0976 - val_loss: 0.8693 - val_mean_squared_error: 0.8677\n",
      "Epoch 303/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0999 - mean_squared_error: 0.0983 - val_loss: 0.8972 - val_mean_squared_error: 0.8956\n",
      "Epoch 304/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0968 - val_loss: 0.8903 - val_mean_squared_error: 0.8887\n",
      "Epoch 305/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0967 - val_loss: 0.9083 - val_mean_squared_error: 0.9067\n",
      "Epoch 306/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0962 - val_loss: 0.9023 - val_mean_squared_error: 0.9007\n",
      "Epoch 307/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0974 - mean_squared_error: 0.0958 - val_loss: 0.8851 - val_mean_squared_error: 0.8835\n",
      "Epoch 308/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0956 - val_loss: 0.8963 - val_mean_squared_error: 0.8947\n",
      "Epoch 309/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0954 - val_loss: 0.9135 - val_mean_squared_error: 0.9119\n",
      "Epoch 310/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0971 - mean_squared_error: 0.0955 - val_loss: 0.8957 - val_mean_squared_error: 0.8942\n",
      "Epoch 311/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0957 - val_loss: 0.8896 - val_mean_squared_error: 0.8880\n",
      "Epoch 312/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0957 - val_loss: 0.9047 - val_mean_squared_error: 0.9032\n",
      "Epoch 313/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0955 - val_loss: 0.8693 - val_mean_squared_error: 0.8677\n",
      "Epoch 314/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0974 - val_loss: 0.9138 - val_mean_squared_error: 0.9122\n",
      "Epoch 315/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0992 - mean_squared_error: 0.0977 - val_loss: 0.8947 - val_mean_squared_error: 0.8931\n",
      "Epoch 316/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0965 - val_loss: 0.9167 - val_mean_squared_error: 0.9152\n",
      "Epoch 317/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0981 - mean_squared_error: 0.0965 - val_loss: 0.8973 - val_mean_squared_error: 0.8957\n",
      "Epoch 318/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0982 - mean_squared_error: 0.0967 - val_loss: 0.9183 - val_mean_squared_error: 0.9168\n",
      "Epoch 319/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0986 - mean_squared_error: 0.0970 - val_loss: 0.8854 - val_mean_squared_error: 0.8839\n",
      "Epoch 320/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0974 - mean_squared_error: 0.0958 - val_loss: 0.8802 - val_mean_squared_error: 0.8787\n",
      "Epoch 321/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0968 - mean_squared_error: 0.0953 - val_loss: 0.8897 - val_mean_squared_error: 0.8882\n",
      "Epoch 322/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0968 - mean_squared_error: 0.0953 - val_loss: 0.8952 - val_mean_squared_error: 0.8937\n",
      "Epoch 323/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0957 - val_loss: 0.8773 - val_mean_squared_error: 0.8758\n",
      "Epoch 324/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0957 - val_loss: 0.9118 - val_mean_squared_error: 0.9103\n",
      "Epoch 325/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0955 - val_loss: 0.8911 - val_mean_squared_error: 0.8896\n",
      "Epoch 326/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0958 - val_loss: 0.9080 - val_mean_squared_error: 0.9065\n",
      "Epoch 327/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0971 - mean_squared_error: 0.0956 - val_loss: 0.9043 - val_mean_squared_error: 0.9028\n",
      "Epoch 328/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0968 - mean_squared_error: 0.0953 - val_loss: 0.8899 - val_mean_squared_error: 0.8884\n",
      "Epoch 329/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0953 - val_loss: 0.9123 - val_mean_squared_error: 0.9108\n",
      "Epoch 330/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0951 - val_loss: 0.9127 - val_mean_squared_error: 0.9113\n",
      "Epoch 331/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0969 - mean_squared_error: 0.0954 - val_loss: 0.9132 - val_mean_squared_error: 0.9118\n",
      "Epoch 332/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0973 - mean_squared_error: 0.0959 - val_loss: 0.8557 - val_mean_squared_error: 0.8543\n",
      "Epoch 333/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0979 - mean_squared_error: 0.0965 - val_loss: 0.9006 - val_mean_squared_error: 0.8991\n",
      "Epoch 334/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0976 - val_loss: 0.9087 - val_mean_squared_error: 0.9073\n",
      "Epoch 335/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1002 - mean_squared_error: 0.0988 - val_loss: 0.9258 - val_mean_squared_error: 0.9244\n",
      "Epoch 336/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0995 - mean_squared_error: 0.0981 - val_loss: 0.8812 - val_mean_squared_error: 0.8797\n",
      "Epoch 337/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0969 - val_loss: 0.8882 - val_mean_squared_error: 0.8867\n",
      "Epoch 338/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0983 - mean_squared_error: 0.0968 - val_loss: 0.8781 - val_mean_squared_error: 0.8767\n",
      "Epoch 339/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0960 - val_loss: 0.8806 - val_mean_squared_error: 0.8791\n",
      "Epoch 340/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0964 - mean_squared_error: 0.0950 - val_loss: 0.8941 - val_mean_squared_error: 0.8927\n",
      "Epoch 341/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0947 - val_loss: 0.8850 - val_mean_squared_error: 0.8836\n",
      "Epoch 342/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0960 - mean_squared_error: 0.0946 - val_loss: 0.8863 - val_mean_squared_error: 0.8849\n",
      "Epoch 343/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0948 - val_loss: 0.8845 - val_mean_squared_error: 0.8831\n",
      "Epoch 344/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0948 - val_loss: 0.8752 - val_mean_squared_error: 0.8738\n",
      "Epoch 345/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0952 - val_loss: 0.8886 - val_mean_squared_error: 0.8872\n",
      "Epoch 346/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0953 - val_loss: 0.8701 - val_mean_squared_error: 0.8687\n",
      "Epoch 347/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0973 - mean_squared_error: 0.0959 - val_loss: 0.9109 - val_mean_squared_error: 0.9095\n",
      "Epoch 348/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0961 - val_loss: 0.8911 - val_mean_squared_error: 0.8897\n",
      "Epoch 349/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0990 - mean_squared_error: 0.0976 - val_loss: 0.8652 - val_mean_squared_error: 0.8638\n",
      "Epoch 350/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0987 - mean_squared_error: 0.0973 - val_loss: 0.8858 - val_mean_squared_error: 0.8845\n",
      "Epoch 351/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0966 - val_loss: 0.8858 - val_mean_squared_error: 0.8844\n",
      "Epoch 352/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0972 - mean_squared_error: 0.0958 - val_loss: 0.8918 - val_mean_squared_error: 0.8905\n",
      "Epoch 353/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0956 - val_loss: 0.9044 - val_mean_squared_error: 0.9031\n",
      "Epoch 354/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0976 - mean_squared_error: 0.0962 - val_loss: 0.8630 - val_mean_squared_error: 0.8616\n",
      "Epoch 355/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0989 - mean_squared_error: 0.0975 - val_loss: 0.8846 - val_mean_squared_error: 0.8832\n",
      "Epoch 356/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0976 - mean_squared_error: 0.0963 - val_loss: 0.8869 - val_mean_squared_error: 0.8855\n",
      "Epoch 357/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0953 - val_loss: 0.8700 - val_mean_squared_error: 0.8687\n",
      "Epoch 358/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0968 - mean_squared_error: 0.0954 - val_loss: 0.8937 - val_mean_squared_error: 0.8923\n",
      "Epoch 359/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0964 - mean_squared_error: 0.0951 - val_loss: 0.8804 - val_mean_squared_error: 0.8790\n",
      "Epoch 360/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0965 - mean_squared_error: 0.0952 - val_loss: 0.8823 - val_mean_squared_error: 0.8810\n",
      "Epoch 361/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0965 - mean_squared_error: 0.0952 - val_loss: 0.8965 - val_mean_squared_error: 0.8952\n",
      "Epoch 362/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0953 - val_loss: 0.8850 - val_mean_squared_error: 0.8836\n",
      "Epoch 363/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0953 - val_loss: 0.8783 - val_mean_squared_error: 0.8769\n",
      "Epoch 364/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0953 - val_loss: 0.9004 - val_mean_squared_error: 0.8991\n",
      "Epoch 365/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0953 - val_loss: 0.8909 - val_mean_squared_error: 0.8895\n",
      "Epoch 366/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0953 - val_loss: 0.8933 - val_mean_squared_error: 0.8919\n",
      "Epoch 367/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0964 - mean_squared_error: 0.0951 - val_loss: 0.9037 - val_mean_squared_error: 0.9024\n",
      "Epoch 368/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0954 - val_loss: 0.8821 - val_mean_squared_error: 0.8808\n",
      "Epoch 369/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0965 - val_loss: 0.8748 - val_mean_squared_error: 0.8735\n",
      "Epoch 370/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.1000 - mean_squared_error: 0.0987 - val_loss: 0.8976 - val_mean_squared_error: 0.8963\n",
      "Epoch 371/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0984 - mean_squared_error: 0.0971 - val_loss: 0.9271 - val_mean_squared_error: 0.9258\n",
      "Epoch 372/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0976 - mean_squared_error: 0.0963 - val_loss: 0.9058 - val_mean_squared_error: 0.9045\n",
      "Epoch 373/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0962 - val_loss: 0.8932 - val_mean_squared_error: 0.8919\n",
      "Epoch 374/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0957 - val_loss: 0.8769 - val_mean_squared_error: 0.8756\n",
      "Epoch 375/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0963 - mean_squared_error: 0.0950 - val_loss: 0.8835 - val_mean_squared_error: 0.8822\n",
      "Epoch 376/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0964 - mean_squared_error: 0.0951 - val_loss: 0.8853 - val_mean_squared_error: 0.8840\n",
      "Epoch 377/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0948 - val_loss: 0.8993 - val_mean_squared_error: 0.8980\n",
      "Epoch 378/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0960 - mean_squared_error: 0.0947 - val_loss: 0.8912 - val_mean_squared_error: 0.8900\n",
      "Epoch 379/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0948 - val_loss: 0.8945 - val_mean_squared_error: 0.8932\n",
      "Epoch 380/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0949 - val_loss: 0.8810 - val_mean_squared_error: 0.8798\n",
      "Epoch 381/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0970 - mean_squared_error: 0.0958 - val_loss: 0.8651 - val_mean_squared_error: 0.8639\n",
      "Epoch 382/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0998 - mean_squared_error: 0.0986 - val_loss: 0.9036 - val_mean_squared_error: 0.9024\n",
      "Epoch 383/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0978 - mean_squared_error: 0.0966 - val_loss: 0.8816 - val_mean_squared_error: 0.8803\n",
      "Epoch 384/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0997 - mean_squared_error: 0.0984 - val_loss: 0.8779 - val_mean_squared_error: 0.8767\n",
      "Epoch 385/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0962 - val_loss: 0.9149 - val_mean_squared_error: 0.9136\n",
      "Epoch 386/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0976 - mean_squared_error: 0.0964 - val_loss: 0.8714 - val_mean_squared_error: 0.8701\n",
      "Epoch 387/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0950 - val_loss: 0.8902 - val_mean_squared_error: 0.8890\n",
      "Epoch 388/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0960 - mean_squared_error: 0.0947 - val_loss: 0.8808 - val_mean_squared_error: 0.8795\n",
      "Epoch 389/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0958 - mean_squared_error: 0.0946 - val_loss: 0.8802 - val_mean_squared_error: 0.8790\n",
      "Epoch 390/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0956 - mean_squared_error: 0.0944 - val_loss: 0.8765 - val_mean_squared_error: 0.8753\n",
      "Epoch 391/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0957 - mean_squared_error: 0.0945 - val_loss: 0.8858 - val_mean_squared_error: 0.8845\n",
      "Epoch 392/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0949 - val_loss: 0.8816 - val_mean_squared_error: 0.8804\n",
      "Epoch 393/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0949 - val_loss: 0.8697 - val_mean_squared_error: 0.8685\n",
      "Epoch 394/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0963 - mean_squared_error: 0.0951 - val_loss: 0.8784 - val_mean_squared_error: 0.8772\n",
      "Epoch 395/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0954 - val_loss: 0.8885 - val_mean_squared_error: 0.8873\n",
      "Epoch 396/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0980 - mean_squared_error: 0.0968 - val_loss: 0.8879 - val_mean_squared_error: 0.8866\n",
      "Epoch 397/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0986 - mean_squared_error: 0.0974 - val_loss: 0.8753 - val_mean_squared_error: 0.8741\n",
      "Epoch 398/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0973 - mean_squared_error: 0.0961 - val_loss: 0.8522 - val_mean_squared_error: 0.8510\n",
      "Epoch 399/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0971 - mean_squared_error: 0.0959 - val_loss: 0.8743 - val_mean_squared_error: 0.8731\n",
      "Epoch 400/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0975 - mean_squared_error: 0.0963 - val_loss: 0.8737 - val_mean_squared_error: 0.8725\n",
      "Epoch 401/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0954 - val_loss: 0.8860 - val_mean_squared_error: 0.8848\n",
      "Epoch 402/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0963 - mean_squared_error: 0.0951 - val_loss: 0.8814 - val_mean_squared_error: 0.8802\n",
      "Epoch 403/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0967 - mean_squared_error: 0.0955 - val_loss: 0.8706 - val_mean_squared_error: 0.8694\n",
      "Epoch 404/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0950 - val_loss: 0.8636 - val_mean_squared_error: 0.8624\n",
      "Epoch 405/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0950 - val_loss: 0.8975 - val_mean_squared_error: 0.8963\n",
      "Epoch 406/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0965 - mean_squared_error: 0.0953 - val_loss: 0.8703 - val_mean_squared_error: 0.8691\n",
      "Epoch 407/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0949 - val_loss: 0.8725 - val_mean_squared_error: 0.8714\n",
      "Epoch 408/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0963 - mean_squared_error: 0.0951 - val_loss: 0.8820 - val_mean_squared_error: 0.8808\n",
      "Epoch 409/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0958 - mean_squared_error: 0.0947 - val_loss: 0.8726 - val_mean_squared_error: 0.8714\n",
      "Epoch 410/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0962 - mean_squared_error: 0.0950 - val_loss: 0.8960 - val_mean_squared_error: 0.8949\n",
      "Epoch 411/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0961 - mean_squared_error: 0.0949 - val_loss: 0.8915 - val_mean_squared_error: 0.8903\n",
      "Epoch 412/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0966 - mean_squared_error: 0.0955 - val_loss: 0.9043 - val_mean_squared_error: 0.9031\n",
      "Epoch 413/5000\n",
      "94/94 [==============================] - 1s 6ms/step - loss: 0.0969 - mean_squared_error: 0.0958 - val_loss: 0.8522 - val_mean_squared_error: 0.8510\n",
      "103/103 [==============================] - 0s 2ms/step - loss: 0.1655 - mean_squared_error: 0.1643\n",
      "Epoch 1/5000\n",
      "94/94 [==============================] - 8s 24ms/step - loss: 6.5728 - mean_squared_error: 6.5669 - val_loss: 1.0912 - val_mean_squared_error: 1.0840\n",
      "Epoch 2/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.6070 - mean_squared_error: 0.5995 - val_loss: 0.7326 - val_mean_squared_error: 0.7250\n",
      "Epoch 3/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.5369 - mean_squared_error: 0.5294 - val_loss: 0.6921 - val_mean_squared_error: 0.6846\n",
      "Epoch 4/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.5363 - mean_squared_error: 0.5288 - val_loss: 0.6896 - val_mean_squared_error: 0.6821\n",
      "Epoch 5/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.5365 - mean_squared_error: 0.5290 - val_loss: 0.6839 - val_mean_squared_error: 0.6765\n",
      "Epoch 6/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.5365 - mean_squared_error: 0.5291 - val_loss: 0.6631 - val_mean_squared_error: 0.6557\n",
      "Epoch 7/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5356 - mean_squared_error: 0.5282 - val_loss: 0.7116 - val_mean_squared_error: 0.7042\n",
      "Epoch 8/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5363 - mean_squared_error: 0.5290 - val_loss: 0.7023 - val_mean_squared_error: 0.6949\n",
      "Epoch 9/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5353 - mean_squared_error: 0.5280 - val_loss: 0.6734 - val_mean_squared_error: 0.6661\n",
      "Epoch 10/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5340 - mean_squared_error: 0.5267 - val_loss: 0.7003 - val_mean_squared_error: 0.6931\n",
      "Epoch 11/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5313 - mean_squared_error: 0.5241 - val_loss: 0.7067 - val_mean_squared_error: 0.6995\n",
      "Epoch 12/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.5276 - mean_squared_error: 0.5204 - val_loss: 0.6607 - val_mean_squared_error: 0.6535\n",
      "Epoch 13/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5209 - mean_squared_error: 0.5137 - val_loss: 0.6767 - val_mean_squared_error: 0.6695\n",
      "Epoch 14/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.5169 - mean_squared_error: 0.5097 - val_loss: 0.7224 - val_mean_squared_error: 0.7152\n",
      "Epoch 15/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4994 - mean_squared_error: 0.4921 - val_loss: 0.7044 - val_mean_squared_error: 0.6972\n",
      "Epoch 16/5000\n",
      "94/94 [==============================] - 1s 9ms/step - loss: 0.4830 - mean_squared_error: 0.4758 - val_loss: 0.6548 - val_mean_squared_error: 0.6475\n",
      "Epoch 17/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4760 - mean_squared_error: 0.4687 - val_loss: 0.7522 - val_mean_squared_error: 0.7448\n",
      "Epoch 18/5000\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 0.4490 - mean_squared_error: 0.4416 - val_loss: 0.7163 - val_mean_squared_error: 0.7088\n",
      "Epoch 19/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.4372 - mean_squared_error: 0.4297 - val_loss: 0.7181 - val_mean_squared_error: 0.7106\n",
      "Epoch 20/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.4199 - mean_squared_error: 0.4123 - val_loss: 0.7317 - val_mean_squared_error: 0.7242\n",
      "Epoch 21/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.4094 - mean_squared_error: 0.4018 - val_loss: 0.7326 - val_mean_squared_error: 0.7250\n",
      "Epoch 22/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3943 - mean_squared_error: 0.3867 - val_loss: 0.7760 - val_mean_squared_error: 0.7684\n",
      "Epoch 23/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3856 - mean_squared_error: 0.3779 - val_loss: 0.7964 - val_mean_squared_error: 0.7887\n",
      "Epoch 24/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3796 - mean_squared_error: 0.3720 - val_loss: 0.7647 - val_mean_squared_error: 0.7571\n",
      "Epoch 25/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3733 - mean_squared_error: 0.3657 - val_loss: 0.7693 - val_mean_squared_error: 0.7617\n",
      "Epoch 26/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3633 - mean_squared_error: 0.3557 - val_loss: 0.8232 - val_mean_squared_error: 0.8156\n",
      "Epoch 27/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3531 - mean_squared_error: 0.3456 - val_loss: 0.8638 - val_mean_squared_error: 0.8562\n",
      "Epoch 28/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3523 - mean_squared_error: 0.3447 - val_loss: 0.8177 - val_mean_squared_error: 0.8102\n",
      "Epoch 29/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3446 - mean_squared_error: 0.3371 - val_loss: 0.8160 - val_mean_squared_error: 0.8085\n",
      "Epoch 30/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3324 - mean_squared_error: 0.3249 - val_loss: 0.7892 - val_mean_squared_error: 0.7817\n",
      "Epoch 31/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3252 - mean_squared_error: 0.3177 - val_loss: 0.7983 - val_mean_squared_error: 0.7908\n",
      "Epoch 32/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3198 - mean_squared_error: 0.3123 - val_loss: 0.8403 - val_mean_squared_error: 0.8328\n",
      "Epoch 33/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3128 - mean_squared_error: 0.3053 - val_loss: 0.8257 - val_mean_squared_error: 0.8182\n",
      "Epoch 34/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.3035 - mean_squared_error: 0.2961 - val_loss: 0.8672 - val_mean_squared_error: 0.8597\n",
      "Epoch 35/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2961 - mean_squared_error: 0.2886 - val_loss: 0.9211 - val_mean_squared_error: 0.9136\n",
      "Epoch 36/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2893 - mean_squared_error: 0.2818 - val_loss: 0.8996 - val_mean_squared_error: 0.8921\n",
      "Epoch 37/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2831 - mean_squared_error: 0.2756 - val_loss: 0.8957 - val_mean_squared_error: 0.8882\n",
      "Epoch 38/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2783 - mean_squared_error: 0.2708 - val_loss: 0.9469 - val_mean_squared_error: 0.9394\n",
      "Epoch 39/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2746 - mean_squared_error: 0.2672 - val_loss: 0.8893 - val_mean_squared_error: 0.8819\n",
      "Epoch 40/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2855 - mean_squared_error: 0.2781 - val_loss: 0.9329 - val_mean_squared_error: 0.9255\n",
      "Epoch 41/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2700 - mean_squared_error: 0.2626 - val_loss: 0.8949 - val_mean_squared_error: 0.8876\n",
      "Epoch 42/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2623 - mean_squared_error: 0.2549 - val_loss: 0.9223 - val_mean_squared_error: 0.9149\n",
      "Epoch 43/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2588 - mean_squared_error: 0.2514 - val_loss: 0.9553 - val_mean_squared_error: 0.9479\n",
      "Epoch 44/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2566 - mean_squared_error: 0.2492 - val_loss: 1.0069 - val_mean_squared_error: 0.9996\n",
      "Epoch 45/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2517 - mean_squared_error: 0.2442 - val_loss: 1.0249 - val_mean_squared_error: 1.0175\n",
      "Epoch 46/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2463 - mean_squared_error: 0.2389 - val_loss: 0.9635 - val_mean_squared_error: 0.9561\n",
      "Epoch 47/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2414 - mean_squared_error: 0.2340 - val_loss: 0.9913 - val_mean_squared_error: 0.9839\n",
      "Epoch 48/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2367 - mean_squared_error: 0.2292 - val_loss: 0.9711 - val_mean_squared_error: 0.9637\n",
      "Epoch 49/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2361 - mean_squared_error: 0.2287 - val_loss: 0.9861 - val_mean_squared_error: 0.9787\n",
      "Epoch 50/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2330 - mean_squared_error: 0.2256 - val_loss: 1.0385 - val_mean_squared_error: 1.0311\n",
      "Epoch 51/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2280 - mean_squared_error: 0.2206 - val_loss: 0.9773 - val_mean_squared_error: 0.9699\n",
      "Epoch 52/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2300 - mean_squared_error: 0.2227 - val_loss: 0.9839 - val_mean_squared_error: 0.9765\n",
      "Epoch 53/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2260 - mean_squared_error: 0.2187 - val_loss: 1.0158 - val_mean_squared_error: 1.0085\n",
      "Epoch 54/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2233 - mean_squared_error: 0.2161 - val_loss: 1.0012 - val_mean_squared_error: 0.9939\n",
      "Epoch 55/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2238 - mean_squared_error: 0.2165 - val_loss: 1.0328 - val_mean_squared_error: 1.0255\n",
      "Epoch 56/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2276 - mean_squared_error: 0.2204 - val_loss: 0.9528 - val_mean_squared_error: 0.9456\n",
      "Epoch 57/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2202 - mean_squared_error: 0.2130 - val_loss: 1.0023 - val_mean_squared_error: 0.9951\n",
      "Epoch 58/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2193 - mean_squared_error: 0.2121 - val_loss: 0.9976 - val_mean_squared_error: 0.9904\n",
      "Epoch 59/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2151 - mean_squared_error: 0.2079 - val_loss: 1.0530 - val_mean_squared_error: 1.0458\n",
      "Epoch 60/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2147 - mean_squared_error: 0.2075 - val_loss: 1.0406 - val_mean_squared_error: 1.0335\n",
      "Epoch 61/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2111 - mean_squared_error: 0.2039 - val_loss: 1.0022 - val_mean_squared_error: 0.9951\n",
      "Epoch 62/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2086 - mean_squared_error: 0.2014 - val_loss: 1.0375 - val_mean_squared_error: 1.0303\n",
      "Epoch 63/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2084 - mean_squared_error: 0.2013 - val_loss: 1.0646 - val_mean_squared_error: 1.0574\n",
      "Epoch 64/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2063 - mean_squared_error: 0.1992 - val_loss: 1.0848 - val_mean_squared_error: 1.0777\n",
      "Epoch 65/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2040 - mean_squared_error: 0.1969 - val_loss: 1.0336 - val_mean_squared_error: 1.0265\n",
      "Epoch 66/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2044 - mean_squared_error: 0.1973 - val_loss: 1.0161 - val_mean_squared_error: 1.0090\n",
      "Epoch 67/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2058 - mean_squared_error: 0.1988 - val_loss: 1.0280 - val_mean_squared_error: 1.0209\n",
      "Epoch 68/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2149 - mean_squared_error: 0.2079 - val_loss: 1.0501 - val_mean_squared_error: 1.0432\n",
      "Epoch 69/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2108 - mean_squared_error: 0.2038 - val_loss: 0.9969 - val_mean_squared_error: 0.9899\n",
      "Epoch 70/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2011 - mean_squared_error: 0.1941 - val_loss: 0.9631 - val_mean_squared_error: 0.9561\n",
      "Epoch 71/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1976 - mean_squared_error: 0.1906 - val_loss: 0.9613 - val_mean_squared_error: 0.9544\n",
      "Epoch 72/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1958 - mean_squared_error: 0.1889 - val_loss: 0.9921 - val_mean_squared_error: 0.9852\n",
      "Epoch 73/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1941 - mean_squared_error: 0.1872 - val_loss: 1.0023 - val_mean_squared_error: 0.9954\n",
      "Epoch 74/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1920 - mean_squared_error: 0.1851 - val_loss: 0.9872 - val_mean_squared_error: 0.9804\n",
      "Epoch 75/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1906 - mean_squared_error: 0.1838 - val_loss: 0.9942 - val_mean_squared_error: 0.9874\n",
      "Epoch 76/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1901 - mean_squared_error: 0.1833 - val_loss: 1.0212 - val_mean_squared_error: 1.0144\n",
      "Epoch 77/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1885 - mean_squared_error: 0.1817 - val_loss: 0.9960 - val_mean_squared_error: 0.9892\n",
      "Epoch 78/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1879 - mean_squared_error: 0.1812 - val_loss: 1.0528 - val_mean_squared_error: 1.0461\n",
      "Epoch 79/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1864 - mean_squared_error: 0.1797 - val_loss: 1.0337 - val_mean_squared_error: 1.0270\n",
      "Epoch 80/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1867 - mean_squared_error: 0.1801 - val_loss: 1.0384 - val_mean_squared_error: 1.0318\n",
      "Epoch 81/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1845 - mean_squared_error: 0.1779 - val_loss: 1.0105 - val_mean_squared_error: 1.0040\n",
      "Epoch 82/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1857 - mean_squared_error: 0.1791 - val_loss: 1.0358 - val_mean_squared_error: 1.0293\n",
      "Epoch 83/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1848 - mean_squared_error: 0.1783 - val_loss: 1.0243 - val_mean_squared_error: 1.0179\n",
      "Epoch 84/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1841 - mean_squared_error: 0.1777 - val_loss: 1.0312 - val_mean_squared_error: 1.0248\n",
      "Epoch 85/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1833 - mean_squared_error: 0.1769 - val_loss: 1.0477 - val_mean_squared_error: 1.0414\n",
      "Epoch 86/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1835 - mean_squared_error: 0.1772 - val_loss: 1.0319 - val_mean_squared_error: 1.0256\n",
      "Epoch 87/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1823 - mean_squared_error: 0.1761 - val_loss: 1.0086 - val_mean_squared_error: 1.0024\n",
      "Epoch 88/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1812 - mean_squared_error: 0.1750 - val_loss: 0.9979 - val_mean_squared_error: 0.9918\n",
      "Epoch 89/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1817 - mean_squared_error: 0.1756 - val_loss: 0.9778 - val_mean_squared_error: 0.9718\n",
      "Epoch 90/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1825 - mean_squared_error: 0.1764 - val_loss: 0.9979 - val_mean_squared_error: 0.9918\n",
      "Epoch 91/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1805 - mean_squared_error: 0.1746 - val_loss: 1.0106 - val_mean_squared_error: 1.0046\n",
      "Epoch 92/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1807 - mean_squared_error: 0.1747 - val_loss: 1.0096 - val_mean_squared_error: 1.0037\n",
      "Epoch 93/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1836 - mean_squared_error: 0.1777 - val_loss: 0.9750 - val_mean_squared_error: 0.9691\n",
      "Epoch 94/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2166 - mean_squared_error: 0.2108 - val_loss: 1.0642 - val_mean_squared_error: 1.0585\n",
      "Epoch 95/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2512 - mean_squared_error: 0.2455 - val_loss: 0.8845 - val_mean_squared_error: 0.8789\n",
      "Epoch 96/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.2073 - mean_squared_error: 0.2016 - val_loss: 1.0066 - val_mean_squared_error: 1.0009\n",
      "Epoch 97/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1862 - mean_squared_error: 0.1805 - val_loss: 0.9946 - val_mean_squared_error: 0.9889\n",
      "Epoch 98/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1807 - mean_squared_error: 0.1751 - val_loss: 1.0415 - val_mean_squared_error: 1.0358\n",
      "Epoch 99/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1789 - mean_squared_error: 0.1733 - val_loss: 1.0054 - val_mean_squared_error: 0.9998\n",
      "Epoch 100/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1780 - mean_squared_error: 0.1724 - val_loss: 1.0227 - val_mean_squared_error: 1.0171\n",
      "Epoch 101/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1779 - mean_squared_error: 0.1723 - val_loss: 1.0153 - val_mean_squared_error: 1.0098\n",
      "Epoch 102/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1775 - mean_squared_error: 0.1720 - val_loss: 1.0063 - val_mean_squared_error: 1.0008\n",
      "Epoch 103/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1770 - mean_squared_error: 0.1715 - val_loss: 1.0089 - val_mean_squared_error: 1.0034\n",
      "Epoch 104/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1768 - mean_squared_error: 0.1714 - val_loss: 0.9869 - val_mean_squared_error: 0.9814\n",
      "Epoch 105/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1761 - mean_squared_error: 0.1707 - val_loss: 1.0010 - val_mean_squared_error: 0.9956\n",
      "Epoch 106/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1754 - mean_squared_error: 0.1700 - val_loss: 0.9876 - val_mean_squared_error: 0.9822\n",
      "Epoch 107/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1756 - mean_squared_error: 0.1702 - val_loss: 1.0402 - val_mean_squared_error: 1.0349\n",
      "Epoch 108/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1746 - mean_squared_error: 0.1693 - val_loss: 1.0065 - val_mean_squared_error: 1.0012\n",
      "Epoch 109/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1733 - mean_squared_error: 0.1681 - val_loss: 1.0138 - val_mean_squared_error: 1.0085\n",
      "Epoch 110/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1715 - mean_squared_error: 0.1662 - val_loss: 1.0164 - val_mean_squared_error: 1.0111\n",
      "Epoch 111/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1702 - mean_squared_error: 0.1650 - val_loss: 1.0427 - val_mean_squared_error: 1.0375\n",
      "Epoch 112/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1689 - mean_squared_error: 0.1637 - val_loss: 1.0561 - val_mean_squared_error: 1.0510\n",
      "Epoch 113/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1677 - mean_squared_error: 0.1625 - val_loss: 1.0450 - val_mean_squared_error: 1.0399\n",
      "Epoch 114/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1669 - mean_squared_error: 0.1617 - val_loss: 1.0371 - val_mean_squared_error: 1.0320\n",
      "Epoch 115/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1660 - mean_squared_error: 0.1609 - val_loss: 1.0239 - val_mean_squared_error: 1.0188\n",
      "Epoch 116/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1655 - mean_squared_error: 0.1605 - val_loss: 1.0373 - val_mean_squared_error: 1.0323\n",
      "Epoch 117/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1648 - mean_squared_error: 0.1598 - val_loss: 1.0746 - val_mean_squared_error: 1.0696\n",
      "Epoch 118/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1645 - mean_squared_error: 0.1595 - val_loss: 1.0281 - val_mean_squared_error: 1.0232\n",
      "Epoch 119/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1644 - mean_squared_error: 0.1595 - val_loss: 1.0499 - val_mean_squared_error: 1.0450\n",
      "Epoch 120/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1639 - mean_squared_error: 0.1590 - val_loss: 1.0307 - val_mean_squared_error: 1.0259\n",
      "Epoch 121/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1627 - mean_squared_error: 0.1578 - val_loss: 1.0270 - val_mean_squared_error: 1.0222\n",
      "Epoch 122/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1627 - mean_squared_error: 0.1579 - val_loss: 1.0571 - val_mean_squared_error: 1.0523\n",
      "Epoch 123/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1625 - mean_squared_error: 0.1577 - val_loss: 1.0309 - val_mean_squared_error: 1.0262\n",
      "Epoch 124/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1625 - mean_squared_error: 0.1578 - val_loss: 1.0225 - val_mean_squared_error: 1.0178\n",
      "Epoch 125/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1630 - mean_squared_error: 0.1583 - val_loss: 0.9971 - val_mean_squared_error: 0.9925\n",
      "Epoch 126/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1623 - mean_squared_error: 0.1577 - val_loss: 1.0196 - val_mean_squared_error: 1.0151\n",
      "Epoch 127/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1619 - mean_squared_error: 0.1574 - val_loss: 1.0337 - val_mean_squared_error: 1.0292\n",
      "Epoch 128/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1614 - mean_squared_error: 0.1569 - val_loss: 1.0407 - val_mean_squared_error: 1.0362\n",
      "Epoch 129/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1608 - mean_squared_error: 0.1563 - val_loss: 1.0582 - val_mean_squared_error: 1.0537\n",
      "Epoch 130/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1604 - mean_squared_error: 0.1559 - val_loss: 1.0578 - val_mean_squared_error: 1.0534\n",
      "Epoch 131/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1610 - mean_squared_error: 0.1566 - val_loss: 1.0236 - val_mean_squared_error: 1.0192\n",
      "Epoch 132/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1614 - mean_squared_error: 0.1570 - val_loss: 1.0570 - val_mean_squared_error: 1.0526\n",
      "Epoch 133/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1618 - mean_squared_error: 0.1575 - val_loss: 0.9533 - val_mean_squared_error: 0.9490\n",
      "Epoch 134/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1648 - mean_squared_error: 0.1605 - val_loss: 0.9906 - val_mean_squared_error: 0.9864\n",
      "Epoch 135/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1676 - mean_squared_error: 0.1633 - val_loss: 0.9573 - val_mean_squared_error: 0.9531\n",
      "Epoch 136/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1639 - mean_squared_error: 0.1597 - val_loss: 1.0164 - val_mean_squared_error: 1.0123\n",
      "Epoch 137/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1618 - mean_squared_error: 0.1576 - val_loss: 1.0694 - val_mean_squared_error: 1.0652\n",
      "Epoch 138/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1602 - mean_squared_error: 0.1561 - val_loss: 1.0234 - val_mean_squared_error: 1.0193\n",
      "Epoch 139/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1597 - mean_squared_error: 0.1556 - val_loss: 1.0321 - val_mean_squared_error: 1.0280\n",
      "Epoch 140/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1590 - mean_squared_error: 0.1550 - val_loss: 1.0296 - val_mean_squared_error: 1.0256\n",
      "Epoch 141/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1590 - mean_squared_error: 0.1550 - val_loss: 1.0255 - val_mean_squared_error: 1.0215\n",
      "Epoch 142/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1588 - mean_squared_error: 0.1548 - val_loss: 1.0587 - val_mean_squared_error: 1.0547\n",
      "Epoch 143/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1585 - mean_squared_error: 0.1546 - val_loss: 1.0521 - val_mean_squared_error: 1.0482\n",
      "Epoch 144/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1588 - mean_squared_error: 0.1548 - val_loss: 1.0319 - val_mean_squared_error: 1.0280\n",
      "Epoch 145/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1588 - mean_squared_error: 0.1549 - val_loss: 1.0377 - val_mean_squared_error: 1.0339\n",
      "Epoch 146/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1589 - mean_squared_error: 0.1551 - val_loss: 1.0524 - val_mean_squared_error: 1.0486\n",
      "Epoch 147/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1592 - mean_squared_error: 0.1554 - val_loss: 1.0391 - val_mean_squared_error: 1.0353\n",
      "Epoch 148/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1603 - mean_squared_error: 0.1565 - val_loss: 1.0245 - val_mean_squared_error: 1.0207\n",
      "Epoch 149/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1596 - mean_squared_error: 0.1559 - val_loss: 1.0112 - val_mean_squared_error: 1.0075\n",
      "Epoch 150/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1591 - mean_squared_error: 0.1554 - val_loss: 1.0893 - val_mean_squared_error: 1.0856\n",
      "Epoch 151/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1592 - mean_squared_error: 0.1556 - val_loss: 1.0513 - val_mean_squared_error: 1.0477\n",
      "Epoch 152/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1589 - mean_squared_error: 0.1553 - val_loss: 1.0145 - val_mean_squared_error: 1.0109\n",
      "Epoch 153/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1592 - mean_squared_error: 0.1556 - val_loss: 1.0554 - val_mean_squared_error: 1.0518\n",
      "Epoch 154/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1587 - mean_squared_error: 0.1552 - val_loss: 1.0548 - val_mean_squared_error: 1.0513\n",
      "Epoch 155/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1583 - mean_squared_error: 0.1548 - val_loss: 1.0568 - val_mean_squared_error: 1.0533\n",
      "Epoch 156/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1588 - mean_squared_error: 0.1553 - val_loss: 1.0076 - val_mean_squared_error: 1.0041\n",
      "Epoch 157/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1594 - mean_squared_error: 0.1559 - val_loss: 1.0410 - val_mean_squared_error: 1.0376\n",
      "Epoch 158/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1585 - mean_squared_error: 0.1550 - val_loss: 1.0660 - val_mean_squared_error: 1.0626\n",
      "Epoch 159/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1587 - mean_squared_error: 0.1553 - val_loss: 1.0847 - val_mean_squared_error: 1.0813\n",
      "Epoch 160/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1593 - mean_squared_error: 0.1559 - val_loss: 1.0030 - val_mean_squared_error: 0.9996\n",
      "Epoch 161/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1597 - mean_squared_error: 0.1564 - val_loss: 1.0358 - val_mean_squared_error: 1.0324\n",
      "Epoch 162/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1588 - mean_squared_error: 0.1555 - val_loss: 1.0298 - val_mean_squared_error: 1.0265\n",
      "Epoch 163/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1578 - mean_squared_error: 0.1545 - val_loss: 1.0326 - val_mean_squared_error: 1.0293\n",
      "Epoch 164/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1574 - mean_squared_error: 0.1541 - val_loss: 1.0470 - val_mean_squared_error: 1.0437\n",
      "Epoch 165/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1578 - mean_squared_error: 0.1545 - val_loss: 1.0265 - val_mean_squared_error: 1.0233\n",
      "Epoch 166/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1574 - mean_squared_error: 0.1542 - val_loss: 1.0595 - val_mean_squared_error: 1.0563\n",
      "Epoch 167/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1575 - mean_squared_error: 0.1543 - val_loss: 1.0628 - val_mean_squared_error: 1.0597\n",
      "Epoch 168/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1576 - mean_squared_error: 0.1544 - val_loss: 1.0580 - val_mean_squared_error: 1.0548\n",
      "Epoch 169/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1604 - mean_squared_error: 0.1572 - val_loss: 0.9948 - val_mean_squared_error: 0.9917\n",
      "Epoch 170/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1604 - mean_squared_error: 0.1573 - val_loss: 1.0328 - val_mean_squared_error: 1.0297\n",
      "Epoch 171/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1590 - mean_squared_error: 0.1559 - val_loss: 1.0427 - val_mean_squared_error: 1.0396\n",
      "Epoch 172/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1814 - mean_squared_error: 0.1784 - val_loss: 0.9702 - val_mean_squared_error: 0.9672\n",
      "Epoch 173/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1917 - mean_squared_error: 0.1886 - val_loss: 1.0028 - val_mean_squared_error: 0.9998\n",
      "Epoch 174/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1622 - mean_squared_error: 0.1592 - val_loss: 0.9991 - val_mean_squared_error: 0.9960\n",
      "Epoch 175/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1590 - mean_squared_error: 0.1559 - val_loss: 1.0041 - val_mean_squared_error: 1.0011\n",
      "Epoch 176/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1581 - mean_squared_error: 0.1550 - val_loss: 1.0179 - val_mean_squared_error: 1.0148\n",
      "Epoch 177/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1575 - mean_squared_error: 0.1545 - val_loss: 1.0176 - val_mean_squared_error: 1.0146\n",
      "Epoch 178/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1573 - mean_squared_error: 0.1543 - val_loss: 1.0149 - val_mean_squared_error: 1.0119\n",
      "Epoch 179/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1567 - mean_squared_error: 0.1537 - val_loss: 1.0321 - val_mean_squared_error: 1.0291\n",
      "Epoch 180/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1568 - mean_squared_error: 0.1539 - val_loss: 1.0227 - val_mean_squared_error: 1.0198\n",
      "Epoch 181/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1566 - mean_squared_error: 0.1537 - val_loss: 1.0358 - val_mean_squared_error: 1.0329\n",
      "Epoch 182/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1532 - val_loss: 1.0340 - val_mean_squared_error: 1.0311\n",
      "Epoch 183/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1563 - mean_squared_error: 0.1534 - val_loss: 1.0266 - val_mean_squared_error: 1.0237\n",
      "Epoch 184/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1532 - val_loss: 1.0342 - val_mean_squared_error: 1.0314\n",
      "Epoch 185/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1560 - mean_squared_error: 0.1532 - val_loss: 1.0310 - val_mean_squared_error: 1.0282\n",
      "Epoch 186/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1562 - mean_squared_error: 0.1534 - val_loss: 1.0443 - val_mean_squared_error: 1.0415\n",
      "Epoch 187/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1531 - val_loss: 1.0455 - val_mean_squared_error: 1.0427\n",
      "Epoch 188/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1532 - val_loss: 1.0505 - val_mean_squared_error: 1.0477\n",
      "Epoch 189/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1532 - val_loss: 1.0413 - val_mean_squared_error: 1.0385\n",
      "Epoch 190/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1558 - mean_squared_error: 0.1531 - val_loss: 1.0412 - val_mean_squared_error: 1.0385\n",
      "Epoch 191/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1558 - mean_squared_error: 0.1531 - val_loss: 1.0422 - val_mean_squared_error: 1.0395\n",
      "Epoch 192/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1534 - val_loss: 1.0302 - val_mean_squared_error: 1.0275\n",
      "Epoch 193/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1557 - mean_squared_error: 0.1530 - val_loss: 1.0345 - val_mean_squared_error: 1.0319\n",
      "Epoch 194/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1560 - mean_squared_error: 0.1533 - val_loss: 1.0415 - val_mean_squared_error: 1.0388\n",
      "Epoch 195/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1533 - val_loss: 1.0332 - val_mean_squared_error: 1.0306\n",
      "Epoch 196/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1558 - mean_squared_error: 0.1532 - val_loss: 1.0308 - val_mean_squared_error: 1.0282\n",
      "Epoch 197/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1535 - val_loss: 1.0424 - val_mean_squared_error: 1.0399\n",
      "Epoch 198/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1565 - mean_squared_error: 0.1539 - val_loss: 1.0434 - val_mean_squared_error: 1.0409\n",
      "Epoch 199/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1564 - mean_squared_error: 0.1539 - val_loss: 1.0306 - val_mean_squared_error: 1.0281\n",
      "Epoch 200/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1563 - mean_squared_error: 0.1537 - val_loss: 1.0327 - val_mean_squared_error: 1.0302\n",
      "Epoch 201/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1567 - mean_squared_error: 0.1542 - val_loss: 1.0546 - val_mean_squared_error: 1.0521\n",
      "Epoch 202/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1566 - mean_squared_error: 0.1541 - val_loss: 1.0305 - val_mean_squared_error: 1.0281\n",
      "Epoch 203/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1564 - mean_squared_error: 0.1540 - val_loss: 1.0458 - val_mean_squared_error: 1.0434\n",
      "Epoch 204/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1564 - mean_squared_error: 0.1540 - val_loss: 0.9967 - val_mean_squared_error: 0.9942\n",
      "Epoch 205/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1566 - mean_squared_error: 0.1542 - val_loss: 1.0410 - val_mean_squared_error: 1.0385\n",
      "Epoch 206/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1537 - val_loss: 1.0212 - val_mean_squared_error: 1.0188\n",
      "Epoch 207/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1558 - mean_squared_error: 0.1535 - val_loss: 1.0327 - val_mean_squared_error: 1.0304\n",
      "Epoch 208/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1535 - val_loss: 1.0470 - val_mean_squared_error: 1.0447\n",
      "Epoch 209/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1559 - mean_squared_error: 0.1536 - val_loss: 1.0386 - val_mean_squared_error: 1.0363\n",
      "Epoch 210/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1564 - mean_squared_error: 0.1540 - val_loss: 1.0223 - val_mean_squared_error: 1.0200\n",
      "Epoch 211/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1562 - mean_squared_error: 0.1539 - val_loss: 1.0334 - val_mean_squared_error: 1.0311\n",
      "Epoch 212/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1562 - mean_squared_error: 0.1539 - val_loss: 1.0426 - val_mean_squared_error: 1.0403\n",
      "Epoch 213/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1558 - mean_squared_error: 0.1535 - val_loss: 1.0120 - val_mean_squared_error: 1.0098\n",
      "Epoch 214/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1538 - val_loss: 1.0248 - val_mean_squared_error: 1.0225\n",
      "Epoch 215/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1538 - val_loss: 1.0434 - val_mean_squared_error: 1.0411\n",
      "Epoch 216/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1562 - mean_squared_error: 0.1539 - val_loss: 0.9949 - val_mean_squared_error: 0.9926\n",
      "Epoch 217/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1557 - mean_squared_error: 0.1534 - val_loss: 1.0224 - val_mean_squared_error: 1.0201\n",
      "Epoch 218/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1539 - val_loss: 1.0065 - val_mean_squared_error: 1.0043\n",
      "Epoch 219/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1563 - mean_squared_error: 0.1541 - val_loss: 1.0418 - val_mean_squared_error: 1.0396\n",
      "Epoch 220/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1563 - mean_squared_error: 0.1542 - val_loss: 1.0413 - val_mean_squared_error: 1.0391\n",
      "Epoch 221/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1562 - mean_squared_error: 0.1541 - val_loss: 1.0445 - val_mean_squared_error: 1.0423\n",
      "Epoch 222/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1557 - mean_squared_error: 0.1535 - val_loss: 1.0722 - val_mean_squared_error: 1.0701\n",
      "Epoch 223/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1540 - val_loss: 1.0438 - val_mean_squared_error: 1.0417\n",
      "Epoch 224/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1561 - mean_squared_error: 0.1540 - val_loss: 1.0230 - val_mean_squared_error: 1.0209\n",
      "Epoch 225/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1578 - mean_squared_error: 0.1557 - val_loss: 1.0203 - val_mean_squared_error: 1.0182\n",
      "Epoch 226/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1575 - mean_squared_error: 0.1554 - val_loss: 1.0565 - val_mean_squared_error: 1.0544\n",
      "Epoch 227/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1573 - mean_squared_error: 0.1552 - val_loss: 1.0529 - val_mean_squared_error: 1.0508\n",
      "Epoch 228/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1570 - mean_squared_error: 0.1549 - val_loss: 1.0232 - val_mean_squared_error: 1.0212\n",
      "Epoch 229/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1557 - mean_squared_error: 0.1537 - val_loss: 0.9961 - val_mean_squared_error: 0.9940\n",
      "Epoch 230/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1555 - mean_squared_error: 0.1534 - val_loss: 1.0360 - val_mean_squared_error: 1.0339\n",
      "Epoch 231/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1552 - mean_squared_error: 0.1532 - val_loss: 1.0271 - val_mean_squared_error: 1.0251\n",
      "Epoch 232/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1548 - mean_squared_error: 0.1528 - val_loss: 1.0383 - val_mean_squared_error: 1.0362\n",
      "Epoch 233/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1548 - mean_squared_error: 0.1528 - val_loss: 1.0518 - val_mean_squared_error: 1.0498\n",
      "Epoch 234/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1549 - mean_squared_error: 0.1529 - val_loss: 1.0084 - val_mean_squared_error: 1.0064\n",
      "Epoch 235/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1546 - mean_squared_error: 0.1526 - val_loss: 1.0515 - val_mean_squared_error: 1.0495\n",
      "Epoch 236/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1547 - mean_squared_error: 0.1528 - val_loss: 1.0265 - val_mean_squared_error: 1.0245\n",
      "Epoch 237/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1551 - mean_squared_error: 0.1531 - val_loss: 1.0481 - val_mean_squared_error: 1.0461\n",
      "Epoch 238/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1549 - mean_squared_error: 0.1530 - val_loss: 1.0211 - val_mean_squared_error: 1.0192\n",
      "Epoch 239/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1552 - mean_squared_error: 0.1532 - val_loss: 1.0223 - val_mean_squared_error: 1.0204\n",
      "Epoch 240/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1560 - mean_squared_error: 0.1540 - val_loss: 1.0114 - val_mean_squared_error: 1.0095\n",
      "Epoch 241/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1555 - mean_squared_error: 0.1536 - val_loss: 1.0227 - val_mean_squared_error: 1.0208\n",
      "Epoch 242/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1557 - mean_squared_error: 0.1538 - val_loss: 1.0888 - val_mean_squared_error: 1.0869\n",
      "Epoch 243/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1556 - mean_squared_error: 0.1537 - val_loss: 1.0571 - val_mean_squared_error: 1.0552\n",
      "Epoch 244/5000\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 0.1556 - mean_squared_error: 0.1537 - val_loss: 1.0445 - val_mean_squared_error: 1.0426\n",
      "Epoch 245/5000\n",
      "36/94 [==========>...................] - ETA: 0s - loss: 0.1507 - mean_squared_error: 0.1488"
     ]
    }
   ],
   "source": [
    "model_history_LSTM_model_1L, score_LSTM_model_factors_1L= compile_and_fit_simple('factors',checkpoints_LSTM_model_1L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000,'fit', class_weights_dict)\n",
    "model_history_LSTM_model_2L, score_LSTM_model_factors_2L = compile_and_fit_simple('factors',checkpoints_LSTM_model_2L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_3L, score_LSTM_model_factors_3L = compile_and_fit_simple('factors',checkpoints_LSTM_model_3L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)\n",
    "model_history_LSTM_model_4L, score_LSTM_model_factors_4L = compile_and_fit_simple('factors',checkpoints_LSTM_model_4L_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    5000,'fit',class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b50b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_1L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_1L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_2L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_2L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_3L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_3L_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_4L_alternative_final.h5', 'checkpoints_alternative/checkpoints_LSTM_model_4L_alternative_final.h5')\n",
    "\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_1L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_1L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_2L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_2L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_3L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_3L_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_LSTM_model_4L_alternative.h5', 'checkpoints_alternative/checkpoints_LSTM_model_4L_alternative.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23f379",
   "metadata": {},
   "source": [
    "### Combined FFN & LSTM NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c17b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_history_FFN_model_L1, score_FFN1_model = compile_and_fit_simple('combined',checkpoints_FFN_model_L1_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000,'fit', class_weights_dict)\n",
    "model_history_FFN_model_L2, score_FFN2_model = compile_and_fit_simple('combined',checkpoints_FFN_model_L2_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000,'fit', class_weights_dict)\n",
    "model_history_FFN_model_L3, score_FFN3_model = compile_and_fit_simple('combined',checkpoints_FFN_model_L3_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train, patience_value, batch_size, \n",
    "                    10000,'fit', class_weights_dict)\n",
    "model_history_FFN_model_L4, score_FFN4_model = compile_and_fit_simple('combined',checkpoints_FFN_model_L4_alternative,config,final_stock_data,x_train_factors,x_train_macro,x_train_merged,y_train,  patience_value, batch_size, \n",
    "                    10000,'fit', class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L1_alternative_final.h5', 'checkpoints_alternative/checkpoints_FFN_model_L1_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L2_alternative_final.h5', 'checkpoints_alternative/checkpoints_FFN_model_L2_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L3_alternative_final.h5', 'checkpoints_alternative/checkpoints_FFN_model_L3_alternative_final.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L4_alternative_final.h5', 'checkpoints_alternative/checkpoints_FFN_model_L4_alternative_final.h5')\n",
    "\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L1_alternative.h5', 'checkpoints_alternative/checkpoints_FFN_model_L1_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L2_alternative.h5', 'checkpoints_alternative/checkpoints_FFN_model_L2_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L3_alternative.h5', 'checkpoints_alternative/checkpoints_FFN_model_L3_alternative.h5')\n",
    "file_saver('checkpoints_alternative/checkpoints_FFN_model_L4_alternative.h5', 'checkpoints_alternative/checkpoints_FFN_model_L4_alternative.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "243c982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model scores \n"
     ]
    }
   ],
   "source": [
    "print('save model scores ')\n",
    "np.savez_compressed('NN_model_scores.npz', \n",
    "                    score_model_factor_1L=score_model_factor_1L,\n",
    "                    score_model_factor_2L=score_model_factor_2L,\n",
    "                   score_model_factor_3L=score_model_factor_3L,\n",
    "                    score_model_factor_4L=score_model_factor_4L, \n",
    "                    score_LSTM_model_merged_1L = score_LSTM_model_merged_1L, \n",
    "                    score_LSTM_model_merged_2L = score_LSTM_model_merged_2L, \n",
    "                    score_LSTM_model_merged_3L = score_LSTM_model_merged_3L, \n",
    "                    score_LSTM_model_merged_4L = score_LSTM_model_merged_4L, \n",
    "                    score_LSTM_model_factors_1L = score_LSTM_model_factors_1L, \n",
    "                    score_LSTM_model_factors_2L = score_LSTM_model_factors_2L, \n",
    "                    score_LSTM_model_factors_3L = score_LSTM_model_factors_3L, \n",
    "                    score_LSTM_model_factors_4L = score_LSTM_model_factors_4L, \n",
    "                    score_FFN1_model = score_FFN1_model, score_FFN2_model = score_FFN2_model, \n",
    "                    score_FFN3_model = score_FFN3_model, score_FFN4_model = score_FFN4_model)\n",
    "                    \n",
    "file_saver('NN_model_scores.npz','NN_model_scores.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f888ce1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12404274940490723, 0.12269492447376251]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_FFN1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "bba79c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19377441704273224, 0.1921270191669464]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_FFN4_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2e0507d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28071215748786926, 0.27915823459625244]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_FFN3_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "94fee80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12673334777355194, 0.12492261081933975]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_FFN2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c32733f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del monthly_factors_adj\n",
    "# import gc \n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11259fd3",
   "metadata": {},
   "source": [
    "# Create Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "efe63a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1977-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>520 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "1977-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-10      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "1977-11      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-07      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-08      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-09      3.0      4.0      4.0      4.0      3.0      4.0      4.0   \n",
       "2020-10      3.0      4.0      4.0      4.0      3.0      4.0      3.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "1977-07      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-08      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-09      4.0      4.0      3.0  ...      3.0      4.0      3.0      3.0   \n",
       "1977-10      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "1977-11      4.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-07      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-08      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-09      3.0      4.0      4.0  ...      3.0      4.0      3.0      2.0   \n",
       "2020-10      3.0      4.0      3.0  ...      3.0      4.0      3.0      2.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "1977-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-10      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "1977-11      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-07      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-08      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-09      4.0      4.0      4.0      4.0      4.0      4.0  \n",
       "2020-10      4.0      4.0      3.0      4.0      4.0      4.0  \n",
       "\n",
       "[520 rows x 501 columns]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_stock_quintiles = pd.read_csv(path_bucket+'complete_stock_quintiles_NN_pred.csv')\n",
    "complete_stock_quintiles = complete_stock_quintiles.set_index('date')\n",
    "complete_stock_quintiles = complete_stock_quintiles.astype(float)\n",
    "complete_stock_quintiles.columns.name = 'permno'\n",
    "complete_stock_quintiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "49715e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date      2012-07\n",
      "permno    10104.0\n",
      "value         1.0\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>permno</th>\n",
       "      <th>10104.0</th>\n",
       "      <th>10107.0</th>\n",
       "      <th>10138.0</th>\n",
       "      <th>10145.0</th>\n",
       "      <th>10516.0</th>\n",
       "      <th>10696.0</th>\n",
       "      <th>10909.0</th>\n",
       "      <th>11308.0</th>\n",
       "      <th>11403.0</th>\n",
       "      <th>11404.0</th>\n",
       "      <th>...</th>\n",
       "      <th>92121.0</th>\n",
       "      <th>92157.0</th>\n",
       "      <th>92239.0</th>\n",
       "      <th>92293.0</th>\n",
       "      <th>92322.0</th>\n",
       "      <th>92402.0</th>\n",
       "      <th>92602.0</th>\n",
       "      <th>92611.0</th>\n",
       "      <th>92614.0</th>\n",
       "      <th>92655.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-07</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-08</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-09</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-11</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "permno   10104.0  10107.0  10138.0  10145.0  10516.0  10696.0  10909.0  \\\n",
       "date                                                                     \n",
       "2012-07      4.0      4.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2012-08      4.0      4.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2012-09      4.0      4.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2012-10      4.0      4.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2012-11      4.0      4.0      5.0      5.0      4.0      5.0      5.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "2020-06      4.0      5.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2020-07      4.0      5.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2020-08      4.0      5.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2020-09      4.0      5.0      5.0      5.0      4.0      5.0      5.0   \n",
       "2020-10      4.0      5.0      5.0      5.0      4.0      5.0      4.0   \n",
       "\n",
       "permno   11308.0  11403.0  11404.0  ...  92121.0  92157.0  92239.0  92293.0  \\\n",
       "date                                ...                                       \n",
       "2012-07      4.0      2.0      5.0  ...      4.0      4.0      5.0      5.0   \n",
       "2012-08      4.0      2.0      5.0  ...      4.0      4.0      5.0      5.0   \n",
       "2012-09      4.0      2.0      5.0  ...      4.0      4.0      5.0      5.0   \n",
       "2012-10      4.0      2.0      5.0  ...      4.0      4.0      5.0      5.0   \n",
       "2012-11      4.0      2.0      5.0  ...      4.0      4.0      5.0      5.0   \n",
       "...          ...      ...      ...  ...      ...      ...      ...      ...   \n",
       "2020-06      4.0      5.0      5.0  ...      4.0      5.0      4.0      3.0   \n",
       "2020-07      4.0      5.0      5.0  ...      4.0      5.0      4.0      3.0   \n",
       "2020-08      4.0      5.0      5.0  ...      4.0      5.0      4.0      3.0   \n",
       "2020-09      4.0      5.0      5.0  ...      4.0      5.0      4.0      3.0   \n",
       "2020-10      4.0      5.0      4.0  ...      4.0      5.0      4.0      3.0   \n",
       "\n",
       "permno   92322.0  92402.0  92602.0  92611.0  92614.0  92655.0  \n",
       "date                                                           \n",
       "2012-07      5.0      4.0      5.0      5.0      4.0      5.0  \n",
       "2012-08      5.0      4.0      5.0      5.0      4.0      5.0  \n",
       "2012-09      5.0      4.0      5.0      5.0      4.0      5.0  \n",
       "2012-10      5.0      4.0      5.0      5.0      4.0      5.0  \n",
       "2012-11      5.0      4.0      5.0      5.0      4.0      5.0  \n",
       "...          ...      ...      ...      ...      ...      ...  \n",
       "2020-06      5.0      5.0      5.0      5.0      5.0      5.0  \n",
       "2020-07      5.0      5.0      5.0      5.0      5.0      5.0  \n",
       "2020-08      5.0      5.0      5.0      5.0      5.0      5.0  \n",
       "2020-09      5.0      5.0      5.0      5.0      5.0      5.0  \n",
       "2020-10      5.0      5.0      4.0      5.0      5.0      5.0  \n",
       "\n",
       "[100 rows x 501 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data, test_data_df = get_test_data_stocks(complete_stock_quintiles, batch_size, 0.8)\n",
    "print(test_data_df.min())\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ad5affc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.load('test_arrays_multi_available.npz',allow_pickle=True)\n",
    "x_test_merged = test_array['x_test_merged']\n",
    "x_test_factors =test_array['x_test_factors']\n",
    "x_test_macro = test_array['x_test_macro']\n",
    "y_test = test_array['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cf7ccae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4, 169)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "b1f870b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nn_predict_simple(model_type,test_data_stocks, x_test_factors,x_test_macro,x_test_merged, model_path, batch_size, label_names):'"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"nn_predict_simple(model_type,test_data_stocks, x_test_factors,x_test_macro,x_test_merged, model_path, batch_size, label_names):\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "794daaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4, 33)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_factors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4173eeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "97682115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 988us/step\n",
      "25/25 [==============================] - 0s 937us/step\n",
      "25/25 [==============================] - 0s 892us/step\n",
      "25/25 [==============================] - 0s 895us/step\n",
      "25/25 [==============================] - 0s 778us/step\n",
      "25/25 [==============================] - 0s 811us/step\n",
      "25/25 [==============================] - 0s 835us/step\n",
      "25/25 [==============================] - 0s 837us/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 1s 2ms/step\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 2s 4ms/step\n",
      "25/25 [==============================] - 3s 5ms/step\n",
      "25/25 [==============================] - 4s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions using merged dataset of factor and macro data \n",
    "predictions_model_merged_1L, predictions_model_merged_1L_df, predictions_model_merged_1L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_model_merged_1L_alternative.h5', batch_size,label_names)\n",
    "predictions_model_merged_2L, predictions_model_merged_2L_df, predictions_model_merged_2L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_model_merged_2L_alternative.h5', batch_size,label_names)\n",
    "predictions_model_merged_3L, predictions_model_merged_3L_df, predictions_model_merged_3L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_model_merged_3L_alternative.h5', batch_size,label_names)\n",
    "predictions_model_merged_4L, predictions_model_merged_4L_df, predictions_model_merged_4L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged,'checkpoints_alternative/checkpoints_model_merged_4L_alternative.h5', batch_size,label_names)\n",
    "\n",
    "#predictions only using factor data\n",
    "predictions_model_1L, predictions_model_1L_df, predictions_model_1L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_model_1L_alternative.h5', batch_size, label_names)\n",
    "predictions_model_2L, predictions_model_2L_df, predictions_model_2L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_model_2L_alternative.h5', batch_size,label_names)\n",
    "predictions_model_3L, predictions_model_3L_df, predictions_model_3L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_model_3L_alternative.h5', batch_size,label_names)\n",
    "predictions_model_4L, predictions_model_4L_df, predictions_model_4L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_model_4L_alternative.h5', batch_size,label_names)\n",
    "\n",
    "\n",
    "#predictions using merged dataset of factor and macro data and LSTM\n",
    "predictions_LSTM_model_merged_1L, predictions_LSTM_model_merged_1L_df, predictions_LSTM_model_merged_1L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_LSTM_model_merged_1L_alternative.h5', batch_size,label_names)\n",
    "predictions_LSTM_model_merged_2L, predictions_LSTM_model_merged_2L_df, predictions_LSTM_model_merged_2L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_LSTM_model_merged_2L_alternative.h5', batch_size,label_names)\n",
    "predictions_LSTM_model_merged_3L, predictions_LSTM_model_merged_3L_df, predictions_LSTM_model_merged_3L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged, 'checkpoints_alternative/checkpoints_LSTM_model_merged_3L_alternative.h5', batch_size,label_names)\n",
    "predictions_LSTM_model_merged_4L, predictions_LSTM_model_merged_4L_df, predictions_LSTM_model_merged_4L_raw = nn_predict_simple('merged',test_data, x_test_factors,x_test_macro,x_test_merged,'checkpoints_alternative/checkpoints_LSTM_model_merged_4L_alternative.h5', batch_size,label_names)\n",
    "\n",
    "#predictions only using factor data and LSTM\n",
    "predictions_LSTM_model_1L, predictions_LSTM_model_1L_df, predictions_LSTM_model_1L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_LSTM_model_1L_alternative.h5', batch_size, label_names)\n",
    "predictions_LSTM_model_2L, predictions_LSTM_model_2L_df, predictions_LSTM_model_2L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_LSTM_model_2L_alternative.h5', batch_size,label_names)\n",
    "predictions_LSTM_model_3L, predictions_LSTM_model_3L_df, predictions_LSTM_model_3L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_LSTM_model_3L_alternative.h5', batch_size,label_names)\n",
    "predictions_LSTM_model_4L, predictions_LSTM_model_4L_df, predictions_LSTM_model_4L_raw = nn_predict_simple('factors',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_LSTM_model_4L_alternative.h5', batch_size,label_names)\n",
    "\n",
    "#predictions Feedforward + LSTM\n",
    "predictions_FFN_model_L1, predictions_FFN_model_L1_df, predictions_FFN_model_L1_raw = nn_predict_simple('combined',test_data, x_test_factors,x_test_macro,x_test_merged,  'checkpoints_alternative/checkpoints_FFN_model_L1_alternative.h5', batch_size,label_names)\n",
    "predictions_FFN_model_L2, predictions_FFN_model_L2_df, predictions_FFN_model_L2_raw = nn_predict_simple('combined',test_data, x_test_factors,x_test_macro,x_test_merged,'checkpoints_alternative/checkpoints_FFN_model_L2_alternative.h5', batch_size,label_names)\n",
    "predictions_FFN_model_L3, predictions_FFN_model_L3_df, predictions_FFN_model_L3_raw = nn_predict_simple('combined',test_data, x_test_factors,x_test_macro,x_test_merged,'checkpoints_alternative/checkpoints_FFN_model_L3_alternative.h5', batch_size,label_names)\n",
    "predictions_FFN_model_L4, predictions_FFN_model_L4_df, predictions_FFN_model_L4_raw = nn_predict_simple('combined',test_data, x_test_factors,x_test_macro,x_test_merged,'checkpoints_alternative/checkpoints_FFN_model_L4_alternative.h5', batch_size,label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7c4fd",
   "metadata": {},
   "source": [
    "## Checking Results for false predicted quintiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "9dbba3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_model_merged_1L.values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1b7ca",
   "metadata": {},
   "source": [
    "## Since the quitniles above 5 are not existent and indicate that the stock lies in the highest quintile the quintiles are adjusted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "83961d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def quintile_adapter(df): \n",
    "    df_copy= df.copy(deep=True)\n",
    "    max_value = df_copy.values.max()\n",
    "    min_value = df_copy.values.min()\n",
    "    max_list = np.arange(1,max_value+1)\n",
    "    min_list = list(np.arange(min_value, 1, dtype=int))\n",
    "    \n",
    "    for i in range(5, len(max_list)):\n",
    "        df_copy = df_copy.replace(max_list[i], 5)\n",
    "    \n",
    "    if len(min_list)>=1:\n",
    "        for i in range(min_list[0], 1):\n",
    "            print(i)\n",
    "            df_copy= df_copy.replace(min_list[i], 1)\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "#model merged\n",
    "predictions_model_merged_1L = quintile_adapter(predictions_model_merged_1L)\n",
    "predictions_model_merged_1L_df.value = quintile_adapter(predictions_model_merged_1L_df.value)\n",
    "\n",
    "predictions_model_merged_2L = quintile_adapter(predictions_model_merged_2L)\n",
    "predictions_model_merged_2L_df.value = quintile_adapter(predictions_model_merged_2L_df.value)\n",
    "\n",
    "predictions_model_merged_3L = quintile_adapter(predictions_model_merged_3L)\n",
    "predictions_model_merged_3L_df.value = quintile_adapter(predictions_model_merged_3L_df.value)\n",
    "\n",
    "predictions_model_merged_4L = quintile_adapter(predictions_model_merged_4L)\n",
    "predictions_model_merged_4L_df.value = quintile_adapter(predictions_model_merged_4L_df.value)\n",
    "\n",
    "# model factors\n",
    "predictions_model_1L = quintile_adapter(predictions_model_1L)\n",
    "predictions_model_1L_df.value = quintile_adapter(predictions_model_1L_df.value)\n",
    "\n",
    "predictions_model_2L= quintile_adapter(predictions_model_2L)\n",
    "predictions_model_2L_df.value = quintile_adapter(predictions_model_2L_df.value)\n",
    "\n",
    "predictions_model_3L = quintile_adapter(predictions_model_3L)\n",
    "predictions_model_3L_df.value = quintile_adapter(predictions_model_3L_df.value)\n",
    "\n",
    "predictions_model_4L = quintile_adapter(predictions_model_4L)\n",
    "predictions_model_4L_df.value = quintile_adapter(predictions_model_4L_df.value)\n",
    "\n",
    "#lstm merged\n",
    "predictions_LSTM_model_merged_1L = quintile_adapter(predictions_LSTM_model_merged_1L)\n",
    "predictions_LSTM_model_merged_1L_df.value  = quintile_adapter(predictions_LSTM_model_merged_1L_df.value)\n",
    "\n",
    "predictions_LSTM_model_merged_2L = quintile_adapter(predictions_LSTM_model_merged_2L)\n",
    "predictions_LSTM_model_merged_2L_df.value  = quintile_adapter(predictions_LSTM_model_merged_2L_df.value)\n",
    "\n",
    "predictions_LSTM_model_merged_3L = quintile_adapter(predictions_LSTM_model_merged_3L)\n",
    "predictions_LSTM_model_merged_3L_df.value  = quintile_adapter(predictions_LSTM_model_merged_3L_df.value)\n",
    "\n",
    "predictions_LSTM_model_merged_4L = quintile_adapter(predictions_LSTM_model_merged_4L)\n",
    "predictions_LSTM_model_merged_4L_df.value  = quintile_adapter(predictions_LSTM_model_merged_4L_df.value)\n",
    "\n",
    "#lstm factors\n",
    "predictions_LSTM_model_1L = quintile_adapter(predictions_LSTM_model_1L)\n",
    "predictions_LSTM_model_1L_df.value  = quintile_adapter(predictions_LSTM_model_1L_df.value)\n",
    "\n",
    "predictions_LSTM_model_2L = quintile_adapter(predictions_LSTM_model_2L)\n",
    "predictions_LSTM_model_2L_df.value  = quintile_adapter(predictions_LSTM_model_2L_df.value)\n",
    "\n",
    "predictions_LSTM_model_3L = quintile_adapter(predictions_LSTM_model_3L)\n",
    "predictions_LSTM_model_3L_df.value  = quintile_adapter(predictions_LSTM_model_3L_df.value)\n",
    "\n",
    "predictions_LSTM_model_4L = quintile_adapter(predictions_LSTM_model_4L)\n",
    "predictions_LSTM_model_4L_df.value  = quintile_adapter(predictions_LSTM_model_4L_df.value)\n",
    "\n",
    "# FFN models\n",
    "predictions_FFN_model_L1 = quintile_adapter(predictions_FFN_model_L1)\n",
    "predictions_FFN_model_L1_df.value  = quintile_adapter(predictions_FFN_model_L1_df.value)\n",
    "\n",
    "predictions_FFN_model_L2 = quintile_adapter(predictions_FFN_model_L2)\n",
    "predictions_FFN_model_L2_df.value  = quintile_adapter(predictions_FFN_model_L2_df.value)\n",
    "\n",
    "predictions_FFN_model_L3 = quintile_adapter(predictions_FFN_model_L3)\n",
    "predictions_FFN_model_L3_df.value  = quintile_adapter(predictions_FFN_model_L3_df.value)\n",
    "\n",
    "predictions_FFN_model_L4 = quintile_adapter(predictions_FFN_model_L4)\n",
    "predictions_FFN_model_L4_df.value  = quintile_adapter(predictions_FFN_model_L4_df.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "04a5eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_model_merged_1L.to_csv('predictions_model_merged_1L_pivot.csv')\n",
    "file_saver('predictions_model_merged_1L_pivot.csv','predictions_model_merged_1L_pivot.csv')\n",
    "predictions_model_merged_1L_df.to_csv('predictions_model_merged_1L_df.csv')\n",
    "file_saver('predictions_model_merged_1L_df.csv','predictions_model_merged_1L_df.csv')\n",
    "\n",
    "predictions_model_merged_2L.to_csv('predictions_model_merged_2L_pivot.csv')\n",
    "file_saver('predictions_model_merged_2L_pivot.csv','predictions_model_merged_2L_pivot.csv')\n",
    "predictions_model_merged_2L_df.to_csv('predictions_model_merged_2L_df.csv')\n",
    "file_saver('predictions_model_merged_2L_df.csv','predictions_model_merged_2L_df.csv')\n",
    "\n",
    "predictions_model_merged_3L.to_csv('predictions_model_merged_3L_pivot.csv')\n",
    "file_saver('predictions_model_merged_3L_pivot.csv','predictions_model_merged_3L_pivot.csv')\n",
    "predictions_model_merged_3L_df.to_csv('predictions_model_merged_3L_df.csv')\n",
    "file_saver('predictions_model_merged_3L_df.csv','predictions_model_merged_3L_df.csv')\n",
    "\n",
    "predictions_model_merged_4L.to_csv('predictions_model_merged_4L_pivot.csv')\n",
    "file_saver('predictions_model_merged_4L_pivot.csv','predictions_model_merged_4L_pivot.csv')\n",
    "predictions_model_merged_4L_df.to_csv('predictions_model_merged_4L_df.csv')\n",
    "file_saver('predictions_model_merged_4L_df.csv','predictions_model_merged_4L_df.csv')\n",
    "\n",
    "\n",
    "# FF factors\n",
    "predictions_model_1L.to_csv('predictions_model_1L_pivot.csv')\n",
    "file_saver('predictions_model_1L_pivot.csv','predictions_model_1L_pivot.csv')\n",
    "predictions_model_1L_df.to_csv('predictions_model_1L_df.csv')\n",
    "file_saver('predictions_model_1L_df.csv','predictions_model_1L_df.csv')\n",
    "\n",
    "predictions_model_2L.to_csv('predictions_model_2L_pivot.csv')\n",
    "file_saver('predictions_model_2L_pivot.csv','predictions_model_2L_pivot.csv')\n",
    "predictions_model_2L_df.to_csv('predictions_model_2L_df.csv')\n",
    "file_saver('predictions_model_2L_df.csv','predictions_model_2L_df.csv')\n",
    "\n",
    "predictions_model_3L.to_csv('predictions_model_3L_pivot.csv')\n",
    "file_saver('predictions_model_3L_pivot.csv','predictions_model_3L_pivot.csv')\n",
    "predictions_model_3L_df.to_csv('predictions_model_3L_df.csv')\n",
    "file_saver('predictions_model_3L_df.csv','predictions_model_3L_df.csv')\n",
    "\n",
    "predictions_model_4L.to_csv('predictions_model_4L_pivot.csv')\n",
    "file_saver('predictions_model_4L_pivot.csv','predictions_model_4L_pivot.csv')\n",
    "predictions_model_4L_df.to_csv('predictions_model_4L_df.csv')\n",
    "file_saver('predictions_model_4L_df.csv','predictions_model_4L_df.csv')\n",
    "\n",
    "\n",
    "#LSTM merged\n",
    "predictions_LSTM_model_merged_1L.to_csv('predictions_LSTM_model_merged_1L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_merged_1L_pivot.csv','predictions_LSTM_model_merged_1L_pivot.csv')\n",
    "predictions_LSTM_model_merged_1L_df.to_csv('predictions_LSTM_model_merged_1L_df.csv')\n",
    "file_saver('predictions_LSTM_model_merged_1L_df.csv','predictions_LSTM_model_merged_1L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_merged_2L.to_csv('predictions_LSTM_model_merged_2L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_merged_2L_pivot.csv','predictions_LSTM_model_merged_2L_pivot.csv')\n",
    "predictions_LSTM_model_merged_2L_df.to_csv('predictions_LSTM_model_merged_2L_df.csv')\n",
    "file_saver('predictions_LSTM_model_merged_2L_df.csv','predictions_LSTM_model_merged_2L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_merged_3L.to_csv('predictions_LSTM_model_merged_3L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_merged_3L_pivot.csv','predictions_LSTM_model_merged_3L_pivot.csv')\n",
    "predictions_LSTM_model_merged_3L_df.to_csv('predictions_LSTM_model_merged_3L_df.csv')\n",
    "file_saver('predictions_LSTM_model_merged_3L_df.csv','predictions_LSTM_model_merged_3L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_merged_4L.to_csv('predictions_LSTM_model_merged_4L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_merged_4L_pivot.csv','predictions_LSTM_model_merged_4L_pivot.csv')\n",
    "predictions_LSTM_model_merged_4L_df.to_csv('predictions_LSTM_model_merged_4L_df.csv')\n",
    "file_saver('predictions_LSTM_model_merged_4L_df.csv','predictions_LSTM_model_merged_4L_df.csv')\n",
    "\n",
    "\n",
    "#LSTM factors\n",
    "predictions_LSTM_model_1L.to_csv('predictions_LSTM_model_1L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_1L_pivot.csv','predictions_LSTM_model_1L_pivot.csv')\n",
    "predictions_LSTM_model_1L_df.to_csv('predictions_LSTM_model_1L_df.csv')\n",
    "file_saver('predictions_LSTM_model_1L_df.csv','predictions_LSTM_model_1L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_2L.to_csv('predictions_LSTM_model_2L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_2L_pivot.csv','predictions_LSTM_model_2L_pivot.csv')\n",
    "predictions_LSTM_model_2L_df.to_csv('predictions_LSTM_model_2L_df.csv')\n",
    "file_saver('predictions_LSTM_model_2L_df.csv','predictions_LSTM_model_2L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_3L.to_csv('predictions_LSTM_model_3L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_3L_pivot.csv','predictions_LSTM_model_3L_pivot.csv')\n",
    "predictions_LSTM_model_3L_df.to_csv('predictions_LSTM_model_3L_df.csv')\n",
    "file_saver('predictions_LSTM_model_3L_df.csv','predictions_LSTM_model_3L_df.csv')\n",
    "\n",
    "predictions_LSTM_model_4L.to_csv('predictions_LSTM_model_4L_pivot.csv')\n",
    "file_saver('predictions_LSTM_model_4L_pivot.csv','predictions_LSTM_model_4L_pivot.csv')\n",
    "predictions_LSTM_model_4L_df.to_csv('predictions_LSTM_model_4L_df.csv')\n",
    "file_saver('predictions_LSTM_model_4L_df.csv','predictions_LSTM_model_4L_df.csv')\n",
    "\n",
    "\n",
    "# FFN \n",
    "predictions_FFN_model_L1.to_csv('predictions_FFN_model_L1_pivot.csv')\n",
    "file_saver('predictions_FFN_model_L1_pivot.csv','predictions_FFN_model_L1_pivot.csv')\n",
    "predictions_FFN_model_L1_df.to_csv('predictions_FFN_model_L1_df.csv')\n",
    "file_saver('predictions_FFN_model_L1_df.csv','predictions_FFN_model_L1_df.csv')\n",
    "\n",
    "predictions_FFN_model_L2.to_csv('predictions_FFN_model_L2_pivot.csv')\n",
    "file_saver('predictions_FFN_model_L2_pivot.csv','predictions_FFN_model_L2_pivot.csv')\n",
    "predictions_FFN_model_L2_df.to_csv('predictions_FFN_model_L2_df.csv')\n",
    "file_saver('predictions_FFN_model_L2_df.csv','predictions_FFN_model_L2_df.csv')\n",
    "\n",
    "predictions_FFN_model_L3.to_csv('predictions_FFN_model_L3_pivot.csv')\n",
    "file_saver('predictions_FFN_model_L3_pivot.csv','predictions_FFN_model_L3_pivot.csv')\n",
    "predictions_FFN_model_L3_df.to_csv('predictions_FFN_model_L3_df.csv')\n",
    "file_saver('predictions_FFN_model_L3_df.csv','predictions_FFN_model_L3_df.csv')\n",
    "\n",
    "predictions_FFN_model_L4.to_csv('predictions_FFN_model_L4_pivot.csv')\n",
    "file_saver('predictions_FFN_model_L4_pivot.csv','predictions_FFN_model_L4_pivot.csv')\n",
    "predictions_FFN_model_L4_df.to_csv('predictions_FFN_model_L4_df.csv')\n",
    "file_saver('predictions_FFN_model_L4_df.csv','predictions_FFN_model_L4_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fabb82",
   "metadata": {},
   "source": [
    "# Creating Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "372c1276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' create_metrics(test_data_df, prediction_df, batch_size, n_factors, r2_oos,dataframe_name) '"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" create_metrics(test_data_df, prediction_df, batch_size, n_factors, r2_oos,dataframe_name) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "fab23cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_merged_1L_results, model_merged_1L_metrics = create_metrics(test_data_df, predictions_model_merged_1L_df, batch_size,n_factors, r2_oos, 'FF_merged_1L_metrics')\n",
    "model_merged_2L_results, model_merged_2L_metrics = create_metrics(test_data_df, predictions_model_merged_2L_df, batch_size,n_factors, r2_oos, 'FF_merged_2L_metrics')\n",
    "model_merged_3L_results, model_merged_3L_metrics = create_metrics(test_data_df, predictions_model_merged_3L_df, batch_size,n_factors, r2_oos, 'FF_merged_3L_metrics')\n",
    "model_merged_4L_results, model_merged_4L_metrics = create_metrics(test_data_df, predictions_model_merged_4L_df, batch_size,n_factors, r2_oos, 'FF_merged_4L_metrics')\n",
    "\n",
    "model_1L_results, model_1L_metrics = create_metrics(test_data_df, predictions_model_1L_df, batch_size,n_factors, r2_oos, 'FF_1L_metrics')\n",
    "model_2L_results, model_2L_metrics = create_metrics(test_data_df, predictions_model_2L_df, batch_size,n_factors, r2_oos, 'FF_2L_metrics')\n",
    "model_3L_results, model_3L_metrics = create_metrics(test_data_df, predictions_model_3L_df, batch_size,n_factors, r2_oos, 'FF_3L_metrics')\n",
    "model_4L_results, model_4L_metrics = create_metrics(test_data_df, predictions_model_4L_df, batch_size,n_factors, r2_oos, 'FF_4L_metrics')\n",
    "\n",
    "LSTM_model_merged_1L_results, LSTM_model_merged_1L_metrics = create_metrics(test_data_df, predictions_LSTM_model_merged_1L_df, batch_size,n_factors, r2_oos, 'LSTM_merged_1L_metrics')\n",
    "LSTM_model_merged_2L_results, LSTM_model_merged_2L_metrics = create_metrics(test_data_df, predictions_LSTM_model_merged_2L_df, batch_size,n_factors, r2_oos, 'LSTM_merged_2L_metrics')\n",
    "LSTM_model_merged_3L_results, LSTM_model_merged_3L_metrics = create_metrics(test_data_df, predictions_LSTM_model_merged_3L_df, batch_size,n_factors, r2_oos, 'LSTM_merged_3L_metrics')\n",
    "LSTM_model_merged_4L_results, LSTM_model_merged_4L_metrics = create_metrics(test_data_df, predictions_LSTM_model_merged_4L_df, batch_size,n_factors, r2_oos, 'LSTM_merged_4L_metrics')\n",
    "\n",
    "LSTM_model_1L_results, LSTM_model_1L_metrics = create_metrics(test_data_df, predictions_LSTM_model_1L_df, batch_size,n_factors, r2_oos, 'LSTM_1L_metrics')\n",
    "LSTM_model_2L_results, LSTM_model_2L_metrics = create_metrics(test_data_df, predictions_LSTM_model_2L_df, batch_size,n_factors, r2_oos, 'LSTM_2L_metrics')\n",
    "LSTM_model_3L_results, LSTM_model_3L_metrics = create_metrics(test_data_df, predictions_LSTM_model_3L_df, batch_size,n_factors, r2_oos, 'LSTM_3L_metrics')\n",
    "LSTM_model_4L_results, LSTM_model_4L_metrics = create_metrics(test_data_df, predictions_LSTM_model_4L_df, batch_size,n_factors, r2_oos, 'LSTM_4L_metrics')\n",
    "\n",
    "FFN_model_L1_results, FFN_model_L1_metrics = create_metrics(test_data_df, predictions_FFN_model_L1_df, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics')\n",
    "FFN_model_L2_results, FFN_model_L2_metrics = create_metrics(test_data_df, predictions_FFN_model_L2_df, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics')\n",
    "FFN_model_L3_results, FFN_model_L3_metrics = create_metrics(test_data_df, predictions_FFN_model_L3_df, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics')\n",
    "FFN_model_L4_results, FFN_model_L4_metrics = create_metrics(test_data_df, predictions_FFN_model_L4_df, batch_size,n_factors, r2_oos, 'FFN_model_L1_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "81eb2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_results = 'prediction_results_nn/'\n",
    "\n",
    "############################################################################################\n",
    "model_1L_results.to_csv('model_1L_results.csv')\n",
    "file_saver('prediction_results_nn/model_1L_results.csv', 'model_1L_results.csv')\n",
    "\n",
    "model_1L_metrics.to_csv('model_1L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_1L_metrics.csv', 'model_1L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_2L_results.to_csv('model_2L_results.csv')\n",
    "file_saver('prediction_results_nn/model_2L_results.csv', 'model_2L_results.csv')\n",
    "\n",
    "model_2L_metrics.to_csv('model_2L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_2L_metrics.csv', 'model_2L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_3L_results.to_csv('model_3L_results.csv')\n",
    "file_saver('prediction_results_nn/model_3L_results.csv', 'model_3L_results.csv')\n",
    "\n",
    "model_3L_metrics.to_csv('model_3L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_3L_metrics.csv', 'model_3L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_4L_results.to_csv('model_4L_results.csv')\n",
    "file_saver('prediction_results_nn/model_4L_results.csv', 'model_4L_results.csv')\n",
    "\n",
    "model_4L_metrics.to_csv('model_4L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_4L_metrics.csv', 'model_4L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"merged model save\"\"\"\n",
    "############################################################################################\n",
    "model_merged_1L_results.to_csv('model_merged_1L_results.csv')\n",
    "file_saver('prediction_results_nn/model_merged_1L_results.csv', 'model_merged_1L_results.csv')\n",
    "\n",
    "model_merged_1L_metrics.to_csv('model_merged_1L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_merged_1L_metrics.csv', 'model_merged_1L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_merged_2L_results.to_csv('model_merged_2L_results.csv')\n",
    "file_saver('prediction_results_nn/model_merged_2L_results.csv', 'model_merged_2L_results.csv')\n",
    "\n",
    "model_merged_2L_metrics.to_csv('model_merged_2L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_merged_2L_metrics.csv', 'model_merged_2L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_merged_3L_results.to_csv('model_merged_3L_results.csv')\n",
    "file_saver('prediction_results_nn/model_merged_3L_results.csv', 'model_merged_3L_results.csv')\n",
    "\n",
    "model_merged_3L_metrics.to_csv('model_merged_3L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_merged_3L_metrics.csv', 'model_merged_3L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "model_merged_4L_results.to_csv('model_merged_4L_results.csv')\n",
    "file_saver('prediction_results_nn/model_merged_4L_results.csv', 'model_merged_4L_results.csv')\n",
    "\n",
    "model_merged_4L_metrics.to_csv('model_merged_4L_metrics.csv')\n",
    "file_saver('prediction_results_nn/model_merged_4L_metrics.csv', 'model_merged_4L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"merged model save\"\"\"\n",
    "############################################################################################\n",
    "LSTM_model_merged_1L_results.to_csv('LSTM_model_merged_1L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_1L_results.csv', 'LSTM_model_merged_1L_results.csv')\n",
    "\n",
    "LSTM_model_merged_1L_metrics.to_csv('LSTM_model_merged_1L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_1L_metrics.csv', 'LSTM_model_merged_1L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_2L_results.to_csv('LSTM_model_merged_2L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_2L_results.csv', 'LSTM_model_merged_2L_results.csv')\n",
    "\n",
    "LSTM_model_merged_2L_metrics.to_csv('LSTM_model_merged_2L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_2L_metrics.csv', 'LSTM_model_merged_2L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_3L_results.to_csv('LSTM_model_merged_3L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_3L_results.csv', 'LSTM_model_merged_3L_results.csv')\n",
    "\n",
    "LSTM_model_merged_3L_metrics.to_csv('LSTM_model_merged_3L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_3L_metrics.csv', 'LSTM_model_merged_3L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_merged_4L_results.to_csv('LSTM_model_merged_4L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_4L_results.csv', 'LSTM_model_merged_4L_results.csv')\n",
    "\n",
    "LSTM_model_merged_4L_metrics.to_csv('LSTM_model_merged_4L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_merged_4L_metrics.csv', 'LSTM_model_merged_4L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "\"\"\" factor models only save\"\"\"\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_1L_results.to_csv('LSTM_model_1L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_1L_results.csv', 'LSTM_model_1L_results.csv')\n",
    "\n",
    "LSTM_model_1L_metrics.to_csv('LSTM_model_1L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_1L_metrics.csv', 'LSTM_model_1L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_2L_results.to_csv('LSTM_model_2L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_2L_results.csv', 'LSTM_model_2L_results.csv')\n",
    "\n",
    "LSTM_model_2L_metrics.to_csv('LSTM_model_2L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_2L_metrics.csv', 'LSTM_model_2L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_3L_results.to_csv('LSTM_model_3L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_3L_results.csv', 'LSTM_model_3L_results.csv')\n",
    "\n",
    "LSTM_model_3L_metrics.to_csv('LSTM_model_3L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_3L_metrics.csv', 'LSTM_model_3L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "############################################################################################\n",
    "LSTM_model_4L_results.to_csv('LSTM_model_4L_results.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_4L_results.csv', 'LSTM_model_4L_results.csv')\n",
    "\n",
    "LSTM_model_4L_metrics.to_csv('LSTM_model_4L_metrics.csv')\n",
    "file_saver('prediction_results_nn/LSTM_model_4L_metrics.csv', 'LSTM_model_4L_metrics.csv')\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "\"\"\"combined model save \"\"\"\n",
    "############################################################################################\n",
    "FFN_model_L1_results.to_csv('FFN_model_L1_results.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L1_results.csv', 'FFN_model_L1_results.csv')\n",
    "\n",
    "FFN_model_L1_metrics.to_csv('FFN_model_L1_metrics.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L1_metrics.csv', 'FFN_model_L1_metrics.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L2_results.to_csv('FFN_model_L2_results.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L2_results.csv', 'FFN_model_L2_results.csv')\n",
    "\n",
    "FFN_model_L2_metrics.to_csv('FFN_model_L2_metrics.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L2_metrics.csv', 'FFN_model_L2_metrics.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L3_results.to_csv('FFN_model_L3_results.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L3_results.csv', 'FFN_model_L3_results.csv')\n",
    "\n",
    "FFN_model_L3_metrics.to_csv('FFN_model_L3_metrics.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L3_metrics.csv', 'FFN_model_L3_metrics.csv')\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "FFN_model_L4_results.to_csv('FFN_model_L4_results.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L4_results.csv', 'FFN_model_L4_results.csv')\n",
    "\n",
    "FFN_model_L4_metrics.to_csv('FFN_model_L4_metrics.csv')\n",
    "file_saver('prediction_results_nn/FFN_model_L4_metrics.csv', 'FFN_model_L4_metrics.csv')\n",
    "############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "d3ee7e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_model_merged_4L_results['pred quint'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f61f7b",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1929cd",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "116461b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_LSTM_model_merged_1L_alternative = path_alternative+'checkpoints_LSTM_model_merged_1L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_2L_alternative = path_alternative+'checkpoints_LSTM_model_merged_2L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_3L_alternative = path_alternative+'checkpoints_LSTM_model_merged_3L_alternative.h5'\n",
    "checkpoints_LSTM_model_merged_4L_alternative = path_alternative+'checkpoints_LSTM_model_merged_4L_alternative.h5'\n",
    "\n",
    "checkpoints_LSTM_model_1L_alternative = path_alternative+'checkpoints_LSTM_model_1L_alternative.h5'\n",
    "checkpoints_LSTM_model_2L_alternative = path_alternative+'checkpoints_LSTM_model_2L_alternative.h5'\n",
    "checkpoints_LSTM_model_3L_alternative = path_alternative+'checkpoints_LSTM_model_3L_alternative.h5'\n",
    "checkpoints_LSTM_model_4L_alternative = path_alternative+'checkpoints_LSTM_model_4L_alternative.h5'\n",
    "\n",
    "checkpoints_FFN_model_L1_alternative = path_alternative+'checkpoints_FFN_model_L1_alternative.h5'\n",
    "checkpoints_FFN_model_L2_alternative = path_alternative+'checkpoints_FFN_model_L2_alternative.h5'\n",
    "checkpoints_FFN_model_L3_alternative = path_alternative+'checkpoints_FFN_model_L3_alternative.h5'\n",
    "checkpoints_FFN_model_L4_alternative = path_alternative+'checkpoints_FFN_model_L4_alternative.h5'\n",
    "\n",
    "checkpoints_model_merged_1L_alternative = path_alternative+'checkpoints_model_merged_1L_alternative.h5'\n",
    "checkpoints_model_merged_2L_alternative = path_alternative+'checkpoints_model_merged_2L_alternative.h5'\n",
    "checkpoints_model_merged_3L_alternative = path_alternative+'checkpoints_model_merged_3L_alternative.h5'\n",
    "checkpoints_model_merged_4L_alternative = path_alternative+'checkpoints_model_merged_4L_alternative.h5'\n",
    "\n",
    "checkpoints_model_1L_alternative = path_alternative+'checkpoints_model_1L_alternative.h5'\n",
    "checkpoints_model_2L_alternative = path_alternative+'checkpoints_model_2L_alternative.h5'\n",
    "checkpoints_model_3L_alternative = path_alternative+'checkpoints_model_3L_alternative.h5'\n",
    "checkpoints_model_4L_alternative = path_alternative+'checkpoints_model_4L_alternative.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "469ca5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM_merged_1L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_1L_alternative)\n",
    "model_LSTM_merged_2L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_2L_alternative)\n",
    "model_LSTM_merged_3L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_3L_alternative)\n",
    "model_LSTM_merged_4L = tf.keras.models.load_model(checkpoints_LSTM_model_merged_4L_alternative)\n",
    "\n",
    "model_LSTM_1L = tf.keras.models.load_model(checkpoints_LSTM_model_1L_alternative)\n",
    "model_LSTM_2L = tf.keras.models.load_model(checkpoints_LSTM_model_1L_alternative)\n",
    "model_LSTM_3L = tf.keras.models.load_model(checkpoints_LSTM_model_1L_alternative)\n",
    "model_LSTM_4L = tf.keras.models.load_model(checkpoints_LSTM_model_1L_alternative)\n",
    "\n",
    "model_FFN_1L = tf.keras.models.load_model(checkpoints_FFN_model_L1_alternative)\n",
    "model_FFN_2L = tf.keras.models.load_model(checkpoints_FFN_model_L2_alternative)\n",
    "model_FFN_3L = tf.keras.models.load_model(checkpoints_FFN_model_L3_alternative)\n",
    "model_FFN_4L = tf.keras.models.load_model(checkpoints_FFN_model_L4_alternative)\n",
    "\n",
    "\n",
    "model_merged_1L = tf.keras.models.load_model(checkpoints_model_merged_1L_alternative)\n",
    "model_merged_2L = tf.keras.models.load_model(checkpoints_model_merged_1L_alternative)\n",
    "model_merged_3L = tf.keras.models.load_model(checkpoints_model_merged_1L_alternative)\n",
    "model_merged_4L = tf.keras.models.load_model(checkpoints_model_merged_1L_alternative)\n",
    "\n",
    "model_1L = tf.keras.models.load_model(checkpoints_model_4L_alternative)\n",
    "model_2L = tf.keras.models.load_model(checkpoints_model_4L_alternative)\n",
    "model_3L = tf.keras.models.load_model(checkpoints_model_4L_alternative)\n",
    "model_4L = tf.keras.models.load_model(checkpoints_model_4L_alternative)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "4b5d4a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4, 169)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "2a960014",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [444]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# # from tensorflow.compat.v1.keras.backend import get_session\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# # import tensorflow.compat.v1.keras.backend as K\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# # import tensorflow as tf\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# # shap.explainers._deep.deep_tf.op_handlers[\"FusedLayerNorm\"] = shap.explainers._deep.deep_tf.linearity_1d(0)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\u001b[39;00m\n\u001b[1;32m     43\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(model_4L, x_train_factors)\n\u001b[0;32m---> 44\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m shap\u001b[38;5;241m.\u001b[39msummary(shap_values)\n\u001b[1;32m     46\u001b[0m shap\u001b[38;5;241m.\u001b[39mbar_plot(shap_values)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/shap/explainers/_permutation.py:154\u001b[0m, in \u001b[0;36mPermutation.shap_values\u001b[0;34m(self, X, npermutations, main_effects, error_bounds, batch_evals, silent)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, npermutations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, main_effects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error_bounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_evals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124;03m\"\"\" Legacy interface to estimate the SHAP values for a set of samples.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m        of such matrices, one for each output.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpermutations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m explanation\u001b[38;5;241m.\u001b[39m_old_format()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/shap/explainers/_explainer.py:255\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[0;32m--> 255\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    260\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/shap/explainers/_permutation.py:99\u001b[0m, in \u001b[0;36mPermutation.explain_row\u001b[0;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[1;32m     96\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# evaluate the masked model\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     row_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(fm),) \u001b[38;5;241m+\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/shap/utils/_masked_model.py:58\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[0;34m(self, masks, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m         full_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m     57\u001b[0m         _convert_delta_mask_to_full(masks, full_masks)\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(masks, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/shap/utils/_masked_model.py:87\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[0;34m(self, masks, batch_size)\u001b[0m\n\u001b[1;32m     85\u001b[0m     masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker(delta_ind, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# wrap the masked inputs if they are not already in a tuple\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(masked_inputs, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# # from tensorflow.compat.v1.keras.backend import get_session\n",
    "# # import tensorflow.compat.v1.keras.backend as K\n",
    "# # import tensorflow as tf\n",
    "# # tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# features_factors = final_factor_data.columns.tolist()\n",
    "# features_macro = final_macro_data.columns.tolist()\n",
    "# features_merged = features_factors +features_macro\n",
    "\n",
    "# background_factors = x_train_factors\n",
    "# background_macro = x_train_macro\n",
    "# background_merged = x_train_merged[:100]\n",
    "# # background_merged = shap.sample(background_merged, 1)\n",
    "\n",
    "# train_factors = x_train_factors[0]\n",
    "# train_macro = x_train_macro[0]\n",
    "# train_merged = x_train_merged[:100]\n",
    "# print(train_merged.shape)\n",
    "\n",
    "# test_factors = x_test_factors[0]\n",
    "# test_macro = x_test_macro[0]\n",
    "# test_merged = x_test_merged[:10]\n",
    "# print(test_merged.shape)\n",
    "\n",
    "\n",
    "# test_final  =  y_test[:10]\n",
    "# # test_final = shap.sample(test_final, 1)\n",
    "# # test_merged = shap.sample(test_merged, 1)\n",
    "\n",
    "\n",
    "# # shap.explainers._deep.deep_tf.op_handlers[\"FusedLayerNorm\"] = shap.explainers._deep.deep_tf.linearity_1d(0)\n",
    "# shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "\n",
    "# explainer = shap.Explainer(model_4L, x_train_factors)\n",
    "# shap_values = explainer.shap_values(y_test)\n",
    "# shap.summary(shap_values)\n",
    "# shap.bar_plot(shap_values)\n",
    "\n",
    "# model = model_LSTM_merged_1L\n",
    "    \n",
    "# # def f(data):\n",
    "# #     return model_LSTM_merged_1L.predict([x_train_merged[:,i] for i in range(x_train_merged.shape[1])]).flatten()\n",
    "\n",
    "# explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "# shap_values = explainer.shap_values(test_merged)\n",
    "# shap.summary(shap_values)\n",
    "# shap.bar_plot(shap_values)\n",
    "\n",
    "# # explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "# # shap_values = explainer.shap_values(test_merged)\n",
    "# # shap.summary(shap_values)\n",
    "# # shap.bar_plot(shap_values)\n",
    "\n",
    "# # explainer = shap.DeepExplainer(model_LSTM_merged_1L, background_merged)\n",
    "# # shap_values = explainer.shap_values(test_merged)\n",
    "# # shap.summary(shap_values)\n",
    "# # shap.bar_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b8f374c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2975a331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1cb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5d.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
